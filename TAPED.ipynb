{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TAPED.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjnF97-N4Db1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c296c85-595d-4ac5-fd0d-82e88d7d80a6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osAePMtIvTf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %cd /content/drive/My\\ Drive/"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sctjhLVC4dXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# !git clone https://github.com/huggingface/transformers\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG9zfezm4rNB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "518ae425-d04f-41b8-ff72-69b81c1fb377"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/transformers/src/transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/transformers/src/transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pb64oMs9zOfc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "43646a9c-1ff2-4af5-c1a3-717dc9b0ba0a"
      },
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (0.0.43)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->-r requirements.txt (line 4)) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->-r requirements.txt (line 4)) (1.12.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 6)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 6)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 6)) (2020.4.5.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 6)) (2.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->-r requirements.txt (line 10)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->-r requirements.txt (line 10)) (0.15.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd1NvjAgTgAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi7gJ8CTms0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzKWRcHj426s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=False)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnuC7cMP5bo5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9509ccfc-ac08-4d6b-d7e5-a70b79642e5c"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YGPV7aKcIrP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2142274-9af7-4b11-d5e2-1a4e3d36c8dd"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LorELVXBcR4z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c64d0955-7aa4-4141-fb29-efb7b81f08f4"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSKw7KFh6HoU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86b6ea3c-1b1c-42bb-ec04-744cce25dddc"
      },
      "source": [
        "import wget\n",
        "import os\n",
        "print('Downloading dataset...')\n",
        "\n",
        "# The URL for the dataset zip file.\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "# Download the file (if we haven't already)\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8_hCpS_6j3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiqZRpb560Et",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "outputId": "866496b8-3a54-4ccf-a516-8b7d3b0bc5eb"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "trans = pd.read_csv(\"transaction_final.csv\")\n",
        "trans=trans.groupby(['STORE_ID','WEEK_NO']).agg(lambda x:list(x)).reset_index()\n",
        "product=pd.read_csv(\"products_final.csv\")\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of assortment sets: {:,}\\n'.format(trans.shape[0]))\n",
        "print('Number of products: {:,}\\n'.format(product.shape[0]))\n",
        "\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "print(trans.sample(10))\n",
        "print(product.tail(10))\n",
        "trans.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of assortment sets: 13,937\n",
            "\n",
            "Number of products: 92,353\n",
            "\n",
            "       STORE_ID  ...                                              INDEX\n",
            "3588        335  ...  [6678, 25715, 25755, 29658, 35085, 35577, 9367...\n",
            "10495       622  ...  [49464, 47022, 49180, 49250, 62933, 69197, 727...\n",
            "12554      3287  ...  [2927, 544, 1635, 514, 1156, 66217, 3709, 424,...\n",
            "488         289  ...  [38263, 29658, 13160, 29243, 12685, 14785, 195...\n",
            "681         292  ...  [6678, 38263, 15968, 25715, 25755, 32779, 3557...\n",
            "6576        380  ...  [38263, 25715, 25755, 35577, 25814, 17780, 229...\n",
            "13042     31582  ...  [29658, 35577, 64106, 17780, 21117, 29410, 712...\n",
            "3357        333  ...  [38263, 8717, 25715, 29658, 35577, 9960, 13160...\n",
            "4410        346  ...  [18151, 38263, 9367, 17780, 34403, 11165, 2822...\n",
            "8460        421  ...  [6678, 8895, 25755, 29658, 35577, 37360, 38132...\n",
            "\n",
            "[10 rows x 5 columns]\n",
            "       PRODUCT_ID  MANUFACTURER  ... CURR_SIZE_OF_PRODUCT  INDEX\n",
            "92343    18273019          2223  ...              11.5 OZ  92344\n",
            "92344    18273051           436  ...                64 OZ  92345\n",
            "92345    18273115          1681  ...                       92346\n",
            "92346    18273133          2227  ...                16 OZ  92347\n",
            "92347    18292005           764  ...                       92348\n",
            "92348    18293142          6384  ...                       92349\n",
            "92349    18293439          6393  ...                       92350\n",
            "92350    18293696          6406  ...                       92351\n",
            "92351    18294080          6442  ...                       92352\n",
            "92352    18316298           764  ...                       92353\n",
            "\n",
            "[10 rows x 8 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>STORE_ID</th>\n",
              "      <th>WEEK_NO</th>\n",
              "      <th>PRODUCT_ID</th>\n",
              "      <th>WEEK_QUANTITY</th>\n",
              "      <th>INDEX</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>[480014, 718226]</td>\n",
              "      <td>[7249, 1]</td>\n",
              "      <td>[4059, 5293]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>[480014]</td>\n",
              "      <td>[2438]</td>\n",
              "      <td>[4059]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>[6903760]</td>\n",
              "      <td>[1]</td>\n",
              "      <td>[58675]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>41</td>\n",
              "      <td>[702733]</td>\n",
              "      <td>[1]</td>\n",
              "      <td>[5223]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>93</td>\n",
              "      <td>[721164]</td>\n",
              "      <td>[1]</td>\n",
              "      <td>[5304]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   STORE_ID  WEEK_NO        PRODUCT_ID WEEK_QUANTITY         INDEX\n",
              "0         1        5  [480014, 718226]     [7249, 1]  [4059, 5293]\n",
              "1         1        6          [480014]        [2438]        [4059]\n",
              "2         1       14         [6903760]           [1]       [58675]\n",
              "3         2       41          [702733]           [1]        [5223]\n",
              "4         2       93          [721164]           [1]        [5304]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUkttVxA7cvb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "01e373d0-9917-4b71-ddad-a1626604492d"
      },
      "source": [
        "\n",
        "assortment = trans.INDEX.values\n",
        "# assortment=[product[product.PRODUCT_ID==25671].index[0] for i in range(len(assortment)) for j in assortment[i]]\n",
        "labels = trans.WEEK_QUANTITY.values\n",
        "assortment"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([4059, 5293]), list([4059]), list([58675]), ...,\n",
              "       list([6678, 18151, 38263, 25715, 29658, 32779, 35577, 53098, 13160, 25814, 8282, 15137, 28845, 32710, 36751, 23070, 8093, 9070, 10425, 16016, 18006, 20974, 40584, 41176, 7700, 15398, 35460, 62070, 16864, 22061, 24467, 30630, 53125, 71200, 11259, 12657, 16607, 32073, 39228, 24251, 41194, 59493, 35574, 38616, 39582, 8723, 9206, 31145, 36965, 26316, 12238, 18194, 20654, 24951, 26209, 33726, 6689, 37475, 13068, 21570, 22187, 30716, 19223, 12168, 32045, 61224, 7113, 21816, 21172, 72641, 15431, 32979, 16656, 30576, 20441, 34160, 23373, 52957, 17111, 30648, 9139, 20879, 25096, 11868, 21968, 30991, 38688, 27918, 32949, 75495, 27441, 31193, 16119, 41694, 11514, 16040, 24962, 14938, 28694, 20490, 21352, 52608, 7711, 25098, 32588, 40155, 22717, 38365, 75312, 25591, 33553, 34645, 15051, 38632, 38878, 69964, 33043, 67228, 20351, 5938, 14808, 24745, 10759, 41180, 15839, 30871, 33968, 19771, 39012, 53992, 6566, 19088, 36413, 59802, 14249, 15897, 16914, 23029, 37315, 40005, 31841, 60797, 15074, 62106, 27689, 15586, 22237, 7840, 14280, 32028, 33918, 37330, 39122, 28543, 38277, 14333, 11530, 9197, 12315, 21074, 13695, 7630, 40719, 52621, 15609, 34260, 35916, 37084, 60244, 70053, 84176, 26466, 27032, 30597, 35751, 52931, 57151, 14047, 17941, 28859, 58192, 7825, 6494, 20039, 40703, 75812, 84617, 18924, 28101, 28253, 27643, 69924, 54465, 11296, 39166, 29172, 18456, 14482, 25509, 90684, 19379, 81357, 5899, 40184, 15459, 36692, 92326, 31392, 25595, 22456, 34013, 55408, 16301, 29117, 15214, 24680, 17488, 14783, 31622, 28866, 7338, 31436, 68632, 78785, 36104, 38206, 23085, 30408, 26229, 60680, 29785, 36933, 39083, 84985, 20862, 14415, 87889, 26382, 17959, 62546, 90192, 67229, 83234, 38424, 13375, 5838, 54075, 74874, 19844, 15588, 39672, 71049, 24364, 27384, 26818, 60147, 28245, 40575, 54121, 10107, 22897, 20790, 78547, 13166, 38582, 38938, 71981, 36246, 10705, 39893, 9718, 73762, 19189, 27297, 80511, 55662, 21333, 17075, 33179, 14648, 37853, 33214, 78817, 34917, 10744, 84289, 38580, 64238, 83593, 59366, 53205, 86266, 88030, 71424, 84228, 40592, 87331, 6604, 19744, 19923, 92062, 92080, 92078, 77457, 91539, 38524, 35563, 15411, 60138, 7625, 20501, 34756, 29291, 86301, 19489, 23605, 54188, 18485, 38163, 41402, 57874, 34489, 18660, 85895, 67242, 89903, 15524, 40856, 57120, 53103, 32751, 25653, 12803, 9571, 15942, 8291, 91796, 81256, 15175, 25559, 84484, 21262, 83859, 10998, 14770, 27952, 60531, 61364, 57222, 54340, 83730, 21323, 79350, 77657, 82227, 29789, 9546, 76010, 54438, 91650, 40416, 8119, 92280, 91128, 6921, 53422, 10251, 10971, 85095, 23575, 18747, 68917, 77571, 84943, 88990, 39160, 16855, 91272, 77756, 61294, 63250, 86753, 91661, 63102, 55072, 61424, 63502, 86709, 89429, 89473, 89976, 90009, 91359, 92270]),\n",
              "       list([12765, 25755, 27079, 29658, 32621, 32779, 35577, 53098, 9367, 13120, 13160, 33407, 6734, 8282, 14209, 17780, 22352, 28845, 12685, 9070, 11165, 19506, 29066, 40257, 41176, 14587, 15398, 28829, 13229, 22061, 53125, 53185, 63378, 11259, 23447, 24251, 32687, 59493, 11170, 8723, 17872, 18153, 31145, 34747, 26316, 30581, 38554, 75167, 34652, 40535, 12792, 21570, 27393, 27691, 22187, 22559, 29107, 30716, 39296, 31952, 39820, 53906, 61224, 33904, 53907, 19743, 54364, 10801, 14692, 17116, 9531, 32427, 16656, 20644, 7451, 20441, 25470, 34160, 10195, 13053, 24800, 32358, 52957, 12021, 17111, 23352, 56814, 27031, 7665, 55732, 71796, 22555, 31613, 32949, 16094, 32530, 41694, 16040, 24962, 30733, 78485, 13372, 39119, 20490, 71667, 7711, 11950, 14686, 24223, 84689, 84691, 32588, 56796, 22717, 75493, 87146, 34186, 11209, 15469, 65094, 20763, 27436, 9784, 38878, 82313, 14679, 40303, 10631, 35582, 9042, 41180, 8638, 33684, 6936, 12507, 24194, 30025, 7506, 12609, 53939, 6036, 41789, 15191, 17306, 17489, 11297, 16410, 19735, 23029, 26941, 8048, 8241, 20928, 22071, 21059, 29723, 25220, 19798, 25850, 26354, 19325, 22966, 23244, 32685, 5908, 41453, 77615, 31281, 35934, 81255, 26626, 13971, 15518, 34260, 56728, 56811, 29543, 37702, 76603, 19354, 37741, 69942, 32338, 8159, 56749, 57151, 12076, 69909, 73863, 19421, 32575, 34052, 57995, 83788, 30392, 22766, 69945, 86625, 18924, 14089, 13082, 33384, 41679, 81390, 70594, 34582, 12776, 16001, 27800, 89263, 5916, 59368, 28053, 60593, 89274, 33571, 16212, 80308, 36349, 40314, 5899, 92150, 91249, 21637, 7833, 27350, 70131, 73145, 7709, 23039, 22416, 24680, 14783, 14189, 36629, 28368, 17575, 29389, 21836, 79624, 54493, 77240, 59199, 61485, 26622, 36104, 38206, 57422, 18142, 21552, 26379, 26229, 35077, 27957, 58832, 60641, 23870, 69174, 16288, 38801, 79368, 27922, 88023, 62546, 33146, 24265, 15829, 91932, 92083, 8506, 60149, 18305, 40877, 12757, 31746, 19202, 17325, 20801, 60323, 71049, 68837, 15517, 27384, 36558, 30237, 17248, 38874, 23272, 29619, 29330, 10107, 20207, 78547, 37529, 33210, 38957, 70469, 7610, 37594, 11136, 26922, 29334, 75036, 69975, 81419, 15979, 55421, 84040, 31544, 34917, 10286, 85544, 55345, 82384, 29439, 70593, 16925, 76462, 89785, 85102, 70349, 19646, 65473, 38625, 30635, 38583, 59104, 91822, 54347, 54505, 6680, 38524, 52734, 64225, 7637, 70417, 27340, 15627, 8926, 8332, 40635, 19489, 77235, 77456, 7758, 14644, 10800, 67744, 34811, 24159, 37958, 26108, 63894, 40338, 39876, 20045, 85895, 85801, 90828, 66654, 21033, 58898, 59239, 62844, 38038, 16357, 32024, 57120, 80680, 81976, 22214, 87149, 40096, 60352, 60604, 55291, 41763, 73729, 32387, 83946, 11804, 12595, 34104, 33051, 55913, 25559, 11347, 27325, 85502, 21262, 17357, 92099, 90984, 19772, 41631, 14770, 27952, 59639, 34200, 87413, 57172, 57222, 57182, 57217, 10408, 63599, 86591, 88942, 64460, 25515, 24028, 69453, 34638, 11731, 38668, 19001, 62537, 31931, 90112, 7836, 69516, 86986, 20983, 13812, 12308, 19138, 31393, 60115, 75598, 77220, 63058, 81304, 91128, 15526, 26514, 59827, 83094, 28028, 62817, 34609, 91966, 23575, 81781, 88214, 53033, 83168, 24161, 31074, 59524, 41860, 87021, 9662, 90634, 58369, 33437, 67004, 89041, 85505, 63250, 91535, 11450, 14539, 58798, 62951, 66002, 73040, 74373, 76730, 83264, 88484, 91341, 91718, 92012, 92191, 92311]),\n",
              "       list([6678, 18151, 38263, 25755, 29658, 35577, 37360, 9960, 13160, 25814, 30032, 31164, 10293, 17780, 8093, 10425, 16016, 20974, 26997, 28222, 14587, 10772, 11892, 24467, 30630, 34023, 19758, 10656, 23014, 24272, 26674, 24251, 11170, 23769, 66496, 8723, 26870, 35038, 36965, 37148, 60652, 33297, 9747, 12238, 24951, 26209, 28044, 30581, 33928, 18295, 6355, 17295, 19131, 26516, 9315, 11794, 22187, 40120, 21503, 37387, 14501, 19307, 19743, 41001, 16144, 15431, 21050, 33685, 7629, 30576, 33751, 63318, 7451, 8491, 20441, 21130, 79347, 18423, 23352, 15260, 25096, 24054, 34194, 11769, 20173, 37266, 31613, 34764, 53059, 32935, 36789, 53111, 22152, 31328, 41694, 59063, 83182, 34732, 13372, 41538, 8088, 12272, 20698, 24933, 20183, 70439, 25098, 12738, 32588, 12805, 21638, 32227, 9976, 25562, 83874, 11209, 65094, 33566, 30909, 33333, 81382, 16668, 29373, 37436, 14979, 33043, 5938, 7268, 8885, 6672, 67718, 7776, 13159, 22328, 21444, 18982, 36413, 55747, 6036, 7962, 9080, 33938, 41862, 6178, 23029, 15074, 14651, 37666, 62106, 65370, 24985, 19348, 18069, 12520, 22099, 71401, 16941, 19169, 35693, 22560, 9417, 31897, 30563, 10762, 21611, 30526, 23148, 34260, 18169, 53030, 29543, 15657, 17582, 26466, 6881, 27608, 11602, 78267, 8513, 26901, 8497, 79589, 11484, 12124, 20787, 38653, 69941, 84617, 21301, 69945, 86625, 28253, 30514, 20588, 33384, 14482, 17157, 88785, 89809, 10354, 87223, 8537, 41443, 20590, 92232, 26361, 27350, 34013, 18233, 33586, 25296, 30044, 36118, 12583, 22416, 11239, 23088, 39229, 41642, 39685, 30323, 26875, 39042, 68752, 41843, 61271, 17339, 20993, 23113, 25324, 26985, 29899, 32325, 27537, 39715, 60784, 67750, 67209, 18727, 30408, 21552, 8901, 12562, 38744, 83873, 21131, 63500, 60155, 40020, 10049, 88023, 35060, 33146, 13375, 83750, 15829, 18305, 12496, 27384, 9592, 8127, 17248, 41123, 31222, 60153, 28763, 23272, 76072, 34516, 35835, 69251, 9313, 24087, 24153, 69854, 58255, 9025, 24011, 66962, 60281, 25406, 27246, 30087, 41306, 31592, 85493, 38580, 55345, 31672, 23441, 83593, 71425, 86266, 18858, 24184, 27108, 81315, 86204, 62466, 6061, 7334, 26334, 60264, 33545, 90716, 41056, 11053, 69161, 14140, 13476, 15291, 70110, 23561, 12377, 83942, 34077, 52613, 85065, 22000, 74991, 17542, 37167, 21318, 24394, 62076, 60352, 80258, 34104, 79941, 81256, 28852, 34782, 29982, 83261, 34501, 85441, 88755, 78965, 88426, 40044, 39730, 16658, 28582, 41062, 57172, 57222, 70382, 70315, 38668, 19001, 89571, 18616, 10462, 75054, 11836, 30365, 22710, 39604, 71418, 36428, 8294, 37785, 77121, 88795, 89405, 79955, 78627, 60900, 87364, 23575, 23334, 53336, 92122, 83718, 32294, 17093, 77021, 75740, 75695, 88458, 92273, 37669, 90284, 8521, 83707, 92173, 86753, 55499, 90709, 73040, 27308, 56576, 70442, 71783, 84731, 91000, 91666, 91675, 91818, 91998])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU3ZImThMLOX",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aUaeHpGezr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYswrPY67vBM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "09888f0d-43a3-4ae1-db67-93255b90f2f4"
      },
      "source": [
        "\n",
        "# Print the sample Assortment.\n",
        "print(' Assortment: ', assortment[29])\n",
        "\n",
        "# Print the corresponding label set.\n",
        "print('Labels: ', labels[29])\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Assortment:  [4592, 4472, 3872, 3889, 4537, 4624, 4652, 4691, 4746, 4831, 4888, 5014, 5404, 5440, 5638]\n",
            "Labels:  [1, 2, 3, 2, 2, 2, 1, 4, 6, 1, 1, 2, 1, 1, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfK5SPhDsiyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfVH__-AyYZZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "302ad391-9d9c-406b-ff7c-a4a98a142274"
      },
      "source": [
        "  import numpy as np\n",
        "# ass=np.concatenate(assortment).astype(None)\n",
        "assortment"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([4059, 5293]), list([4059]), list([58675]), ...,\n",
              "       list([6678, 18151, 38263, 25715, 29658, 32779, 35577, 53098, 13160, 25814, 8282, 15137, 28845, 32710, 36751, 23070, 8093, 9070, 10425, 16016, 18006, 20974, 40584, 41176, 7700, 15398, 35460, 62070, 16864, 22061, 24467, 30630, 53125, 71200, 11259, 12657, 16607, 32073, 39228, 24251, 41194, 59493, 35574, 38616, 39582, 8723, 9206, 31145, 36965, 26316, 12238, 18194, 20654, 24951, 26209, 33726, 6689, 37475, 13068, 21570, 22187, 30716, 19223, 12168, 32045, 61224, 7113, 21816, 21172, 72641, 15431, 32979, 16656, 30576, 20441, 34160, 23373, 52957, 17111, 30648, 9139, 20879, 25096, 11868, 21968, 30991, 38688, 27918, 32949, 75495, 27441, 31193, 16119, 41694, 11514, 16040, 24962, 14938, 28694, 20490, 21352, 52608, 7711, 25098, 32588, 40155, 22717, 38365, 75312, 25591, 33553, 34645, 15051, 38632, 38878, 69964, 33043, 67228, 20351, 5938, 14808, 24745, 10759, 41180, 15839, 30871, 33968, 19771, 39012, 53992, 6566, 19088, 36413, 59802, 14249, 15897, 16914, 23029, 37315, 40005, 31841, 60797, 15074, 62106, 27689, 15586, 22237, 7840, 14280, 32028, 33918, 37330, 39122, 28543, 38277, 14333, 11530, 9197, 12315, 21074, 13695, 7630, 40719, 52621, 15609, 34260, 35916, 37084, 60244, 70053, 84176, 26466, 27032, 30597, 35751, 52931, 57151, 14047, 17941, 28859, 58192, 7825, 6494, 20039, 40703, 75812, 84617, 18924, 28101, 28253, 27643, 69924, 54465, 11296, 39166, 29172, 18456, 14482, 25509, 90684, 19379, 81357, 5899, 40184, 15459, 36692, 92326, 31392, 25595, 22456, 34013, 55408, 16301, 29117, 15214, 24680, 17488, 14783, 31622, 28866, 7338, 31436, 68632, 78785, 36104, 38206, 23085, 30408, 26229, 60680, 29785, 36933, 39083, 84985, 20862, 14415, 87889, 26382, 17959, 62546, 90192, 67229, 83234, 38424, 13375, 5838, 54075, 74874, 19844, 15588, 39672, 71049, 24364, 27384, 26818, 60147, 28245, 40575, 54121, 10107, 22897, 20790, 78547, 13166, 38582, 38938, 71981, 36246, 10705, 39893, 9718, 73762, 19189, 27297, 80511, 55662, 21333, 17075, 33179, 14648, 37853, 33214, 78817, 34917, 10744, 84289, 38580, 64238, 83593, 59366, 53205, 86266, 88030, 71424, 84228, 40592, 87331, 6604, 19744, 19923, 92062, 92080, 92078, 77457, 91539, 38524, 35563, 15411, 60138, 7625, 20501, 34756, 29291, 86301, 19489, 23605, 54188, 18485, 38163, 41402, 57874, 34489, 18660, 85895, 67242, 89903, 15524, 40856, 57120, 53103, 32751, 25653, 12803, 9571, 15942, 8291, 91796, 81256, 15175, 25559, 84484, 21262, 83859, 10998, 14770, 27952, 60531, 61364, 57222, 54340, 83730, 21323, 79350, 77657, 82227, 29789, 9546, 76010, 54438, 91650, 40416, 8119, 92280, 91128, 6921, 53422, 10251, 10971, 85095, 23575, 18747, 68917, 77571, 84943, 88990, 39160, 16855, 91272, 77756, 61294, 63250, 86753, 91661, 63102, 55072, 61424, 63502, 86709, 89429, 89473, 89976, 90009, 91359, 92270]),\n",
              "       list([12765, 25755, 27079, 29658, 32621, 32779, 35577, 53098, 9367, 13120, 13160, 33407, 6734, 8282, 14209, 17780, 22352, 28845, 12685, 9070, 11165, 19506, 29066, 40257, 41176, 14587, 15398, 28829, 13229, 22061, 53125, 53185, 63378, 11259, 23447, 24251, 32687, 59493, 11170, 8723, 17872, 18153, 31145, 34747, 26316, 30581, 38554, 75167, 34652, 40535, 12792, 21570, 27393, 27691, 22187, 22559, 29107, 30716, 39296, 31952, 39820, 53906, 61224, 33904, 53907, 19743, 54364, 10801, 14692, 17116, 9531, 32427, 16656, 20644, 7451, 20441, 25470, 34160, 10195, 13053, 24800, 32358, 52957, 12021, 17111, 23352, 56814, 27031, 7665, 55732, 71796, 22555, 31613, 32949, 16094, 32530, 41694, 16040, 24962, 30733, 78485, 13372, 39119, 20490, 71667, 7711, 11950, 14686, 24223, 84689, 84691, 32588, 56796, 22717, 75493, 87146, 34186, 11209, 15469, 65094, 20763, 27436, 9784, 38878, 82313, 14679, 40303, 10631, 35582, 9042, 41180, 8638, 33684, 6936, 12507, 24194, 30025, 7506, 12609, 53939, 6036, 41789, 15191, 17306, 17489, 11297, 16410, 19735, 23029, 26941, 8048, 8241, 20928, 22071, 21059, 29723, 25220, 19798, 25850, 26354, 19325, 22966, 23244, 32685, 5908, 41453, 77615, 31281, 35934, 81255, 26626, 13971, 15518, 34260, 56728, 56811, 29543, 37702, 76603, 19354, 37741, 69942, 32338, 8159, 56749, 57151, 12076, 69909, 73863, 19421, 32575, 34052, 57995, 83788, 30392, 22766, 69945, 86625, 18924, 14089, 13082, 33384, 41679, 81390, 70594, 34582, 12776, 16001, 27800, 89263, 5916, 59368, 28053, 60593, 89274, 33571, 16212, 80308, 36349, 40314, 5899, 92150, 91249, 21637, 7833, 27350, 70131, 73145, 7709, 23039, 22416, 24680, 14783, 14189, 36629, 28368, 17575, 29389, 21836, 79624, 54493, 77240, 59199, 61485, 26622, 36104, 38206, 57422, 18142, 21552, 26379, 26229, 35077, 27957, 58832, 60641, 23870, 69174, 16288, 38801, 79368, 27922, 88023, 62546, 33146, 24265, 15829, 91932, 92083, 8506, 60149, 18305, 40877, 12757, 31746, 19202, 17325, 20801, 60323, 71049, 68837, 15517, 27384, 36558, 30237, 17248, 38874, 23272, 29619, 29330, 10107, 20207, 78547, 37529, 33210, 38957, 70469, 7610, 37594, 11136, 26922, 29334, 75036, 69975, 81419, 15979, 55421, 84040, 31544, 34917, 10286, 85544, 55345, 82384, 29439, 70593, 16925, 76462, 89785, 85102, 70349, 19646, 65473, 38625, 30635, 38583, 59104, 91822, 54347, 54505, 6680, 38524, 52734, 64225, 7637, 70417, 27340, 15627, 8926, 8332, 40635, 19489, 77235, 77456, 7758, 14644, 10800, 67744, 34811, 24159, 37958, 26108, 63894, 40338, 39876, 20045, 85895, 85801, 90828, 66654, 21033, 58898, 59239, 62844, 38038, 16357, 32024, 57120, 80680, 81976, 22214, 87149, 40096, 60352, 60604, 55291, 41763, 73729, 32387, 83946, 11804, 12595, 34104, 33051, 55913, 25559, 11347, 27325, 85502, 21262, 17357, 92099, 90984, 19772, 41631, 14770, 27952, 59639, 34200, 87413, 57172, 57222, 57182, 57217, 10408, 63599, 86591, 88942, 64460, 25515, 24028, 69453, 34638, 11731, 38668, 19001, 62537, 31931, 90112, 7836, 69516, 86986, 20983, 13812, 12308, 19138, 31393, 60115, 75598, 77220, 63058, 81304, 91128, 15526, 26514, 59827, 83094, 28028, 62817, 34609, 91966, 23575, 81781, 88214, 53033, 83168, 24161, 31074, 59524, 41860, 87021, 9662, 90634, 58369, 33437, 67004, 89041, 85505, 63250, 91535, 11450, 14539, 58798, 62951, 66002, 73040, 74373, 76730, 83264, 88484, 91341, 91718, 92012, 92191, 92311]),\n",
              "       list([6678, 18151, 38263, 25755, 29658, 35577, 37360, 9960, 13160, 25814, 30032, 31164, 10293, 17780, 8093, 10425, 16016, 20974, 26997, 28222, 14587, 10772, 11892, 24467, 30630, 34023, 19758, 10656, 23014, 24272, 26674, 24251, 11170, 23769, 66496, 8723, 26870, 35038, 36965, 37148, 60652, 33297, 9747, 12238, 24951, 26209, 28044, 30581, 33928, 18295, 6355, 17295, 19131, 26516, 9315, 11794, 22187, 40120, 21503, 37387, 14501, 19307, 19743, 41001, 16144, 15431, 21050, 33685, 7629, 30576, 33751, 63318, 7451, 8491, 20441, 21130, 79347, 18423, 23352, 15260, 25096, 24054, 34194, 11769, 20173, 37266, 31613, 34764, 53059, 32935, 36789, 53111, 22152, 31328, 41694, 59063, 83182, 34732, 13372, 41538, 8088, 12272, 20698, 24933, 20183, 70439, 25098, 12738, 32588, 12805, 21638, 32227, 9976, 25562, 83874, 11209, 65094, 33566, 30909, 33333, 81382, 16668, 29373, 37436, 14979, 33043, 5938, 7268, 8885, 6672, 67718, 7776, 13159, 22328, 21444, 18982, 36413, 55747, 6036, 7962, 9080, 33938, 41862, 6178, 23029, 15074, 14651, 37666, 62106, 65370, 24985, 19348, 18069, 12520, 22099, 71401, 16941, 19169, 35693, 22560, 9417, 31897, 30563, 10762, 21611, 30526, 23148, 34260, 18169, 53030, 29543, 15657, 17582, 26466, 6881, 27608, 11602, 78267, 8513, 26901, 8497, 79589, 11484, 12124, 20787, 38653, 69941, 84617, 21301, 69945, 86625, 28253, 30514, 20588, 33384, 14482, 17157, 88785, 89809, 10354, 87223, 8537, 41443, 20590, 92232, 26361, 27350, 34013, 18233, 33586, 25296, 30044, 36118, 12583, 22416, 11239, 23088, 39229, 41642, 39685, 30323, 26875, 39042, 68752, 41843, 61271, 17339, 20993, 23113, 25324, 26985, 29899, 32325, 27537, 39715, 60784, 67750, 67209, 18727, 30408, 21552, 8901, 12562, 38744, 83873, 21131, 63500, 60155, 40020, 10049, 88023, 35060, 33146, 13375, 83750, 15829, 18305, 12496, 27384, 9592, 8127, 17248, 41123, 31222, 60153, 28763, 23272, 76072, 34516, 35835, 69251, 9313, 24087, 24153, 69854, 58255, 9025, 24011, 66962, 60281, 25406, 27246, 30087, 41306, 31592, 85493, 38580, 55345, 31672, 23441, 83593, 71425, 86266, 18858, 24184, 27108, 81315, 86204, 62466, 6061, 7334, 26334, 60264, 33545, 90716, 41056, 11053, 69161, 14140, 13476, 15291, 70110, 23561, 12377, 83942, 34077, 52613, 85065, 22000, 74991, 17542, 37167, 21318, 24394, 62076, 60352, 80258, 34104, 79941, 81256, 28852, 34782, 29982, 83261, 34501, 85441, 88755, 78965, 88426, 40044, 39730, 16658, 28582, 41062, 57172, 57222, 70382, 70315, 38668, 19001, 89571, 18616, 10462, 75054, 11836, 30365, 22710, 39604, 71418, 36428, 8294, 37785, 77121, 88795, 89405, 79955, 78627, 60900, 87364, 23575, 23334, 53336, 92122, 83718, 32294, 17093, 77021, 75740, 75695, 88458, 92273, 37669, 90284, 8521, 83707, 92173, 86753, 55499, 90709, 73040, 27308, 56576, 70442, 71783, 84731, 91000, 91666, 91675, 91818, 91998])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr_gzIQgTnpe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "9b19e707-4f5b-49c6-bd5d-b0e9f2157785"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "MAX_LEN = 400\n",
        "\n",
        "print('\\nPadding/truncating all assortments to %d values...' % MAX_LEN)\n",
        "product_tokens= pad_sequences(assortment, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "demand_labels= pad_sequences(labels, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all assortments to 400 values...\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAERCjQgmoWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attention_masks = []\n",
        "\n",
        "# For each assortment...\n",
        "for sent in product_tokens:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZZ5ouPMmI0S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "5cb55f0b-18e4-4fd2-bafc-f88fc4afd5f1"
      },
      "source": [
        "\n",
        "\n",
        "trans['PRODUCT_ID']"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                         [480014, 718226]\n",
              "1                                                 [480014]\n",
              "2                                                [6903760]\n",
              "3                                                 [702733]\n",
              "4                                                 [721164]\n",
              "                               ...                        \n",
              "13932    [926905, 1106523, 879755, 994928, 995242, 1029...\n",
              "13933    [926905, 1106523, 995242, 1006878, 1029743, 10...\n",
              "13934    [826249, 926905, 1106523, 994928, 1029743, 105...\n",
              "13935    [879755, 995242, 1006878, 1029743, 1055863, 10...\n",
              "13936    [826249, 926905, 1106523, 995242, 1029743, 108...\n",
              "Name: PRODUCT_ID, Length: 13937, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk2Z7ohDUS7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(product_tokens, demand_labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbQZo0z13GUg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "4f8bbf71-3edc-47ee-ddc8-7249e5abce0c"
      },
      "source": [
        "product_tokens"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 4059,  5293,     0, ...,     0,     0,     0],\n",
              "       [ 4059,     0,     0, ...,     0,     0,     0],\n",
              "       [58675,     0,     0, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [ 6678, 18151, 38263, ...,     0,     0,     0],\n",
              "       [12765, 25755, 27079, ..., 59639, 34200, 87413],\n",
              "       [ 6678, 18151, 38263, ..., 91675, 91818, 91998]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-2L2T05Uiuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSKuy8Q8Us4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "batch_size = 64\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEazFjx2n8vS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6179b3b3-853f-44d3-c030-3dd265c4c8f1"
      },
      "source": [
        "x=np.arange(10)\n",
        "y=torch.tensor(x)\n",
        "y.view(1,-1)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aui6PC16eDRT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "a1b91e8d-8104-48fa-854a-0a7c7e3a5511"
      },
      "source": [
        "# from modeling_bert import ProductBert\n",
        "# model = ProductBert.from_pretrained(\n",
        "#     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "#     num_labels = 1 # The number of output labels--2 for binary classification.\n",
        "#     #                 # You can increase this for multi-class tasks.   \n",
        "#     # output_attentions = False, # Whether the model returns attentions weights.\n",
        "#     # output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        "# )\n",
        "from modeling_bert import BertForTokenClassifications,BertConfig\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "\n",
        "configuration = BertConfig()\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',num_labels=1)\n",
        "model = BertForTokenClassifications(configuration)\n",
        "\n",
        "input_ids = torch.tensor(assortment[11]).unsqueeze(0)  # Batch size 1\n",
        "labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1\n",
        "print(input_ids)\n",
        "print(labels)\n",
        "print(input_ids.size(1))\n",
        "\n",
        "outputs = model(input_ids=input_ids, labels=labels)\n",
        "\n",
        "labels=torch.tensor([([1]*400)])\n",
        "for step, batch in enumerate(train_dataloader):\n",
        "  print(batch[2].size(0))\n",
        "  print(batch[2])\n",
        "  print(labels)\n",
        "  outputs = model(input_ids=batch[0],token_type_ids=None, attention_mask=batch[1], labels=labels)\n",
        "  break\n",
        "outputs\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[5189, 5238, 5379, 5595]])\n",
            "tensor([[1, 1, 1, 1]])\n",
            "4\n",
            "64\n",
            "tensor([[1, 6, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 3, 1,  ..., 0, 0, 0],\n",
            "        [1, 2, 1,  ..., 0, 0, 0]])\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlkoT1tQ-qoG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fda59a02-0aac-41a6-d76e-3edb6434061e"
      },
      "source": [
        "model.cuda()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassifications(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(92353, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2wstGWDWWts",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "6ffd7fe3-3216-4d0d-f978-a4bf57a6f1bc"
      },
      "source": [
        "params = list(model.named_parameters())\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (92353, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (1, 768)\n",
            "classifier.bias                                                 (1,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjFxDXGgY94m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import  AdamW\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1qru2H6Zt1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 50\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI4oCie7Z2V1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GMtbvVAaNoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JctZFMcaiqd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "d2a32a93-7f58-4f4a-ce96-e35fdd28194c"
      },
      "source": [
        "import random\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        " \n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "    \n",
        "        loss = outputs\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "    \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # # ========================================\n",
        "    # #               Validation\n",
        "    # # ========================================\n",
        "    # # After the completion of each training epoch, measure our performance on\n",
        "    # # our validation set.\n",
        "\n",
        "    # print(\"\")\n",
        "    # print(\"Running Validation...\")\n",
        "\n",
        "    # t0 = time.time()\n",
        "\n",
        "    # # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # # during evaluation.\n",
        "    # model.eval()\n",
        "\n",
        "    # # Tracking variables \n",
        "    # eval_loss, eval_accuracy = 0, 0\n",
        "    # nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # # Evaluate data for one epoch\n",
        "    # for batch in validation_dataloader:\n",
        "        \n",
        "    #     # Add batch to GPU\n",
        "    #     batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "    #     # Unpack the inputs from our dataloader\n",
        "    #     b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "    #     # Telling the model not to compute or store gradients, saving memory and\n",
        "    #     # speeding up validation\n",
        "    #     with torch.no_grad():        \n",
        "\n",
        "    #         outputs = model(b_input_ids[0], \n",
        "    #                         token_type_ids=None, \n",
        "    #                         attention_mask=b_input_mask)\n",
        "        \n",
        "    #     # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "    #     # values prior to applying an activation function like the softmax.\n",
        "    #     logits = outputs[0]\n",
        "\n",
        "    #     # Move logits and labels to CPU\n",
        "    #     logits = logits.detach().cpu().numpy()\n",
        "    #     label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "    #     # Calculate the accuracy for this batch of test sentences.\n",
        "    #     tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "    #     # Accumulate the total accuracy.\n",
        "    #     eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    #     # Track the number of batches\n",
        "    #     nb_eval_steps += 1\n",
        "\n",
        "    # # Report the final accuracy for this validation run.\n",
        "    # print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    # print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-24df857f166b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_input_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                     labels=b_labels)\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/transformers/src/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions)\u001b[0m\n\u001b[1;32m   1491\u001b[0m                 \u001b[0mactive_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactive_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m                 \u001b[0mactive_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactive_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1493\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactive_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1494\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m               \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loss_fct' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLmlIqLubN7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyvCNKLibcF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}