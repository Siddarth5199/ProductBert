{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "LorELVXBcR4z",
    "outputId": "c64d0955-7aa4-4141-fb29-efb7b81f08f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 GPU(s) available.\n",
      "We will use the GPU:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:')\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "colab_type": "code",
    "id": "XiqZRpb560Et",
    "outputId": "866496b8-3a54-4ccf-a516-8b7d3b0bc5eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transactions: 2,359,168\n",
      "\n",
      "Number of products: 92,353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "trans = pd.read_csv(\"DATA/transaction_final.csv\")\n",
    "product=pd.read_csv(\"DATA/products_final.csv\")\n",
    "\n",
    "trans_products=trans.merge(product,on='PRODUCT_ID',how='inner')\n",
    "trans_products=trans_products[trans_products['SUB_COMMODITY_DESC']!='GASOLINE-REG UNLEADED']\n",
    "\n",
    "\n",
    "# Report the number of transactions & products.\n",
    "print('Number of transactions: {:,}\\n'.format(trans.shape[0]))\n",
    "print('Number of products: {:,}\\n'.format(product.shape[0]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORE_ID</th>\n",
       "      <th>WEEK_NO</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>WEEK_QUANTITY</th>\n",
       "      <th>INDEX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2038949</th>\n",
       "      <td>335</td>\n",
       "      <td>72</td>\n",
       "      <td>10456152</td>\n",
       "      <td>1</td>\n",
       "      <td>73671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402921</th>\n",
       "      <td>389</td>\n",
       "      <td>13</td>\n",
       "      <td>1038998</td>\n",
       "      <td>1</td>\n",
       "      <td>30719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061400</th>\n",
       "      <td>309</td>\n",
       "      <td>55</td>\n",
       "      <td>1070820</td>\n",
       "      <td>1</td>\n",
       "      <td>34260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502216</th>\n",
       "      <td>367</td>\n",
       "      <td>71</td>\n",
       "      <td>1093603</td>\n",
       "      <td>1</td>\n",
       "      <td>36818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544269</th>\n",
       "      <td>346</td>\n",
       "      <td>43</td>\n",
       "      <td>944466</td>\n",
       "      <td>1</td>\n",
       "      <td>20173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1784268</th>\n",
       "      <td>340</td>\n",
       "      <td>73</td>\n",
       "      <td>9677880</td>\n",
       "      <td>1</td>\n",
       "      <td>69159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987327</th>\n",
       "      <td>298</td>\n",
       "      <td>60</td>\n",
       "      <td>947201</td>\n",
       "      <td>1</td>\n",
       "      <td>20469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242460</th>\n",
       "      <td>384</td>\n",
       "      <td>61</td>\n",
       "      <td>880969</td>\n",
       "      <td>1</td>\n",
       "      <td>12897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853997</th>\n",
       "      <td>362</td>\n",
       "      <td>95</td>\n",
       "      <td>15716676</td>\n",
       "      <td>2</td>\n",
       "      <td>87947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122429</th>\n",
       "      <td>445</td>\n",
       "      <td>48</td>\n",
       "      <td>878996</td>\n",
       "      <td>1</td>\n",
       "      <td>12685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676147</th>\n",
       "      <td>367</td>\n",
       "      <td>60</td>\n",
       "      <td>868546</td>\n",
       "      <td>1</td>\n",
       "      <td>11527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2132601</th>\n",
       "      <td>427</td>\n",
       "      <td>13</td>\n",
       "      <td>5566905</td>\n",
       "      <td>2</td>\n",
       "      <td>52735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435218</th>\n",
       "      <td>292</td>\n",
       "      <td>85</td>\n",
       "      <td>920654</td>\n",
       "      <td>1</td>\n",
       "      <td>17394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291141</th>\n",
       "      <td>445</td>\n",
       "      <td>10</td>\n",
       "      <td>5592329</td>\n",
       "      <td>1</td>\n",
       "      <td>54492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860791</th>\n",
       "      <td>32004</td>\n",
       "      <td>19</td>\n",
       "      <td>8090532</td>\n",
       "      <td>1</td>\n",
       "      <td>62444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         STORE_ID  WEEK_NO  PRODUCT_ID  WEEK_QUANTITY  INDEX\n",
       "2038949       335       72    10456152              1  73671\n",
       "1402921       389       13     1038998              1  30719\n",
       "1061400       309       55     1070820              1  34260\n",
       "1502216       367       71     1093603              1  36818\n",
       "544269        346       43      944466              1  20173\n",
       "1784268       340       73     9677880              1  69159\n",
       "987327        298       60      947201              1  20469\n",
       "1242460       384       61      880969              1  12897\n",
       "1853997       362       95    15716676              2  87947\n",
       "122429        445       48      878996              1  12685\n",
       "676147        367       60      868546              1  11527\n",
       "2132601       427       13     5566905              2  52735\n",
       "435218        292       85      920654              1  17394\n",
       "2291141       445       10     5592329              1  54492\n",
       "860791      32004       19     8090532              1  62444"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans.sample(15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>MANUFACTURER</th>\n",
       "      <th>DEPARTMENT</th>\n",
       "      <th>BRAND</th>\n",
       "      <th>COMMODITY_DESC</th>\n",
       "      <th>SUB_COMMODITY_DESC</th>\n",
       "      <th>CURR_SIZE_OF_PRODUCT</th>\n",
       "      <th>INDEX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87009</th>\n",
       "      <td>15596030</td>\n",
       "      <td>253</td>\n",
       "      <td>NUTRITION</td>\n",
       "      <td>National</td>\n",
       "      <td>FROZEN</td>\n",
       "      <td>FROZEN ENTREES</td>\n",
       "      <td>10 OZ</td>\n",
       "      <td>87010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85329</th>\n",
       "      <td>13987205</td>\n",
       "      <td>5786</td>\n",
       "      <td>DRUG GM</td>\n",
       "      <td>National</td>\n",
       "      <td>APPAREL</td>\n",
       "      <td>WOMENS COMFORT SHOES</td>\n",
       "      <td>INFANT</td>\n",
       "      <td>85330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27447</th>\n",
       "      <td>1010107</td>\n",
       "      <td>5024</td>\n",
       "      <td>DRUG GM</td>\n",
       "      <td>National</td>\n",
       "      <td>ORAL HYGIENE PRODUCTS</td>\n",
       "      <td>MOUTHWASH RINSES AND SPRAYS</td>\n",
       "      <td>24 OZ</td>\n",
       "      <td>27448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40687</th>\n",
       "      <td>1128582</td>\n",
       "      <td>2021</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>National</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>ORIENTAL OTHER SAUCES MARINAD</td>\n",
       "      <td>9.85 OZ</td>\n",
       "      <td>40688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82160</th>\n",
       "      <td>13190557</td>\n",
       "      <td>693</td>\n",
       "      <td>DRUG GM</td>\n",
       "      <td>National</td>\n",
       "      <td>CANDY - PACKAGED</td>\n",
       "      <td>CANDY BARS (SINGLES)(INCLUDING</td>\n",
       "      <td></td>\n",
       "      <td>82161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70005</th>\n",
       "      <td>9836559</td>\n",
       "      <td>1838</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>National</td>\n",
       "      <td>BAKED BREAD/BUNS/ROLLS</td>\n",
       "      <td>MAINSTREAM WHEAT/MULTIGRAIN BR</td>\n",
       "      <td>20 OZ</td>\n",
       "      <td>70006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40952</th>\n",
       "      <td>1130969</td>\n",
       "      <td>4974</td>\n",
       "      <td>DRUG GM</td>\n",
       "      <td>National</td>\n",
       "      <td>FOOT CARE PRODUCTS</td>\n",
       "      <td>FOOT SPRAYS POWDERS</td>\n",
       "      <td></td>\n",
       "      <td>40953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22739</th>\n",
       "      <td>967461</td>\n",
       "      <td>1142</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>National</td>\n",
       "      <td>FROZEN PIZZA</td>\n",
       "      <td>SNACKS/APPETIZERS</td>\n",
       "      <td>11 OZ</td>\n",
       "      <td>22740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30873</th>\n",
       "      <td>1040365</td>\n",
       "      <td>3018</td>\n",
       "      <td>MEAT</td>\n",
       "      <td>National</td>\n",
       "      <td>PORK</td>\n",
       "      <td>RIBS - COUNTRY/WESTERN STYLE</td>\n",
       "      <td></td>\n",
       "      <td>30874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63941</th>\n",
       "      <td>8209544</td>\n",
       "      <td>5103</td>\n",
       "      <td>DRUG GM</td>\n",
       "      <td>National</td>\n",
       "      <td>HAIR CARE PRODUCTS</td>\n",
       "      <td>SHAMPOO</td>\n",
       "      <td>13 OZ</td>\n",
       "      <td>63942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47478</th>\n",
       "      <td>1826563</td>\n",
       "      <td>69</td>\n",
       "      <td>DRUG GM</td>\n",
       "      <td>Private</td>\n",
       "      <td>KITCHEN GADGETS</td>\n",
       "      <td>GADGETS/TOOLS</td>\n",
       "      <td>255182 4PC</td>\n",
       "      <td>47479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73113</th>\n",
       "      <td>10341708</td>\n",
       "      <td>1803</td>\n",
       "      <td>NUTRITION</td>\n",
       "      <td>National</td>\n",
       "      <td>FROZEN</td>\n",
       "      <td>FROZEN ICE CREAM</td>\n",
       "      <td>3/3 OZ</td>\n",
       "      <td>73114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34287</th>\n",
       "      <td>1071107</td>\n",
       "      <td>69</td>\n",
       "      <td>COSMETICS</td>\n",
       "      <td>Private</td>\n",
       "      <td>MAKEUP AND TREATMENT</td>\n",
       "      <td>IMPLEMENTS SETS</td>\n",
       "      <td>2 OZ</td>\n",
       "      <td>34288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29594</th>\n",
       "      <td>1029287</td>\n",
       "      <td>1282</td>\n",
       "      <td>DRUG GM</td>\n",
       "      <td>National</td>\n",
       "      <td>BABY FOODS</td>\n",
       "      <td>BABY FOOD - BEGINNER</td>\n",
       "      <td>4 OZ</td>\n",
       "      <td>29595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69807</th>\n",
       "      <td>9829093</td>\n",
       "      <td>693</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>National</td>\n",
       "      <td>SYRUPS/TOPPINGS</td>\n",
       "      <td>CHOCOLATE SYRUP</td>\n",
       "      <td>24 OZ</td>\n",
       "      <td>69808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PRODUCT_ID  MANUFACTURER DEPARTMENT     BRAND          COMMODITY_DESC  \\\n",
       "87009    15596030           253  NUTRITION  National                  FROZEN   \n",
       "85329    13987205          5786    DRUG GM  National                 APPAREL   \n",
       "27447     1010107          5024    DRUG GM  National   ORAL HYGIENE PRODUCTS   \n",
       "40687     1128582          2021    GROCERY  National                HISPANIC   \n",
       "82160    13190557           693    DRUG GM  National        CANDY - PACKAGED   \n",
       "70005     9836559          1838    GROCERY  National  BAKED BREAD/BUNS/ROLLS   \n",
       "40952     1130969          4974    DRUG GM  National      FOOT CARE PRODUCTS   \n",
       "22739      967461          1142    GROCERY  National            FROZEN PIZZA   \n",
       "30873     1040365          3018       MEAT  National                    PORK   \n",
       "63941     8209544          5103    DRUG GM  National      HAIR CARE PRODUCTS   \n",
       "47478     1826563            69    DRUG GM   Private         KITCHEN GADGETS   \n",
       "73113    10341708          1803  NUTRITION  National                  FROZEN   \n",
       "34287     1071107            69  COSMETICS   Private    MAKEUP AND TREATMENT   \n",
       "29594     1029287          1282    DRUG GM  National              BABY FOODS   \n",
       "69807     9829093           693    GROCERY  National         SYRUPS/TOPPINGS   \n",
       "\n",
       "                   SUB_COMMODITY_DESC CURR_SIZE_OF_PRODUCT  INDEX  \n",
       "87009                  FROZEN ENTREES                10 OZ  87010  \n",
       "85329            WOMENS COMFORT SHOES               INFANT  85330  \n",
       "27447     MOUTHWASH RINSES AND SPRAYS                24 OZ  27448  \n",
       "40687   ORIENTAL OTHER SAUCES MARINAD              9.85 OZ  40688  \n",
       "82160  CANDY BARS (SINGLES)(INCLUDING                       82161  \n",
       "70005  MAINSTREAM WHEAT/MULTIGRAIN BR                20 OZ  70006  \n",
       "40952             FOOT SPRAYS POWDERS                       40953  \n",
       "22739               SNACKS/APPETIZERS                11 OZ  22740  \n",
       "30873    RIBS - COUNTRY/WESTERN STYLE                       30874  \n",
       "63941                         SHAMPOO                13 OZ  63942  \n",
       "47478                   GADGETS/TOOLS           255182 4PC  47479  \n",
       "73113                FROZEN ICE CREAM               3/3 OZ  73114  \n",
       "34287                 IMPLEMENTS SETS                 2 OZ  34288  \n",
       "29594            BABY FOOD - BEGINNER                 4 OZ  29595  \n",
       "69807                 CHOCOLATE SYRUP                24 OZ  69808  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product.sample(15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GROCERY' 'MISC. TRANS.' 'PASTRY' 'DRUG GM' 'MEAT-PCKGD' 'SEAFOOD-PCKGD'\n",
      " 'PRODUCE' 'NUTRITION' 'DELI' 'COSMETICS' 'MEAT' 'FLORAL'\n",
      " 'TRAVEL & LEISUR' 'SEAFOOD' 'MISC SALES TRAN' 'SALAD BAR' 'KIOSK-GAS'\n",
      " 'ELECT &PLUMBING' 'GRO BAKERY' 'GM MERCH EXP' 'FROZEN GROCERY'\n",
      " 'COUP/STR & MFG' 'SPIRITS' 'GARDEN CENTER' 'TOYS' 'CHARITABLE CONT'\n",
      " 'RESTAURANT' 'RX' 'PROD-WHS SALES' 'MEAT-WHSE' 'DAIRY DELI' 'CHEF SHOPPE'\n",
      " 'HBC' 'DELI/SNACK BAR' 'PORK' 'AUTOMOTIVE' 'VIDEO RENTAL' ' '\n",
      " 'CNTRL/STORE SUP' 'HOUSEWARES' 'POSTAL CENTER' 'PHOTO' 'VIDEO'\n",
      " 'PHARMACY SUPPLY']\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "print(product['DEPARTMENT'].unique())\n",
    "print(len(product['DEPARTMENT'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582\n"
     ]
    }
   ],
   "source": [
    "print(len(trans['STORE_ID'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6476\n"
     ]
    }
   ],
   "source": [
    "print(len(product['MANUFACTURER'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92353\n"
     ]
    }
   ],
   "source": [
    "print(len(product['PRODUCT_ID'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308\n"
     ]
    }
   ],
   "source": [
    "print(len(product['COMMODITY_DESC'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2383\n"
     ]
    }
   ],
   "source": [
    "print(len(product['SUB_COMMODITY_DESC'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assortments=trans_products.groupby(['STORE_ID','WEEK_NO','DEPARTMENT']).agg(lambda x:list(x)).reset_index()\n",
    "assortments['SIZE']=assortments['PRODUCT_ID'].apply(lambda x:len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "CUkttVxA7cvb",
    "outputId": "01e373d0-9917-4b71-ddad-a1626604492d"
   },
   "outputs": [],
   "source": [
    "labels = assortments.WEEK_QUANTITY.values\n",
    "assortment = assortments.INDEX_x.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORE_ID</th>\n",
       "      <th>WEEK_NO</th>\n",
       "      <th>DEPARTMENT</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>WEEK_QUANTITY</th>\n",
       "      <th>INDEX_x</th>\n",
       "      <th>MANUFACTURER</th>\n",
       "      <th>BRAND</th>\n",
       "      <th>COMMODITY_DESC</th>\n",
       "      <th>SUB_COMMODITY_DESC</th>\n",
       "      <th>CURR_SIZE_OF_PRODUCT</th>\n",
       "      <th>INDEX_y</th>\n",
       "      <th>SIZE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>MISC SALES TRAN</td>\n",
       "      <td>[718226]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[5293]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[National]</td>\n",
       "      <td>[COUPON/MISC ITEMS]</td>\n",
       "      <td>[MISC SALES TRANS]</td>\n",
       "      <td>[ ]</td>\n",
       "      <td>[5293]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>DRUG GM</td>\n",
       "      <td>[6903760]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[58675]</td>\n",
       "      <td>[69]</td>\n",
       "      <td>[Private]</td>\n",
       "      <td>[LAXATIVES]</td>\n",
       "      <td>[LAXATIVES]</td>\n",
       "      <td>[3OZ 211565]</td>\n",
       "      <td>[58675]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>DRUG GM</td>\n",
       "      <td>[702733]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[5223]</td>\n",
       "      <td>[487]</td>\n",
       "      <td>[National]</td>\n",
       "      <td>[ELECTRICAL SUPPPLIES]</td>\n",
       "      <td>[FINISHED ELECTRICAL]</td>\n",
       "      <td>[ ]</td>\n",
       "      <td>[5223]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>93</td>\n",
       "      <td>FLORAL</td>\n",
       "      <td>[721164]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[5304]</td>\n",
       "      <td>[3100]</td>\n",
       "      <td>[National]</td>\n",
       "      <td>[FLORAL-FLOWERING PLANTS]</td>\n",
       "      <td>[PREMIUM FLOWERING PLANTS]</td>\n",
       "      <td>[6.5 INCH]</td>\n",
       "      <td>[5304]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>PRODUCE</td>\n",
       "      <td>[480415, 714433, 772976]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[4063, 5280, 5589]</td>\n",
       "      <td>[529, 2, 2]</td>\n",
       "      <td>[National, National, National]</td>\n",
       "      <td>[VEGETABLES SALAD, TROPICAL FRUIT, CITRUS]</td>\n",
       "      <td>[VARIETY LETTUCE, BANANAS, GRAPEFRUIT]</td>\n",
       "      <td>[12 CT, 40 LB, 36 CT]</td>\n",
       "      <td>[4063, 5280, 5589]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114635</th>\n",
       "      <td>34280</td>\n",
       "      <td>102</td>\n",
       "      <td>PASTRY</td>\n",
       "      <td>[827999, 956812, 1009449]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[6881, 21552, 27384]</td>\n",
       "      <td>[69, 1885, 69]</td>\n",
       "      <td>[Private, National, Private]</td>\n",
       "      <td>[COOKIES, ROLLS, BREAD]</td>\n",
       "      <td>[COOKIES: REGULAR, ROLLS: DINNER, BREAD:ITALIA...</td>\n",
       "      <td>[ , 12 OZ,  ]</td>\n",
       "      <td>[6881, 21552, 27384]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114636</th>\n",
       "      <td>34280</td>\n",
       "      <td>102</td>\n",
       "      <td>PRODUCE</td>\n",
       "      <td>[1082185, 854852, 995785, 1033142, 1006184, 96...</td>\n",
       "      <td>[5, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, ...</td>\n",
       "      <td>[35577, 9960, 25814, 30032, 26997, 23014, 2687...</td>\n",
       "      <td>[2, 2, 2, 2, 1646, 673, 69, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[National, National, National, National, Natio...</td>\n",
       "      <td>[TROPICAL FRUIT, TOMATOES, PEPPERS-ALL, ONIONS...</td>\n",
       "      <td>[BANANAS, TOMATOES HOTHOUSE ON THE VINE, PEPPE...</td>\n",
       "      <td>[40 LB, 13 LB, 48-54 CT, 40 LB, 12 OZ, 2 LB, 5...</td>\n",
       "      <td>[35577, 9960, 25814, 30032, 26997, 23014, 2687...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114637</th>\n",
       "      <td>34280</td>\n",
       "      <td>102</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>[16728894, 13910157]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[90284, 84731]</td>\n",
       "      <td>[3348, 3804]</td>\n",
       "      <td>[National, National]</td>\n",
       "      <td>[SEAFOOD - FROZEN, SEAFOOD - FROZEN]</td>\n",
       "      <td>[SEAFOOD-FRZ-RW-ALL, SEAFOOD-FRZ-RW-ALL]</td>\n",
       "      <td>[ ,  ]</td>\n",
       "      <td>[90284, 84731]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114638</th>\n",
       "      <td>34280</td>\n",
       "      <td>102</td>\n",
       "      <td>SEAFOOD-PCKGD</td>\n",
       "      <td>[7442353, 981165, 13417960]</td>\n",
       "      <td>[1, 2, 1]</td>\n",
       "      <td>[61271, 24184, 83261]</td>\n",
       "      <td>[1087, 914, 69]</td>\n",
       "      <td>[National, National, Private]</td>\n",
       "      <td>[FROZEN - BOXED(GROCERY), SEAFOOD - FROZEN, FR...</td>\n",
       "      <td>[FRZN BRD WHOLE FILLETS, SEAFOOD-FRZ-IQF RAW S...</td>\n",
       "      <td>[18.2OZ, 12 OZ, 4 OZ]</td>\n",
       "      <td>[61271, 24184, 83261]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114639</th>\n",
       "      <td>34280</td>\n",
       "      <td>102</td>\n",
       "      <td>SPIRITS</td>\n",
       "      <td>[13512608]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[83707]</td>\n",
       "      <td>[2518]</td>\n",
       "      <td>[National]</td>\n",
       "      <td>[LIQUOR]</td>\n",
       "      <td>[LIQUEURS/SPECIALTIES (42 UNDER]</td>\n",
       "      <td>[750 ML]</td>\n",
       "      <td>[83707]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114640 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        STORE_ID  WEEK_NO       DEPARTMENT  \\\n",
       "0              1        5  MISC SALES TRAN   \n",
       "1              1       14          DRUG GM   \n",
       "2              2       41          DRUG GM   \n",
       "3              2       93           FLORAL   \n",
       "4              2      100          PRODUCE   \n",
       "...          ...      ...              ...   \n",
       "114635     34280      102           PASTRY   \n",
       "114636     34280      102          PRODUCE   \n",
       "114637     34280      102          SEAFOOD   \n",
       "114638     34280      102    SEAFOOD-PCKGD   \n",
       "114639     34280      102          SPIRITS   \n",
       "\n",
       "                                               PRODUCT_ID  \\\n",
       "0                                                [718226]   \n",
       "1                                               [6903760]   \n",
       "2                                                [702733]   \n",
       "3                                                [721164]   \n",
       "4                                [480415, 714433, 772976]   \n",
       "...                                                   ...   \n",
       "114635                          [827999, 956812, 1009449]   \n",
       "114636  [1082185, 854852, 995785, 1033142, 1006184, 96...   \n",
       "114637                               [16728894, 13910157]   \n",
       "114638                        [7442353, 981165, 13417960]   \n",
       "114639                                         [13512608]   \n",
       "\n",
       "                                            WEEK_QUANTITY  \\\n",
       "0                                                     [1]   \n",
       "1                                                     [1]   \n",
       "2                                                     [1]   \n",
       "3                                                     [1]   \n",
       "4                                               [1, 1, 1]   \n",
       "...                                                   ...   \n",
       "114635                                          [1, 1, 1]   \n",
       "114636  [5, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, ...   \n",
       "114637                                             [1, 1]   \n",
       "114638                                          [1, 2, 1]   \n",
       "114639                                                [1]   \n",
       "\n",
       "                                                  INDEX_x  \\\n",
       "0                                                  [5293]   \n",
       "1                                                 [58675]   \n",
       "2                                                  [5223]   \n",
       "3                                                  [5304]   \n",
       "4                                      [4063, 5280, 5589]   \n",
       "...                                                   ...   \n",
       "114635                               [6881, 21552, 27384]   \n",
       "114636  [35577, 9960, 25814, 30032, 26997, 23014, 2687...   \n",
       "114637                                     [90284, 84731]   \n",
       "114638                              [61271, 24184, 83261]   \n",
       "114639                                            [83707]   \n",
       "\n",
       "                                             MANUFACTURER  \\\n",
       "0                                                     [4]   \n",
       "1                                                    [69]   \n",
       "2                                                   [487]   \n",
       "3                                                  [3100]   \n",
       "4                                             [529, 2, 2]   \n",
       "...                                                   ...   \n",
       "114635                                     [69, 1885, 69]   \n",
       "114636  [2, 2, 2, 2, 1646, 673, 69, 2, 2, 2, 2, 2, 2, ...   \n",
       "114637                                       [3348, 3804]   \n",
       "114638                                    [1087, 914, 69]   \n",
       "114639                                             [2518]   \n",
       "\n",
       "                                                    BRAND  \\\n",
       "0                                              [National]   \n",
       "1                                               [Private]   \n",
       "2                                              [National]   \n",
       "3                                              [National]   \n",
       "4                          [National, National, National]   \n",
       "...                                                   ...   \n",
       "114635                       [Private, National, Private]   \n",
       "114636  [National, National, National, National, Natio...   \n",
       "114637                               [National, National]   \n",
       "114638                      [National, National, Private]   \n",
       "114639                                         [National]   \n",
       "\n",
       "                                           COMMODITY_DESC  \\\n",
       "0                                     [COUPON/MISC ITEMS]   \n",
       "1                                             [LAXATIVES]   \n",
       "2                                  [ELECTRICAL SUPPPLIES]   \n",
       "3                               [FLORAL-FLOWERING PLANTS]   \n",
       "4              [VEGETABLES SALAD, TROPICAL FRUIT, CITRUS]   \n",
       "...                                                   ...   \n",
       "114635                            [COOKIES, ROLLS, BREAD]   \n",
       "114636  [TROPICAL FRUIT, TOMATOES, PEPPERS-ALL, ONIONS...   \n",
       "114637               [SEAFOOD - FROZEN, SEAFOOD - FROZEN]   \n",
       "114638  [FROZEN - BOXED(GROCERY), SEAFOOD - FROZEN, FR...   \n",
       "114639                                           [LIQUOR]   \n",
       "\n",
       "                                       SUB_COMMODITY_DESC  \\\n",
       "0                                      [MISC SALES TRANS]   \n",
       "1                                             [LAXATIVES]   \n",
       "2                                   [FINISHED ELECTRICAL]   \n",
       "3                              [PREMIUM FLOWERING PLANTS]   \n",
       "4                  [VARIETY LETTUCE, BANANAS, GRAPEFRUIT]   \n",
       "...                                                   ...   \n",
       "114635  [COOKIES: REGULAR, ROLLS: DINNER, BREAD:ITALIA...   \n",
       "114636  [BANANAS, TOMATOES HOTHOUSE ON THE VINE, PEPPE...   \n",
       "114637           [SEAFOOD-FRZ-RW-ALL, SEAFOOD-FRZ-RW-ALL]   \n",
       "114638  [FRZN BRD WHOLE FILLETS, SEAFOOD-FRZ-IQF RAW S...   \n",
       "114639                   [LIQUEURS/SPECIALTIES (42 UNDER]   \n",
       "\n",
       "                                     CURR_SIZE_OF_PRODUCT  \\\n",
       "0                                                     [ ]   \n",
       "1                                            [3OZ 211565]   \n",
       "2                                                     [ ]   \n",
       "3                                              [6.5 INCH]   \n",
       "4                                   [12 CT, 40 LB, 36 CT]   \n",
       "...                                                   ...   \n",
       "114635                                      [ , 12 OZ,  ]   \n",
       "114636  [40 LB, 13 LB, 48-54 CT, 40 LB, 12 OZ, 2 LB, 5...   \n",
       "114637                                             [ ,  ]   \n",
       "114638                              [18.2OZ, 12 OZ, 4 OZ]   \n",
       "114639                                           [750 ML]   \n",
       "\n",
       "                                                  INDEX_y  SIZE  \n",
       "0                                                  [5293]     1  \n",
       "1                                                 [58675]     1  \n",
       "2                                                  [5223]     1  \n",
       "3                                                  [5304]     1  \n",
       "4                                      [4063, 5280, 5589]     3  \n",
       "...                                                   ...   ...  \n",
       "114635                               [6881, 21552, 27384]     3  \n",
       "114636  [35577, 9960, 25814, 30032, 26997, 23014, 2687...    37  \n",
       "114637                                     [90284, 84731]     2  \n",
       "114638                              [61271, 24184, 83261]     3  \n",
       "114639                                            [83707]     1  \n",
       "\n",
       "[114640 rows x 13 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "assortments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORE_ID</th>\n",
       "      <th>WEEK_NO</th>\n",
       "      <th>DEPARTMENT</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>WEEK_QUANTITY</th>\n",
       "      <th>INDEX_x</th>\n",
       "      <th>MANUFACTURER</th>\n",
       "      <th>BRAND</th>\n",
       "      <th>COMMODITY_DESC</th>\n",
       "      <th>SUB_COMMODITY_DESC</th>\n",
       "      <th>CURR_SIZE_OF_PRODUCT</th>\n",
       "      <th>INDEX_y</th>\n",
       "      <th>SIZE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3506</th>\n",
       "      <td>292</td>\n",
       "      <td>59</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>[826249, 1106523, 879755, 994928, 995242, 1029...</td>\n",
       "      <td>[7, 3, 2, 15, 14, 1, 5, 2, 1, 9, 4, 1, 1, 1, 1...</td>\n",
       "      <td>[6678, 38263, 12765, 25715, 25755, 29658, 3736...</td>\n",
       "      <td>[69, 69, 103, 69, 69, 69, 69, 1208, 69, 69, 69...</td>\n",
       "      <td>[Private, Private, National, Private, Private,...</td>\n",
       "      <td>[BAKED BREAD/BUNS/ROLLS, FLUID MILK PRODUCTS, ...</td>\n",
       "      <td>[HAMBURGER BUNS, FLUID MILK WHITE ONLY, SFT DR...</td>\n",
       "      <td>[12 OZ, 1 GA, 20 OZ, 1 DZ,  , 1 GA, 11 OZ, 12 ...</td>\n",
       "      <td>[6678, 38263, 12765, 25715, 25755, 29658, 3736...</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3699</th>\n",
       "      <td>292</td>\n",
       "      <td>74</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>[1106523, 968932, 988208, 995242, 1029743, 556...</td>\n",
       "      <td>[2, 3, 1, 18, 2, 6, 2, 2, 2, 3, 3, 1, 1, 1, 6,...</td>\n",
       "      <td>[38263, 22901, 25001, 25755, 29658, 53098, 545...</td>\n",
       "      <td>[69, 1276, 69, 69, 69, 1208, 69, 69, 69, 1075,...</td>\n",
       "      <td>[Private, National, Private, Private, Private,...</td>\n",
       "      <td>[FLUID MILK PRODUCTS, ISOTONIC DRINKS, FRUIT -...</td>\n",
       "      <td>[FLUID MILK WHITE ONLY, ISOTONIC DRINKS SINGLE...</td>\n",
       "      <td>[1 GA, 32 OZ, 4 OZ,  , 1 GA, 12 OZ, 4 LB, 20 O...</td>\n",
       "      <td>[38263, 22901, 25001, 25755, 29658, 53098, 545...</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37609</th>\n",
       "      <td>343</td>\n",
       "      <td>42</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>[826249, 1022428, 1106523, 995242, 1029743, 10...</td>\n",
       "      <td>[1, 1, 1, 10, 1, 1, 1, 3, 1, 2, 1, 1, 2, 4, 8,...</td>\n",
       "      <td>[6678, 28870, 38263, 25755, 29658, 37360, 9367...</td>\n",
       "      <td>[69, 2, 69, 69, 69, 69, 69, 69, 794, 1046, 69,...</td>\n",
       "      <td>[Private, National, Private, Private, Private,...</td>\n",
       "      <td>[BAKED BREAD/BUNS/ROLLS, SOFT DRINKS, FLUID MI...</td>\n",
       "      <td>[HAMBURGER BUNS, SOFT DRINKS 20PK&amp;24PK CAN CAR...</td>\n",
       "      <td>[12 OZ, 12 OZ, 1 GA,  , 1 GA, 11 OZ, 20 OZ,  ,...</td>\n",
       "      <td>[6678, 28870, 38263, 25755, 29658, 37360, 9367...</td>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42734</th>\n",
       "      <td>356</td>\n",
       "      <td>49</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>[826249, 1106523, 879755, 908213, 844165, 9952...</td>\n",
       "      <td>[1, 2, 2, 4, 1, 13, 4, 1, 1, 1, 3, 1, 1, 1, 4,...</td>\n",
       "      <td>[6678, 38263, 12765, 15968, 8717, 25755, 29658...</td>\n",
       "      <td>[69, 69, 103, 213, 103, 69, 69, 69, 69, 69, 69...</td>\n",
       "      <td>[Private, Private, National, National, Nationa...</td>\n",
       "      <td>[BAKED BREAD/BUNS/ROLLS, FLUID MILK PRODUCTS, ...</td>\n",
       "      <td>[HAMBURGER BUNS, FLUID MILK WHITE ONLY, SFT DR...</td>\n",
       "      <td>[12 OZ, 1 GA, 20 OZ, L    5.5OZ, 2 LTR,  , 1 G...</td>\n",
       "      <td>[6678, 38263, 12765, 15968, 8717, 25755, 29658...</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42748</th>\n",
       "      <td>356</td>\n",
       "      <td>50</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>[826249, 1106523, 844165, 1006878, 1029743, 55...</td>\n",
       "      <td>[1, 8, 2, 1, 7, 3, 1, 1, 1, 1, 3, 2, 1, 1, 1, ...</td>\n",
       "      <td>[6678, 38263, 8717, 27079, 29658, 53098, 13120...</td>\n",
       "      <td>[69, 69, 103, 194, 69, 1208, 69, 69, 1075, 69,...</td>\n",
       "      <td>[Private, Private, National, National, Private...</td>\n",
       "      <td>[BAKED BREAD/BUNS/ROLLS, FLUID MILK PRODUCTS, ...</td>\n",
       "      <td>[HAMBURGER BUNS, FLUID MILK WHITE ONLY, SFT DR...</td>\n",
       "      <td>[12 OZ, 1 GA, 2 LTR, 18 OZ, 1 GA, 12 OZ, 12 OZ...</td>\n",
       "      <td>[6678, 38263, 8717, 27079, 29658, 53098, 13120...</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75057</th>\n",
       "      <td>406</td>\n",
       "      <td>99</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>[1106523, 908213, 994928, 995242, 1029743, 849...</td>\n",
       "      <td>[3, 1, 6, 4, 2, 3, 1, 3, 1, 5, 1, 12, 2, 2, 1,...</td>\n",
       "      <td>[38263, 15968, 25715, 25755, 29658, 9367, 1316...</td>\n",
       "      <td>[69, 213, 69, 69, 69, 69, 69, 69, 1046, 69, 70...</td>\n",
       "      <td>[Private, National, Private, Private, Private,...</td>\n",
       "      <td>[FLUID MILK PRODUCTS, MEAT - SHELF STABLE, EGG...</td>\n",
       "      <td>[FLUID MILK WHITE ONLY, VIENNA SAUSAGE, EGGS -...</td>\n",
       "      <td>[1 GA, L    5.5OZ, 1 DZ,  , 1 GA, 20 OZ, 20 OZ...</td>\n",
       "      <td>[38263, 15968, 25715, 25755, 29658, 9367, 1316...</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109457</th>\n",
       "      <td>31782</td>\n",
       "      <td>24</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>[826249, 1106523, 994928, 995242, 1029743, 109...</td>\n",
       "      <td>[5, 1, 2, 1, 5, 3, 3, 1, 1, 1, 1, 2, 2, 1, 1, ...</td>\n",
       "      <td>[6678, 38263, 25715, 25755, 29658, 37360, 5309...</td>\n",
       "      <td>[69, 69, 69, 69, 69, 69, 1208, 794, 69, 1075, ...</td>\n",
       "      <td>[Private, Private, Private, Private, Private, ...</td>\n",
       "      <td>[BAKED BREAD/BUNS/ROLLS, FLUID MILK PRODUCTS, ...</td>\n",
       "      <td>[HAMBURGER BUNS, FLUID MILK WHITE ONLY, EGGS -...</td>\n",
       "      <td>[12 OZ, 1 GA, 1 DZ,  , 1 GA, 11 OZ, 12 OZ, 12....</td>\n",
       "      <td>[6678, 38263, 25715, 25755, 29658, 37360, 5309...</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109516</th>\n",
       "      <td>31782</td>\n",
       "      <td>29</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>[977266, 995242, 1029743, 1057260, 1098066, 55...</td>\n",
       "      <td>[2, 1, 15, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 4, 2,...</td>\n",
       "      <td>[23794, 25755, 29658, 32779, 37360, 53098, 131...</td>\n",
       "      <td>[69, 69, 69, 69, 69, 1208, 69, 544, 69, 69, 70...</td>\n",
       "      <td>[Private, Private, Private, Private, Private, ...</td>\n",
       "      <td>[FRZN NOVELTIES/WTR ICE, FLUID MILK PRODUCTS, ...</td>\n",
       "      <td>[WATER ICE, FLUID MILK WHITE ONLY, FLUID MILK ...</td>\n",
       "      <td>[18 CT,  , 1 GA, 1 QT, 11 OZ, 12 OZ, 20 OZ, 11...</td>\n",
       "      <td>[23794, 25755, 29658, 32779, 37360, 53098, 131...</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110260</th>\n",
       "      <td>31782</td>\n",
       "      <td>92</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>[826249, 1106523, 908213, 1050741, 968932, 972...</td>\n",
       "      <td>[1, 4, 1, 1, 1, 1, 4, 3, 2, 2, 1, 9, 1, 1, 1, ...</td>\n",
       "      <td>[6678, 38263, 15968, 32032, 22901, 23288, 2571...</td>\n",
       "      <td>[69, 69, 213, 103, 1276, 103, 69, 69, 69, 69, ...</td>\n",
       "      <td>[Private, Private, National, National, Nationa...</td>\n",
       "      <td>[BAKED BREAD/BUNS/ROLLS, FLUID MILK PRODUCTS, ...</td>\n",
       "      <td>[HAMBURGER BUNS, FLUID MILK WHITE ONLY, VIENNA...</td>\n",
       "      <td>[12 OZ, 1 GA, L    5.5OZ, 12 OZ, 32 OZ, 2 LTR,...</td>\n",
       "      <td>[6678, 38263, 15968, 32032, 22901, 23288, 2571...</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112806</th>\n",
       "      <td>32004</td>\n",
       "      <td>98</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>[826249, 966684, 879755, 844165, 995242, 10297...</td>\n",
       "      <td>[2, 1, 1, 1, 11, 4, 36, 1, 3, 1, 2, 2, 1, 1, 1...</td>\n",
       "      <td>[6678, 22659, 12765, 8717, 25755, 29658, 53098...</td>\n",
       "      <td>[69, 103, 103, 103, 69, 69, 1208, 69, 69, 794,...</td>\n",
       "      <td>[Private, National, National, National, Privat...</td>\n",
       "      <td>[BAKED BREAD/BUNS/ROLLS, WATER - CARBONATED/FL...</td>\n",
       "      <td>[HAMBURGER BUNS, NON-CRBNTD DRNKING/MNERAL WAT...</td>\n",
       "      <td>[12 OZ, 24/16.9 OZ, 20 OZ, 2 LTR,  , 1 GA, 12 ...</td>\n",
       "      <td>[6678, 22659, 12765, 8717, 25755, 29658, 53098...</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        STORE_ID  WEEK_NO DEPARTMENT  \\\n",
       "3506         292       59    GROCERY   \n",
       "3699         292       74    GROCERY   \n",
       "37609        343       42    GROCERY   \n",
       "42734        356       49    GROCERY   \n",
       "42748        356       50    GROCERY   \n",
       "...          ...      ...        ...   \n",
       "75057        406       99    GROCERY   \n",
       "109457     31782       24    GROCERY   \n",
       "109516     31782       29    GROCERY   \n",
       "110260     31782       92    GROCERY   \n",
       "112806     32004       98    GROCERY   \n",
       "\n",
       "                                               PRODUCT_ID  \\\n",
       "3506    [826249, 1106523, 879755, 994928, 995242, 1029...   \n",
       "3699    [1106523, 968932, 988208, 995242, 1029743, 556...   \n",
       "37609   [826249, 1022428, 1106523, 995242, 1029743, 10...   \n",
       "42734   [826249, 1106523, 879755, 908213, 844165, 9952...   \n",
       "42748   [826249, 1106523, 844165, 1006878, 1029743, 55...   \n",
       "...                                                   ...   \n",
       "75057   [1106523, 908213, 994928, 995242, 1029743, 849...   \n",
       "109457  [826249, 1106523, 994928, 995242, 1029743, 109...   \n",
       "109516  [977266, 995242, 1029743, 1057260, 1098066, 55...   \n",
       "110260  [826249, 1106523, 908213, 1050741, 968932, 972...   \n",
       "112806  [826249, 966684, 879755, 844165, 995242, 10297...   \n",
       "\n",
       "                                            WEEK_QUANTITY  \\\n",
       "3506    [7, 3, 2, 15, 14, 1, 5, 2, 1, 9, 4, 1, 1, 1, 1...   \n",
       "3699    [2, 3, 1, 18, 2, 6, 2, 2, 2, 3, 3, 1, 1, 1, 6,...   \n",
       "37609   [1, 1, 1, 10, 1, 1, 1, 3, 1, 2, 1, 1, 2, 4, 8,...   \n",
       "42734   [1, 2, 2, 4, 1, 13, 4, 1, 1, 1, 3, 1, 1, 1, 4,...   \n",
       "42748   [1, 8, 2, 1, 7, 3, 1, 1, 1, 1, 3, 2, 1, 1, 1, ...   \n",
       "...                                                   ...   \n",
       "75057   [3, 1, 6, 4, 2, 3, 1, 3, 1, 5, 1, 12, 2, 2, 1,...   \n",
       "109457  [5, 1, 2, 1, 5, 3, 3, 1, 1, 1, 1, 2, 2, 1, 1, ...   \n",
       "109516  [2, 1, 15, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 4, 2,...   \n",
       "110260  [1, 4, 1, 1, 1, 1, 4, 3, 2, 2, 1, 9, 1, 1, 1, ...   \n",
       "112806  [2, 1, 1, 1, 11, 4, 36, 1, 3, 1, 2, 2, 1, 1, 1...   \n",
       "\n",
       "                                                  INDEX_x  \\\n",
       "3506    [6678, 38263, 12765, 25715, 25755, 29658, 3736...   \n",
       "3699    [38263, 22901, 25001, 25755, 29658, 53098, 545...   \n",
       "37609   [6678, 28870, 38263, 25755, 29658, 37360, 9367...   \n",
       "42734   [6678, 38263, 12765, 15968, 8717, 25755, 29658...   \n",
       "42748   [6678, 38263, 8717, 27079, 29658, 53098, 13120...   \n",
       "...                                                   ...   \n",
       "75057   [38263, 15968, 25715, 25755, 29658, 9367, 1316...   \n",
       "109457  [6678, 38263, 25715, 25755, 29658, 37360, 5309...   \n",
       "109516  [23794, 25755, 29658, 32779, 37360, 53098, 131...   \n",
       "110260  [6678, 38263, 15968, 32032, 22901, 23288, 2571...   \n",
       "112806  [6678, 22659, 12765, 8717, 25755, 29658, 53098...   \n",
       "\n",
       "                                             MANUFACTURER  \\\n",
       "3506    [69, 69, 103, 69, 69, 69, 69, 1208, 69, 69, 69...   \n",
       "3699    [69, 1276, 69, 69, 69, 1208, 69, 69, 69, 1075,...   \n",
       "37609   [69, 2, 69, 69, 69, 69, 69, 69, 794, 1046, 69,...   \n",
       "42734   [69, 69, 103, 213, 103, 69, 69, 69, 69, 69, 69...   \n",
       "42748   [69, 69, 103, 194, 69, 1208, 69, 69, 1075, 69,...   \n",
       "...                                                   ...   \n",
       "75057   [69, 213, 69, 69, 69, 69, 69, 69, 1046, 69, 70...   \n",
       "109457  [69, 69, 69, 69, 69, 69, 1208, 794, 69, 1075, ...   \n",
       "109516  [69, 69, 69, 69, 69, 1208, 69, 544, 69, 69, 70...   \n",
       "110260  [69, 69, 213, 103, 1276, 103, 69, 69, 69, 69, ...   \n",
       "112806  [69, 103, 103, 103, 69, 69, 1208, 69, 69, 794,...   \n",
       "\n",
       "                                                    BRAND  \\\n",
       "3506    [Private, Private, National, Private, Private,...   \n",
       "3699    [Private, National, Private, Private, Private,...   \n",
       "37609   [Private, National, Private, Private, Private,...   \n",
       "42734   [Private, Private, National, National, Nationa...   \n",
       "42748   [Private, Private, National, National, Private...   \n",
       "...                                                   ...   \n",
       "75057   [Private, National, Private, Private, Private,...   \n",
       "109457  [Private, Private, Private, Private, Private, ...   \n",
       "109516  [Private, Private, Private, Private, Private, ...   \n",
       "110260  [Private, Private, National, National, Nationa...   \n",
       "112806  [Private, National, National, National, Privat...   \n",
       "\n",
       "                                           COMMODITY_DESC  \\\n",
       "3506    [BAKED BREAD/BUNS/ROLLS, FLUID MILK PRODUCTS, ...   \n",
       "3699    [FLUID MILK PRODUCTS, ISOTONIC DRINKS, FRUIT -...   \n",
       "37609   [BAKED BREAD/BUNS/ROLLS, SOFT DRINKS, FLUID MI...   \n",
       "42734   [BAKED BREAD/BUNS/ROLLS, FLUID MILK PRODUCTS, ...   \n",
       "42748   [BAKED BREAD/BUNS/ROLLS, FLUID MILK PRODUCTS, ...   \n",
       "...                                                   ...   \n",
       "75057   [FLUID MILK PRODUCTS, MEAT - SHELF STABLE, EGG...   \n",
       "109457  [BAKED BREAD/BUNS/ROLLS, FLUID MILK PRODUCTS, ...   \n",
       "109516  [FRZN NOVELTIES/WTR ICE, FLUID MILK PRODUCTS, ...   \n",
       "110260  [BAKED BREAD/BUNS/ROLLS, FLUID MILK PRODUCTS, ...   \n",
       "112806  [BAKED BREAD/BUNS/ROLLS, WATER - CARBONATED/FL...   \n",
       "\n",
       "                                       SUB_COMMODITY_DESC  \\\n",
       "3506    [HAMBURGER BUNS, FLUID MILK WHITE ONLY, SFT DR...   \n",
       "3699    [FLUID MILK WHITE ONLY, ISOTONIC DRINKS SINGLE...   \n",
       "37609   [HAMBURGER BUNS, SOFT DRINKS 20PK&24PK CAN CAR...   \n",
       "42734   [HAMBURGER BUNS, FLUID MILK WHITE ONLY, SFT DR...   \n",
       "42748   [HAMBURGER BUNS, FLUID MILK WHITE ONLY, SFT DR...   \n",
       "...                                                   ...   \n",
       "75057   [FLUID MILK WHITE ONLY, VIENNA SAUSAGE, EGGS -...   \n",
       "109457  [HAMBURGER BUNS, FLUID MILK WHITE ONLY, EGGS -...   \n",
       "109516  [WATER ICE, FLUID MILK WHITE ONLY, FLUID MILK ...   \n",
       "110260  [HAMBURGER BUNS, FLUID MILK WHITE ONLY, VIENNA...   \n",
       "112806  [HAMBURGER BUNS, NON-CRBNTD DRNKING/MNERAL WAT...   \n",
       "\n",
       "                                     CURR_SIZE_OF_PRODUCT  \\\n",
       "3506    [12 OZ, 1 GA, 20 OZ, 1 DZ,  , 1 GA, 11 OZ, 12 ...   \n",
       "3699    [1 GA, 32 OZ, 4 OZ,  , 1 GA, 12 OZ, 4 LB, 20 O...   \n",
       "37609   [12 OZ, 12 OZ, 1 GA,  , 1 GA, 11 OZ, 20 OZ,  ,...   \n",
       "42734   [12 OZ, 1 GA, 20 OZ, L    5.5OZ, 2 LTR,  , 1 G...   \n",
       "42748   [12 OZ, 1 GA, 2 LTR, 18 OZ, 1 GA, 12 OZ, 12 OZ...   \n",
       "...                                                   ...   \n",
       "75057   [1 GA, L    5.5OZ, 1 DZ,  , 1 GA, 20 OZ, 20 OZ...   \n",
       "109457  [12 OZ, 1 GA, 1 DZ,  , 1 GA, 11 OZ, 12 OZ, 12....   \n",
       "109516  [18 CT,  , 1 GA, 1 QT, 11 OZ, 12 OZ, 20 OZ, 11...   \n",
       "110260  [12 OZ, 1 GA, L    5.5OZ, 12 OZ, 32 OZ, 2 LTR,...   \n",
       "112806  [12 OZ, 24/16.9 OZ, 20 OZ, 2 LTR,  , 1 GA, 12 ...   \n",
       "\n",
       "                                                  INDEX_y  SIZE  \n",
       "3506    [6678, 38263, 12765, 25715, 25755, 29658, 3736...   416  \n",
       "3699    [38263, 22901, 25001, 25755, 29658, 53098, 545...   476  \n",
       "37609   [6678, 28870, 38263, 25755, 29658, 37360, 9367...   408  \n",
       "42734   [6678, 38263, 12765, 15968, 8717, 25755, 29658...   429  \n",
       "42748   [6678, 38263, 8717, 27079, 29658, 53098, 13120...   405  \n",
       "...                                                   ...   ...  \n",
       "75057   [38263, 15968, 25715, 25755, 29658, 9367, 1316...   498  \n",
       "109457  [6678, 38263, 25715, 25755, 29658, 37360, 5309...   407  \n",
       "109516  [23794, 25755, 29658, 32779, 37360, 53098, 131...   410  \n",
       "110260  [6678, 38263, 15968, 32032, 22901, 23288, 2571...   427  \n",
       "112806  [6678, 22659, 12765, 8717, 25755, 29658, 53098...   401  \n",
       "\n",
       "[96 rows x 13 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assortments[assortments['SIZE']>400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "HYswrPY67vBM",
    "outputId": "09888f0d-43a3-4ae1-db67-93255b90f2f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assortment:  [5422, 52375]\n",
      "Labels:  [2, 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the sample Assortment.\n",
    "print(' Assortment: ', assortment[200])\n",
    "\n",
    "# Print the corresponding label set.\n",
    "print('Labels: ', labels[200])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss=[]\n",
    "training_time=[]\n",
    "validation_loss=[]\n",
    "validation_time=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assortment_sizes=[32,40,64,80,128,160,256,300,320,400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "Tr_gzIQgTnpe",
    "outputId": "9b19e707-4f5b-49c6-bd5d-b0e9f2157785"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all assortments to 32 values...\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_LEN = assortment_sizes[0]\n",
    "\n",
    "print('\\nPadding/truncating all assortments to %d values...' % MAX_LEN)\n",
    "product_tokens= pad_sequences(assortment, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "demand_labels= pad_sequences(labels, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qAERCjQgmoWZ"
   },
   "outputs": [],
   "source": [
    "attention_masks = []\n",
    "\n",
    "# For each assortment...\n",
    "for sent in product_tokens:\n",
    "    \n",
    "    # Create the attention mask.\n",
    "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    \n",
    "    # Store the attention mask for this assortment.\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sk2Z7ohDUS7X"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(product_tokens, demand_labels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B-2L2T05Uiuo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nSKuy8Q8Us4U"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size =64\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = SequentialSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "aui6PC16eDRT",
    "outputId": "a1b91e8d-8104-48fa-854a-0a7c7e3a5511"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'pro-bert')\n",
    "\n",
    "from modeling_bert import ProductBert,BertConfig\n",
    "\n",
    "import torch\n",
    "\n",
    "configuration = BertConfig()\n",
    "\n",
    "model = ProductBert(configuration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=model(torch.tensor(assortment[12434]).unsqueeze(0), \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=None, \n",
    "                    labels=torch.tensor(labels[12434]).unsqueeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jlkoT1tQ-qoG",
    "outputId": "fda59a02-0aac-41a6-d76e-3edb6434061e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductBert(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(92354, 784, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 784)\n",
       "      (token_type_embeddings): Embedding(2, 784)\n",
       "      (LayerNorm): LayerNorm((784,), eps=1e-08, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=784, out_features=784, bias=True)\n",
       "              (key): Linear(in_features=784, out_features=784, bias=True)\n",
       "              (value): Linear(in_features=784, out_features=784, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=784, out_features=784, bias=True)\n",
       "              (LayerNorm): LayerNorm((784,), eps=1e-08, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=784, out_features=64, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=64, out_features=784, bias=True)\n",
       "            (LayerNorm): LayerNorm((784,), eps=1e-08, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=784, out_features=784, bias=True)\n",
       "              (key): Linear(in_features=784, out_features=784, bias=True)\n",
       "              (value): Linear(in_features=784, out_features=784, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=784, out_features=784, bias=True)\n",
       "              (LayerNorm): LayerNorm((784,), eps=1e-08, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=784, out_features=64, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=64, out_features=784, bias=True)\n",
       "            (LayerNorm): LayerNorm((784,), eps=1e-08, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=784, out_features=784, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=784, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x2b8f10efda40>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "colab_type": "code",
    "id": "V2wstGWDWWts",
    "outputId": "6ffd7fe3-3216-4d0d-f978-a4bf57a6f1bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 41 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (92354, 784)\n",
      "bert.embeddings.position_embeddings.weight                (512, 784)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 784)\n",
      "bert.embeddings.LayerNorm.weight                              (784,)\n",
      "bert.embeddings.LayerNorm.bias                                (784,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (784, 784)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (784,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (784, 784)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (784,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (784, 784)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (784,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (784, 784)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (784,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (784,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (784,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight             (64, 784)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                   (64,)\n",
      "bert.encoder.layer.0.output.dense.weight                   (784, 64)\n",
      "bert.encoder.layer.0.output.dense.bias                        (784,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (784,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (784,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (784, 784)\n",
      "bert.pooler.dense.bias                                        (784,)\n",
      "classifier.weight                                           (1, 784)\n",
      "classifier.bias                                                 (1,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yjFxDXGgY94m"
   },
   "outputs": [],
   "source": [
    "from transformers import  AdamW\n",
    "import torch.optim\n",
    "optimizer=torch.optim.SGD(model.parameters(), lr=5e-3)\n",
    "# optimizer = AdamW(model.parameters(),\n",
    "#                   lr = 5e-3, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "#                   eps = 1e-6 # args.adam_epsilon  - default is 1e-8.\n",
    "#                 )\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.05, betas=(0.9, 0.999), eps=1e-06, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs \n",
    "epochs = 15\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0GMtbvVAaNoy"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    \n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch     0  of  1,613.    Elapsed: 0:00:00. with Loss: 2.6787307262420654 with total_loss: 0\n",
      "  Batch     1  of  1,613.    Elapsed: 0:00:00. with Loss: 202.87039184570312 with total_loss: 2.6787307262420654\n",
      "  Batch     2  of  1,613.    Elapsed: 0:00:00. with Loss: 1292.97998046875 with total_loss: 205.5491225719452\n",
      "  Batch     3  of  1,613.    Elapsed: 0:00:01. with Loss: 373.68798828125 with total_loss: 1498.5291030406952\n",
      "  Batch     4  of  1,613.    Elapsed: 0:00:01. with Loss: 22.04460906982422 with total_loss: 1872.2170913219452\n",
      "  Batch     5  of  1,613.    Elapsed: 0:00:01. with Loss: 44.1263427734375 with total_loss: 1894.2617003917694\n",
      "  Batch     6  of  1,613.    Elapsed: 0:00:01. with Loss: 21.268756866455078 with total_loss: 1938.388043165207\n",
      "  Batch     7  of  1,613.    Elapsed: 0:00:01. with Loss: 2.369746208190918 with total_loss: 1959.656800031662\n",
      "  Batch     8  of  1,613.    Elapsed: 0:00:01. with Loss: 8.316697120666504 with total_loss: 1962.026546239853\n",
      "  Batch     9  of  1,613.    Elapsed: 0:00:01. with Loss: 21.575578689575195 with total_loss: 1970.3432433605194\n",
      "  Batch    10  of  1,613.    Elapsed: 0:00:02. with Loss: 16.867172241210938 with total_loss: 1991.9188220500946\n",
      "  Batch    11  of  1,613.    Elapsed: 0:00:02. with Loss: 3.91318941116333 with total_loss: 2008.7859942913055\n",
      "  Batch    12  of  1,613.    Elapsed: 0:00:02. with Loss: 3.7429261207580566 with total_loss: 2012.6991837024689\n",
      "  Batch    13  of  1,613.    Elapsed: 0:00:02. with Loss: 15.236701011657715 with total_loss: 2016.442109823227\n",
      "  Batch    14  of  1,613.    Elapsed: 0:00:02. with Loss: 14.857145309448242 with total_loss: 2031.6788108348846\n",
      "  Batch    15  of  1,613.    Elapsed: 0:00:02. with Loss: 4.964151859283447 with total_loss: 2046.5359561443329\n",
      "  Batch    16  of  1,613.    Elapsed: 0:00:02. with Loss: 2.074759006500244 with total_loss: 2051.5001080036163\n",
      "  Batch    17  of  1,613.    Elapsed: 0:00:03. with Loss: 9.085114479064941 with total_loss: 2053.5748670101166\n",
      "  Batch    18  of  1,613.    Elapsed: 0:00:03. with Loss: 13.455843925476074 with total_loss: 2062.6599814891815\n",
      "  Batch    19  of  1,613.    Elapsed: 0:00:03. with Loss: 4.546812057495117 with total_loss: 2076.1158254146576\n",
      "  Batch    20  of  1,613.    Elapsed: 0:00:03. with Loss: 2.0340027809143066 with total_loss: 2080.6626374721527\n",
      "  Batch    21  of  1,613.    Elapsed: 0:00:03. with Loss: 6.494497299194336 with total_loss: 2082.696640253067\n",
      "  Batch    22  of  1,613.    Elapsed: 0:00:03. with Loss: 6.779443740844727 with total_loss: 2089.1911375522614\n",
      "  Batch    23  of  1,613.    Elapsed: 0:00:03. with Loss: 3.8820993900299072 with total_loss: 2095.970581293106\n",
      "  Batch    24  of  1,613.    Elapsed: 0:00:03. with Loss: 1.4683923721313477 with total_loss: 2099.852680683136\n",
      "  Batch    25  of  1,613.    Elapsed: 0:00:04. with Loss: 4.703726768493652 with total_loss: 2101.3210730552673\n",
      "  Batch    26  of  1,613.    Elapsed: 0:00:04. with Loss: 5.525081157684326 with total_loss: 2106.024799823761\n",
      "  Batch    27  of  1,613.    Elapsed: 0:00:04. with Loss: 2.9423093795776367 with total_loss: 2111.5498809814453\n",
      "  Batch    28  of  1,613.    Elapsed: 0:00:04. with Loss: 2.0755388736724854 with total_loss: 2114.492190361023\n",
      "  Batch    29  of  1,613.    Elapsed: 0:00:04. with Loss: 4.214417457580566 with total_loss: 2116.5677292346954\n",
      "  Batch    30  of  1,613.    Elapsed: 0:00:04. with Loss: 4.259060382843018 with total_loss: 2120.782146692276\n",
      "  Batch    31  of  1,613.    Elapsed: 0:00:04. with Loss: 3.0262160301208496 with total_loss: 2125.041207075119\n",
      "  Batch    32  of  1,613.    Elapsed: 0:00:05. with Loss: 1.6171077489852905 with total_loss: 2128.06742310524\n",
      "  Batch    33  of  1,613.    Elapsed: 0:00:05. with Loss: 3.19537353515625 with total_loss: 2129.684530854225\n",
      "  Batch    34  of  1,613.    Elapsed: 0:00:05. with Loss: 3.59869384765625 with total_loss: 2132.8799043893814\n",
      "  Batch    35  of  1,613.    Elapsed: 0:00:05. with Loss: 2.1310336589813232 with total_loss: 2136.4785982370377\n",
      "  Batch    36  of  1,613.    Elapsed: 0:00:05. with Loss: 2.271695137023926 with total_loss: 2138.609631896019\n",
      "  Batch    37  of  1,613.    Elapsed: 0:00:05. with Loss: 3.4053213596343994 with total_loss: 2140.881327033043\n",
      "  Batch    38  of  1,613.    Elapsed: 0:00:05. with Loss: 2.3387932777404785 with total_loss: 2144.2866483926773\n",
      "  Batch    39  of  1,613.    Elapsed: 0:00:06. with Loss: 2.00091290473938 with total_loss: 2146.625441670418\n",
      "  Batch    40  of  1,613.    Elapsed: 0:00:06. with Loss: 2.1465349197387695 with total_loss: 2148.626354575157\n",
      "  Batch    41  of  1,613.    Elapsed: 0:00:06. with Loss: 2.730213165283203 with total_loss: 2150.772889494896\n",
      "  Batch    42  of  1,613.    Elapsed: 0:00:06. with Loss: 2.0786538124084473 with total_loss: 2153.503102660179\n",
      "  Batch    43  of  1,613.    Elapsed: 0:00:06. with Loss: 1.7013264894485474 with total_loss: 2155.5817564725876\n",
      "  Batch    44  of  1,613.    Elapsed: 0:00:06. with Loss: 3.965940475463867 with total_loss: 2157.283082962036\n",
      "  Batch    45  of  1,613.    Elapsed: 0:00:06. with Loss: 2.2428858280181885 with total_loss: 2161.2490234375\n",
      "  Batch    46  of  1,613.    Elapsed: 0:00:07. with Loss: 1.1406043767929077 with total_loss: 2163.491909265518\n",
      "  Batch    47  of  1,613.    Elapsed: 0:00:07. with Loss: 1.9512346982955933 with total_loss: 2164.632513642311\n",
      "  Batch    48  of  1,613.    Elapsed: 0:00:07. with Loss: 2.4516255855560303 with total_loss: 2166.5837483406067\n",
      "  Batch    49  of  1,613.    Elapsed: 0:00:07. with Loss: 1.982998251914978 with total_loss: 2169.0353739261627\n",
      "  Batch    50  of  1,613.    Elapsed: 0:00:07. with Loss: 1.8356140851974487 with total_loss: 2171.0183721780777\n",
      "  Batch    51  of  1,613.    Elapsed: 0:00:07. with Loss: 2.518737316131592 with total_loss: 2172.853986263275\n",
      "  Batch    52  of  1,613.    Elapsed: 0:00:07. with Loss: 1.9476449489593506 with total_loss: 2175.3727235794067\n",
      "  Batch    53  of  1,613.    Elapsed: 0:00:07. with Loss: 1.5044063329696655 with total_loss: 2177.320368528366\n",
      "  Batch    54  of  1,613.    Elapsed: 0:00:08. with Loss: 2.7797605991363525 with total_loss: 2178.8247748613358\n",
      "  Batch    55  of  1,613.    Elapsed: 0:00:08. with Loss: 1.9582089185714722 with total_loss: 2181.604535460472\n",
      "  Batch    56  of  1,613.    Elapsed: 0:00:08. with Loss: 1.1381982564926147 with total_loss: 2183.5627443790436\n",
      "  Batch    57  of  1,613.    Elapsed: 0:00:08. with Loss: 1.7599672079086304 with total_loss: 2184.700942635536\n",
      "  Batch    58  of  1,613.    Elapsed: 0:00:08. with Loss: 1.5426470041275024 with total_loss: 2186.460909843445\n",
      "  Batch    59  of  1,613.    Elapsed: 0:00:08. with Loss: 2.776297092437744 with total_loss: 2188.0035568475723\n",
      "  Batch    60  of  1,613.    Elapsed: 0:00:08. with Loss: 1.67439603805542 with total_loss: 2190.77985394001\n",
      "  Batch    61  of  1,613.    Elapsed: 0:00:09. with Loss: 1.746914267539978 with total_loss: 2192.4542499780655\n",
      "  Batch    62  of  1,613.    Elapsed: 0:00:09. with Loss: 1.7797398567199707 with total_loss: 2194.2011642456055\n",
      "  Batch    63  of  1,613.    Elapsed: 0:00:09. with Loss: 1.3087120056152344 with total_loss: 2195.9809041023254\n",
      "  Batch    64  of  1,613.    Elapsed: 0:00:09. with Loss: 1.7037465572357178 with total_loss: 2197.2896161079407\n",
      "  Batch    65  of  1,613.    Elapsed: 0:00:09. with Loss: 1.4790321588516235 with total_loss: 2198.9933626651764\n",
      "  Batch    66  of  1,613.    Elapsed: 0:00:09. with Loss: 1.7966598272323608 with total_loss: 2200.472394824028\n",
      "  Batch    67  of  1,613.    Elapsed: 0:00:09. with Loss: 2.3840882778167725 with total_loss: 2202.2690546512604\n",
      "  Batch    68  of  1,613.    Elapsed: 0:00:10. with Loss: 1.9457646608352661 with total_loss: 2204.653142929077\n",
      "  Batch    69  of  1,613.    Elapsed: 0:00:10. with Loss: 2.025657892227173 with total_loss: 2206.5989075899124\n",
      "  Batch    70  of  1,613.    Elapsed: 0:00:10. with Loss: 1.9506210088729858 with total_loss: 2208.6245654821396\n",
      "  Batch    71  of  1,613.    Elapsed: 0:00:10. with Loss: 1.2132633924484253 with total_loss: 2210.5751864910126\n",
      "  Batch    72  of  1,613.    Elapsed: 0:00:10. with Loss: 1.8552203178405762 with total_loss: 2211.788449883461\n",
      "  Batch    73  of  1,613.    Elapsed: 0:00:10. with Loss: 1.1096789836883545 with total_loss: 2213.6436702013016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    74  of  1,613.    Elapsed: 0:00:10. with Loss: 3.542407274246216 with total_loss: 2214.75334918499\n",
      "  Batch    75  of  1,613.    Elapsed: 0:00:10. with Loss: 2.6448848247528076 with total_loss: 2218.295756459236\n",
      "  Batch    76  of  1,613.    Elapsed: 0:00:11. with Loss: 3.303420066833496 with total_loss: 2220.940641283989\n",
      "  Batch    77  of  1,613.    Elapsed: 0:00:11. with Loss: 1.8795206546783447 with total_loss: 2224.2440613508224\n",
      "  Batch    78  of  1,613.    Elapsed: 0:00:11. with Loss: 1.9462765455245972 with total_loss: 2226.123582005501\n",
      "  Batch    79  of  1,613.    Elapsed: 0:00:11. with Loss: 1.5037506818771362 with total_loss: 2228.0698585510254\n",
      "  Batch    80  of  1,613.    Elapsed: 0:00:11. with Loss: 1.6284931898117065 with total_loss: 2229.5736092329025\n",
      "  Batch    81  of  1,613.    Elapsed: 0:00:11. with Loss: 1.9329949617385864 with total_loss: 2231.2021024227142\n",
      "  Batch    82  of  1,613.    Elapsed: 0:00:11. with Loss: 1.6393380165100098 with total_loss: 2233.135097384453\n",
      "  Batch    83  of  1,613.    Elapsed: 0:00:12. with Loss: 1.1739782094955444 with total_loss: 2234.774435400963\n",
      "  Batch    84  of  1,613.    Elapsed: 0:00:12. with Loss: 1.7560802698135376 with total_loss: 2235.9484136104584\n",
      "  Batch    85  of  1,613.    Elapsed: 0:00:12. with Loss: 1.2139904499053955 with total_loss: 2237.704493880272\n",
      "  Batch    86  of  1,613.    Elapsed: 0:00:12. with Loss: 1.7361304759979248 with total_loss: 2238.9184843301773\n",
      "  Batch    87  of  1,613.    Elapsed: 0:00:12. with Loss: 11.636693000793457 with total_loss: 2240.6546148061752\n",
      "  Batch    88  of  1,613.    Elapsed: 0:00:12. with Loss: 2.0577423572540283 with total_loss: 2252.2913078069687\n",
      "  Batch    89  of  1,613.    Elapsed: 0:00:12. with Loss: 1.4641332626342773 with total_loss: 2254.3490501642227\n",
      "  Batch    90  of  1,613.    Elapsed: 0:00:13. with Loss: 1.5999431610107422 with total_loss: 2255.813183426857\n",
      "  Batch    91  of  1,613.    Elapsed: 0:00:13. with Loss: 1.2504414319992065 with total_loss: 2257.4131265878677\n",
      "  Batch    92  of  1,613.    Elapsed: 0:00:13. with Loss: 2.4351725578308105 with total_loss: 2258.663568019867\n",
      "  Batch    93  of  1,613.    Elapsed: 0:00:13. with Loss: 1.589993953704834 with total_loss: 2261.0987405776978\n",
      "  Batch    94  of  1,613.    Elapsed: 0:00:13. with Loss: 2.4374616146087646 with total_loss: 2262.6887345314026\n",
      "  Batch    95  of  1,613.    Elapsed: 0:00:13. with Loss: 1.4786796569824219 with total_loss: 2265.1261961460114\n",
      "  Batch    96  of  1,613.    Elapsed: 0:00:13. with Loss: 1.6562267541885376 with total_loss: 2266.604875802994\n",
      "  Batch    97  of  1,613.    Elapsed: 0:00:14. with Loss: 1.3876755237579346 with total_loss: 2268.2611025571823\n",
      "  Batch    98  of  1,613.    Elapsed: 0:00:14. with Loss: 4.024322509765625 with total_loss: 2269.6487780809402\n",
      "  Batch    99  of  1,613.    Elapsed: 0:00:14. with Loss: 1.6332900524139404 with total_loss: 2273.673100590706\n",
      "  Batch   100  of  1,613.    Elapsed: 0:00:14. with Loss: 2.3506202697753906 with total_loss: 2275.30639064312\n",
      "  Batch   101  of  1,613.    Elapsed: 0:00:14. with Loss: 1.7509833574295044 with total_loss: 2277.657010912895\n",
      "  Batch   102  of  1,613.    Elapsed: 0:00:14. with Loss: 2.021441698074341 with total_loss: 2279.4079942703247\n",
      "  Batch   103  of  1,613.    Elapsed: 0:00:14. with Loss: 1.3940361738204956 with total_loss: 2281.429435968399\n",
      "  Batch   104  of  1,613.    Elapsed: 0:00:14. with Loss: 1.7720297574996948 with total_loss: 2282.8234721422195\n",
      "  Batch   105  of  1,613.    Elapsed: 0:00:15. with Loss: 1.4877859354019165 with total_loss: 2284.5955018997192\n",
      "  Batch   106  of  1,613.    Elapsed: 0:00:15. with Loss: 1.9036052227020264 with total_loss: 2286.083287835121\n",
      "  Batch   107  of  1,613.    Elapsed: 0:00:15. with Loss: 1.0898712873458862 with total_loss: 2287.986893057823\n",
      "  Batch   108  of  1,613.    Elapsed: 0:00:15. with Loss: 1.4410783052444458 with total_loss: 2289.076764345169\n",
      "  Batch   109  of  1,613.    Elapsed: 0:00:15. with Loss: 2.567068099975586 with total_loss: 2290.5178426504135\n",
      "  Batch   110  of  1,613.    Elapsed: 0:00:15. with Loss: 1.7092376947402954 with total_loss: 2293.084910750389\n",
      "  Batch   111  of  1,613.    Elapsed: 0:00:15. with Loss: 1.7933238744735718 with total_loss: 2294.7941484451294\n",
      "  Batch   112  of  1,613.    Elapsed: 0:00:16. with Loss: 1.4897630214691162 with total_loss: 2296.587472319603\n",
      "  Batch   113  of  1,613.    Elapsed: 0:00:16. with Loss: 1.8713141679763794 with total_loss: 2298.077235341072\n",
      "  Batch   114  of  1,613.    Elapsed: 0:00:16. with Loss: 1.5577144622802734 with total_loss: 2299.9485495090485\n",
      "  Batch   115  of  1,613.    Elapsed: 0:00:16. with Loss: 2.0826683044433594 with total_loss: 2301.5062639713287\n",
      "  Batch   116  of  1,613.    Elapsed: 0:00:16. with Loss: 2.280921220779419 with total_loss: 2303.588932275772\n",
      "  Batch   117  of  1,613.    Elapsed: 0:00:16. with Loss: 1.4820756912231445 with total_loss: 2305.8698534965515\n",
      "  Batch   118  of  1,613.    Elapsed: 0:00:16. with Loss: 1.4613709449768066 with total_loss: 2307.3519291877747\n",
      "  Batch   119  of  1,613.    Elapsed: 0:00:17. with Loss: 1.8841525316238403 with total_loss: 2308.8133001327515\n",
      "  Batch   120  of  1,613.    Elapsed: 0:00:17. with Loss: 1.391680359840393 with total_loss: 2310.6974526643753\n",
      "  Batch   121  of  1,613.    Elapsed: 0:00:17. with Loss: 3.5852508544921875 with total_loss: 2312.0891330242157\n",
      "  Batch   122  of  1,613.    Elapsed: 0:00:17. with Loss: 1.1690984964370728 with total_loss: 2315.674383878708\n",
      "  Batch   123  of  1,613.    Elapsed: 0:00:17. with Loss: 1.434867262840271 with total_loss: 2316.843482375145\n",
      "  Batch   124  of  1,613.    Elapsed: 0:00:17. with Loss: 1.87602698802948 with total_loss: 2318.2783496379852\n",
      "  Batch   125  of  1,613.    Elapsed: 0:00:17. with Loss: 1.5523245334625244 with total_loss: 2320.1543766260147\n",
      "  Batch   126  of  1,613.    Elapsed: 0:00:18. with Loss: 1.3980469703674316 with total_loss: 2321.7067011594772\n",
      "  Batch   127  of  1,613.    Elapsed: 0:00:18. with Loss: 2.3200061321258545 with total_loss: 2323.1047481298447\n",
      "  Batch   128  of  1,613.    Elapsed: 0:00:18. with Loss: 1.696931004524231 with total_loss: 2325.4247542619705\n",
      "  Batch   129  of  1,613.    Elapsed: 0:00:18. with Loss: 2.450155258178711 with total_loss: 2327.1216852664948\n",
      "  Batch   130  of  1,613.    Elapsed: 0:00:18. with Loss: 1.4406416416168213 with total_loss: 2329.5718405246735\n",
      "  Batch   131  of  1,613.    Elapsed: 0:00:18. with Loss: 2.026085138320923 with total_loss: 2331.0124821662903\n",
      "  Batch   132  of  1,613.    Elapsed: 0:00:18. with Loss: 1.8257802724838257 with total_loss: 2333.038567304611\n",
      "  Batch   133  of  1,613.    Elapsed: 0:00:18. with Loss: 1.7362511157989502 with total_loss: 2334.864347577095\n",
      "  Batch   134  of  1,613.    Elapsed: 0:00:19. with Loss: 1.7460259199142456 with total_loss: 2336.600598692894\n",
      "  Batch   135  of  1,613.    Elapsed: 0:00:19. with Loss: 1.6588990688323975 with total_loss: 2338.3466246128082\n",
      "  Batch   136  of  1,613.    Elapsed: 0:00:19. with Loss: 1.3218138217926025 with total_loss: 2340.0055236816406\n",
      "  Batch   137  of  1,613.    Elapsed: 0:00:19. with Loss: 1.1904752254486084 with total_loss: 2341.3273375034332\n",
      "  Batch   138  of  1,613.    Elapsed: 0:00:19. with Loss: 1.1997274160385132 with total_loss: 2342.517812728882\n",
      "  Batch   139  of  1,613.    Elapsed: 0:00:19. with Loss: 1.8714345693588257 with total_loss: 2343.7175401449203\n",
      "  Batch   140  of  1,613.    Elapsed: 0:00:19. with Loss: 1.7370039224624634 with total_loss: 2345.588974714279\n",
      "  Batch   141  of  1,613.    Elapsed: 0:00:20. with Loss: 1.72317373752594 with total_loss: 2347.3259786367416\n",
      "  Batch   142  of  1,613.    Elapsed: 0:00:20. with Loss: 1.628603219985962 with total_loss: 2349.0491523742676\n",
      "  Batch   143  of  1,613.    Elapsed: 0:00:20. with Loss: 1.1008280515670776 with total_loss: 2350.6777555942535\n",
      "  Batch   144  of  1,613.    Elapsed: 0:00:20. with Loss: 1.0909616947174072 with total_loss: 2351.7785836458206\n",
      "  Batch   145  of  1,613.    Elapsed: 0:00:20. with Loss: 1.2488837242126465 with total_loss: 2352.869545340538\n",
      "  Batch   146  of  1,613.    Elapsed: 0:00:20. with Loss: 1.7504633665084839 with total_loss: 2354.1184290647507\n",
      "  Batch   147  of  1,613.    Elapsed: 0:00:20. with Loss: 3.5254812240600586 with total_loss: 2355.868892431259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   148  of  1,613.    Elapsed: 0:00:21. with Loss: 1.3755980730056763 with total_loss: 2359.394373655319\n",
      "  Batch   149  of  1,613.    Elapsed: 0:00:21. with Loss: 1.599674940109253 with total_loss: 2360.769971728325\n",
      "  Batch   150  of  1,613.    Elapsed: 0:00:21. with Loss: 2.119379997253418 with total_loss: 2362.369646668434\n",
      "  Batch   151  of  1,613.    Elapsed: 0:00:21. with Loss: 2.5561118125915527 with total_loss: 2364.4890266656876\n",
      "  Batch   152  of  1,613.    Elapsed: 0:00:21. with Loss: 1.155858039855957 with total_loss: 2367.045138478279\n",
      "  Batch   153  of  1,613.    Elapsed: 0:00:21. with Loss: 2.035057544708252 with total_loss: 2368.200996518135\n",
      "  Batch   154  of  1,613.    Elapsed: 0:00:21. with Loss: 3.9545397758483887 with total_loss: 2370.2360540628433\n",
      "  Batch   155  of  1,613.    Elapsed: 0:00:21. with Loss: 2.0583693981170654 with total_loss: 2374.1905938386917\n",
      "  Batch   156  of  1,613.    Elapsed: 0:00:22. with Loss: 1.725855827331543 with total_loss: 2376.248963236809\n",
      "  Batch   157  of  1,613.    Elapsed: 0:00:22. with Loss: 1.6891642808914185 with total_loss: 2377.9748190641403\n",
      "  Batch   158  of  1,613.    Elapsed: 0:00:22. with Loss: 1.2081600427627563 with total_loss: 2379.6639833450317\n",
      "  Batch   159  of  1,613.    Elapsed: 0:00:22. with Loss: 2.1646881103515625 with total_loss: 2380.8721433877945\n",
      "  Batch   160  of  1,613.    Elapsed: 0:00:22. with Loss: 2.751173257827759 with total_loss: 2383.036831498146\n",
      "  Batch   161  of  1,613.    Elapsed: 0:00:22. with Loss: 3.0710983276367188 with total_loss: 2385.788004755974\n",
      "  Batch   162  of  1,613.    Elapsed: 0:00:22. with Loss: 2.0583486557006836 with total_loss: 2388.8591030836105\n",
      "  Batch   163  of  1,613.    Elapsed: 0:00:23. with Loss: 1.7565834522247314 with total_loss: 2390.917451739311\n",
      "  Batch   164  of  1,613.    Elapsed: 0:00:23. with Loss: 2.334977388381958 with total_loss: 2392.674035191536\n",
      "  Batch   165  of  1,613.    Elapsed: 0:00:23. with Loss: 1.6256219148635864 with total_loss: 2395.009012579918\n",
      "  Batch   166  of  1,613.    Elapsed: 0:00:23. with Loss: 1.883601188659668 with total_loss: 2396.6346344947815\n",
      "  Batch   167  of  1,613.    Elapsed: 0:00:23. with Loss: 1.1347116231918335 with total_loss: 2398.518235683441\n",
      "  Batch   168  of  1,613.    Elapsed: 0:00:23. with Loss: 1.6551638841629028 with total_loss: 2399.652947306633\n",
      "  Batch   169  of  1,613.    Elapsed: 0:00:23. with Loss: 1.4701255559921265 with total_loss: 2401.308111190796\n",
      "  Batch   170  of  1,613.    Elapsed: 0:00:24. with Loss: 2.4503302574157715 with total_loss: 2402.778236746788\n",
      "  Batch   171  of  1,613.    Elapsed: 0:00:24. with Loss: 1.8274767398834229 with total_loss: 2405.228567004204\n",
      "  Batch   172  of  1,613.    Elapsed: 0:00:24. with Loss: 2.572113275527954 with total_loss: 2407.056043744087\n",
      "  Batch   173  of  1,613.    Elapsed: 0:00:24. with Loss: 3.1176857948303223 with total_loss: 2409.628157019615\n",
      "  Batch   174  of  1,613.    Elapsed: 0:00:24. with Loss: 1.3323167562484741 with total_loss: 2412.7458428144455\n",
      "  Batch   175  of  1,613.    Elapsed: 0:00:24. with Loss: 2.460416316986084 with total_loss: 2414.078159570694\n",
      "  Batch   176  of  1,613.    Elapsed: 0:00:24. with Loss: 1.646208643913269 with total_loss: 2416.53857588768\n",
      "  Batch   177  of  1,613.    Elapsed: 0:00:25. with Loss: 1.5051733255386353 with total_loss: 2418.1847845315933\n",
      "  Batch   178  of  1,613.    Elapsed: 0:00:25. with Loss: 2.1491858959198 with total_loss: 2419.689957857132\n",
      "  Batch   179  of  1,613.    Elapsed: 0:00:25. with Loss: 1.9172569513320923 with total_loss: 2421.8391437530518\n",
      "  Batch   180  of  1,613.    Elapsed: 0:00:25. with Loss: 1.5354076623916626 with total_loss: 2423.756400704384\n",
      "  Batch   181  of  1,613.    Elapsed: 0:00:25. with Loss: 1.4256500005722046 with total_loss: 2425.2918083667755\n",
      "  Batch   182  of  1,613.    Elapsed: 0:00:25. with Loss: 2.7222259044647217 with total_loss: 2426.7174583673477\n",
      "  Batch   183  of  1,613.    Elapsed: 0:00:25. with Loss: 1.6925104856491089 with total_loss: 2429.4396842718124\n",
      "  Batch   184  of  1,613.    Elapsed: 0:00:25. with Loss: 1.4979698657989502 with total_loss: 2431.1321947574615\n",
      "  Batch   185  of  1,613.    Elapsed: 0:00:26. with Loss: 1.7307956218719482 with total_loss: 2432.6301646232605\n",
      "  Batch   186  of  1,613.    Elapsed: 0:00:26. with Loss: 2.210124969482422 with total_loss: 2434.3609602451324\n",
      "  Batch   187  of  1,613.    Elapsed: 0:00:26. with Loss: 1.7463613748550415 with total_loss: 2436.571085214615\n",
      "  Batch   188  of  1,613.    Elapsed: 0:00:26. with Loss: 2.0895049571990967 with total_loss: 2438.31744658947\n",
      "  Batch   189  of  1,613.    Elapsed: 0:00:26. with Loss: 1.344762921333313 with total_loss: 2440.406951546669\n",
      "  Batch   190  of  1,613.    Elapsed: 0:00:26. with Loss: 1.1184459924697876 with total_loss: 2441.7517144680023\n",
      "  Batch   191  of  1,613.    Elapsed: 0:00:26. with Loss: 2.097799777984619 with total_loss: 2442.870160460472\n",
      "  Batch   192  of  1,613.    Elapsed: 0:00:27. with Loss: 1.4310951232910156 with total_loss: 2444.9679602384567\n",
      "  Batch   193  of  1,613.    Elapsed: 0:00:27. with Loss: 1.639466643333435 with total_loss: 2446.3990553617477\n",
      "  Batch   194  of  1,613.    Elapsed: 0:00:27. with Loss: 1.5702521800994873 with total_loss: 2448.038522005081\n",
      "  Batch   195  of  1,613.    Elapsed: 0:00:27. with Loss: 1.498841643333435 with total_loss: 2449.6087741851807\n",
      "  Batch   196  of  1,613.    Elapsed: 0:00:27. with Loss: 1.3410190343856812 with total_loss: 2451.107615828514\n",
      "  Batch   197  of  1,613.    Elapsed: 0:00:27. with Loss: 3.735773801803589 with total_loss: 2452.4486348629\n",
      "  Batch   198  of  1,613.    Elapsed: 0:00:27. with Loss: 2.2254419326782227 with total_loss: 2456.1844086647034\n",
      "  Batch   199  of  1,613.    Elapsed: 0:00:28. with Loss: 1.579716682434082 with total_loss: 2458.4098505973816\n",
      "  Batch   200  of  1,613.    Elapsed: 0:00:28. with Loss: 3.1598000526428223 with total_loss: 2459.9895672798157\n",
      "  Batch   201  of  1,613.    Elapsed: 0:00:28. with Loss: 1.7123019695281982 with total_loss: 2463.1493673324585\n",
      "  Batch   202  of  1,613.    Elapsed: 0:00:28. with Loss: 1.2519402503967285 with total_loss: 2464.8616693019867\n",
      "  Batch   203  of  1,613.    Elapsed: 0:00:28. with Loss: 1.8112398386001587 with total_loss: 2466.1136095523834\n",
      "  Batch   204  of  1,613.    Elapsed: 0:00:28. with Loss: 1.8307499885559082 with total_loss: 2467.9248493909836\n",
      "  Batch   205  of  1,613.    Elapsed: 0:00:28. with Loss: 1.7708756923675537 with total_loss: 2469.7555993795395\n",
      "  Batch   206  of  1,613.    Elapsed: 0:00:29. with Loss: 1.8100427389144897 with total_loss: 2471.526475071907\n",
      "  Batch   207  of  1,613.    Elapsed: 0:00:29. with Loss: 2.034106969833374 with total_loss: 2473.3365178108215\n",
      "  Batch   208  of  1,613.    Elapsed: 0:00:29. with Loss: 1.6960028409957886 with total_loss: 2475.370624780655\n",
      "  Batch   209  of  1,613.    Elapsed: 0:00:29. with Loss: 2.11838436126709 with total_loss: 2477.0666276216507\n",
      "  Batch   210  of  1,613.    Elapsed: 0:00:29. with Loss: 1.572281002998352 with total_loss: 2479.185011982918\n",
      "  Batch   211  of  1,613.    Elapsed: 0:00:29. with Loss: 2.8122904300689697 with total_loss: 2480.757292985916\n",
      "  Batch   212  of  1,613.    Elapsed: 0:00:29. with Loss: 1.005570650100708 with total_loss: 2483.569583415985\n",
      "  Batch   213  of  1,613.    Elapsed: 0:00:29. with Loss: 1.8277515172958374 with total_loss: 2484.575154066086\n",
      "  Batch   214  of  1,613.    Elapsed: 0:00:30. with Loss: 1.3927879333496094 with total_loss: 2486.4029055833817\n",
      "  Batch   215  of  1,613.    Elapsed: 0:00:30. with Loss: 1.2993965148925781 with total_loss: 2487.7956935167313\n",
      "  Batch   216  of  1,613.    Elapsed: 0:00:30. with Loss: 2.5892813205718994 with total_loss: 2489.095090031624\n",
      "  Batch   217  of  1,613.    Elapsed: 0:00:30. with Loss: 1.1303144693374634 with total_loss: 2491.6843713521957\n",
      "  Batch   218  of  1,613.    Elapsed: 0:00:30. with Loss: 1.8386000394821167 with total_loss: 2492.814685821533\n",
      "  Batch   219  of  1,613.    Elapsed: 0:00:30. with Loss: 1.4101285934448242 with total_loss: 2494.6532858610153\n",
      "  Batch   220  of  1,613.    Elapsed: 0:00:30. with Loss: 1.7451015710830688 with total_loss: 2496.06341445446\n",
      "  Batch   221  of  1,613.    Elapsed: 0:00:31. with Loss: 2.1030917167663574 with total_loss: 2497.808516025543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   222  of  1,613.    Elapsed: 0:00:31. with Loss: 2.3085546493530273 with total_loss: 2499.9116077423096\n",
      "  Batch   223  of  1,613.    Elapsed: 0:00:31. with Loss: 1.5230889320373535 with total_loss: 2502.2201623916626\n",
      "  Batch   224  of  1,613.    Elapsed: 0:00:31. with Loss: 1.70685613155365 with total_loss: 2503.7432513237\n",
      "  Batch   225  of  1,613.    Elapsed: 0:00:31. with Loss: 1.1185328960418701 with total_loss: 2505.4501074552536\n",
      "  Batch   226  of  1,613.    Elapsed: 0:00:31. with Loss: 1.798183798789978 with total_loss: 2506.5686403512955\n",
      "  Batch   227  of  1,613.    Elapsed: 0:00:31. with Loss: 1.307862639427185 with total_loss: 2508.3668241500854\n",
      "  Batch   228  of  1,613.    Elapsed: 0:00:32. with Loss: 1.3361538648605347 with total_loss: 2509.6746867895126\n",
      "  Batch   229  of  1,613.    Elapsed: 0:00:32. with Loss: 2.814981698989868 with total_loss: 2511.010840654373\n",
      "  Batch   230  of  1,613.    Elapsed: 0:00:32. with Loss: 1.3211215734481812 with total_loss: 2513.825822353363\n",
      "  Batch   231  of  1,613.    Elapsed: 0:00:32. with Loss: 2.604402542114258 with total_loss: 2515.146943926811\n",
      "  Batch   232  of  1,613.    Elapsed: 0:00:32. with Loss: 1.6197552680969238 with total_loss: 2517.7513464689255\n",
      "  Batch   233  of  1,613.    Elapsed: 0:00:32. with Loss: 1.828603744506836 with total_loss: 2519.3711017370224\n",
      "  Batch   234  of  1,613.    Elapsed: 0:00:32. with Loss: 1.8243985176086426 with total_loss: 2521.1997054815292\n",
      "  Batch   235  of  1,613.    Elapsed: 0:00:32. with Loss: 3.1101930141448975 with total_loss: 2523.024103999138\n",
      "  Batch   236  of  1,613.    Elapsed: 0:00:33. with Loss: 1.175821304321289 with total_loss: 2526.134297013283\n",
      "  Batch   237  of  1,613.    Elapsed: 0:00:33. with Loss: 2.3976387977600098 with total_loss: 2527.310118317604\n",
      "  Batch   238  of  1,613.    Elapsed: 0:00:33. with Loss: 1.4183181524276733 with total_loss: 2529.707757115364\n",
      "  Batch   239  of  1,613.    Elapsed: 0:00:33. with Loss: 1.662444829940796 with total_loss: 2531.1260752677917\n",
      "  Batch   240  of  1,613.    Elapsed: 0:00:33. with Loss: 1.562050223350525 with total_loss: 2532.7885200977325\n",
      "  Batch   241  of  1,613.    Elapsed: 0:00:33. with Loss: 1.9004735946655273 with total_loss: 2534.350570321083\n",
      "  Batch   242  of  1,613.    Elapsed: 0:00:33. with Loss: 1.273571491241455 with total_loss: 2536.2510439157486\n",
      "  Batch   243  of  1,613.    Elapsed: 0:00:34. with Loss: 3.01068115234375 with total_loss: 2537.52461540699\n",
      "  Batch   244  of  1,613.    Elapsed: 0:00:34. with Loss: 2.710433006286621 with total_loss: 2540.535296559334\n",
      "  Batch   245  of  1,613.    Elapsed: 0:00:34. with Loss: 1.0699666738510132 with total_loss: 2543.2457295656204\n",
      "  Batch   246  of  1,613.    Elapsed: 0:00:34. with Loss: 1.2643136978149414 with total_loss: 2544.3156962394714\n",
      "  Batch   247  of  1,613.    Elapsed: 0:00:34. with Loss: 1.414644718170166 with total_loss: 2545.5800099372864\n",
      "  Batch   248  of  1,613.    Elapsed: 0:00:34. with Loss: 1.6206210851669312 with total_loss: 2546.9946546554565\n",
      "  Batch   249  of  1,613.    Elapsed: 0:00:34. with Loss: 1.0459542274475098 with total_loss: 2548.6152757406235\n",
      "  Batch   250  of  1,613.    Elapsed: 0:00:35. with Loss: 1.1422837972640991 with total_loss: 2549.661229968071\n",
      "  Batch   251  of  1,613.    Elapsed: 0:00:35. with Loss: 1.6768625974655151 with total_loss: 2550.803513765335\n",
      "  Batch   252  of  1,613.    Elapsed: 0:00:35. with Loss: 2.272736072540283 with total_loss: 2552.4803763628006\n",
      "  Batch   253  of  1,613.    Elapsed: 0:00:35. with Loss: 1.4559346437454224 with total_loss: 2554.753112435341\n",
      "  Batch   254  of  1,613.    Elapsed: 0:00:35. with Loss: 1.4204134941101074 with total_loss: 2556.2090470790863\n",
      "  Batch   255  of  1,613.    Elapsed: 0:00:35. with Loss: 1.3461768627166748 with total_loss: 2557.6294605731964\n",
      "  Batch   256  of  1,613.    Elapsed: 0:00:35. with Loss: 1.3142167329788208 with total_loss: 2558.975637435913\n",
      "  Batch   257  of  1,613.    Elapsed: 0:00:36. with Loss: 1.2719517946243286 with total_loss: 2560.289854168892\n",
      "  Batch   258  of  1,613.    Elapsed: 0:00:36. with Loss: 2.0953803062438965 with total_loss: 2561.5618059635162\n",
      "  Batch   259  of  1,613.    Elapsed: 0:00:36. with Loss: 1.2585113048553467 with total_loss: 2563.65718626976\n",
      "  Batch   260  of  1,613.    Elapsed: 0:00:36. with Loss: 1.7552247047424316 with total_loss: 2564.9156975746155\n",
      "  Batch   261  of  1,613.    Elapsed: 0:00:36. with Loss: 1.3225750923156738 with total_loss: 2566.670922279358\n",
      "  Batch   262  of  1,613.    Elapsed: 0:00:36. with Loss: 1.5001518726348877 with total_loss: 2567.9934973716736\n",
      "  Batch   263  of  1,613.    Elapsed: 0:00:36. with Loss: 2.096034288406372 with total_loss: 2569.4936492443085\n",
      "  Batch   264  of  1,613.    Elapsed: 0:00:36. with Loss: 1.2537174224853516 with total_loss: 2571.589683532715\n",
      "  Batch   265  of  1,613.    Elapsed: 0:00:37. with Loss: 1.6097460985183716 with total_loss: 2572.8434009552\n",
      "  Batch   266  of  1,613.    Elapsed: 0:00:37. with Loss: 1.6733025312423706 with total_loss: 2574.4531470537186\n",
      "  Batch   267  of  1,613.    Elapsed: 0:00:37. with Loss: 2.6085214614868164 with total_loss: 2576.126449584961\n",
      "  Batch   268  of  1,613.    Elapsed: 0:00:37. with Loss: 1.3013856410980225 with total_loss: 2578.7349710464478\n",
      "  Batch   269  of  1,613.    Elapsed: 0:00:37. with Loss: 1.345012903213501 with total_loss: 2580.036356687546\n",
      "  Batch   270  of  1,613.    Elapsed: 0:00:37. with Loss: 1.1782422065734863 with total_loss: 2581.3813695907593\n",
      "  Batch   271  of  1,613.    Elapsed: 0:00:37. with Loss: 2.334752082824707 with total_loss: 2582.5596117973328\n",
      "  Batch   272  of  1,613.    Elapsed: 0:00:38. with Loss: 2.4692063331604004 with total_loss: 2584.8943638801575\n",
      "  Batch   273  of  1,613.    Elapsed: 0:00:38. with Loss: 1.8500117063522339 with total_loss: 2587.363570213318\n",
      "  Batch   274  of  1,613.    Elapsed: 0:00:38. with Loss: 1.8987736701965332 with total_loss: 2589.21358191967\n",
      "  Batch   275  of  1,613.    Elapsed: 0:00:38. with Loss: 1.7874382734298706 with total_loss: 2591.1123555898666\n",
      "  Batch   276  of  1,613.    Elapsed: 0:00:38. with Loss: 2.264204978942871 with total_loss: 2592.8997938632965\n",
      "  Batch   277  of  1,613.    Elapsed: 0:00:38. with Loss: 2.2712957859039307 with total_loss: 2595.1639988422394\n",
      "  Batch   278  of  1,613.    Elapsed: 0:00:38. with Loss: 2.488629102706909 with total_loss: 2597.4352946281433\n",
      "  Batch   279  of  1,613.    Elapsed: 0:00:39. with Loss: 3.942662000656128 with total_loss: 2599.92392373085\n",
      "  Batch   280  of  1,613.    Elapsed: 0:00:39. with Loss: 1.556372880935669 with total_loss: 2603.8665857315063\n",
      "  Batch   281  of  1,613.    Elapsed: 0:00:39. with Loss: 2.582223415374756 with total_loss: 2605.422958612442\n",
      "  Batch   282  of  1,613.    Elapsed: 0:00:39. with Loss: 2.1814348697662354 with total_loss: 2608.0051820278168\n",
      "  Batch   283  of  1,613.    Elapsed: 0:00:39. with Loss: 2.1054770946502686 with total_loss: 2610.186616897583\n",
      "  Batch   284  of  1,613.    Elapsed: 0:00:39. with Loss: 3.6693480014801025 with total_loss: 2612.2920939922333\n",
      "  Batch   285  of  1,613.    Elapsed: 0:00:39. with Loss: 1.258130669593811 with total_loss: 2615.9614419937134\n",
      "  Batch   286  of  1,613.    Elapsed: 0:00:40. with Loss: 2.9016714096069336 with total_loss: 2617.219572663307\n",
      "  Batch   287  of  1,613.    Elapsed: 0:00:40. with Loss: 2.0622169971466064 with total_loss: 2620.121244072914\n",
      "  Batch   288  of  1,613.    Elapsed: 0:00:40. with Loss: 1.768735408782959 with total_loss: 2622.1834610700607\n",
      "  Batch   289  of  1,613.    Elapsed: 0:00:40. with Loss: 4.904003620147705 with total_loss: 2623.9521964788437\n",
      "  Batch   290  of  1,613.    Elapsed: 0:00:40. with Loss: 2.498945713043213 with total_loss: 2628.8562000989914\n",
      "  Batch   291  of  1,613.    Elapsed: 0:00:40. with Loss: 2.5661678314208984 with total_loss: 2631.3551458120346\n",
      "  Batch   292  of  1,613.    Elapsed: 0:00:40. with Loss: 1.2996129989624023 with total_loss: 2633.9213136434555\n",
      "  Batch   293  of  1,613.    Elapsed: 0:00:40. with Loss: 1.078984260559082 with total_loss: 2635.220926642418\n",
      "  Batch   294  of  1,613.    Elapsed: 0:00:41. with Loss: 1.3950830698013306 with total_loss: 2636.299910902977\n",
      "  Batch   295  of  1,613.    Elapsed: 0:00:41. with Loss: 1.5587586164474487 with total_loss: 2637.6949939727783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   296  of  1,613.    Elapsed: 0:00:41. with Loss: 1.419840693473816 with total_loss: 2639.2537525892258\n",
      "  Batch   297  of  1,613.    Elapsed: 0:00:41. with Loss: 1.1941930055618286 with total_loss: 2640.6735932826996\n",
      "  Batch   298  of  1,613.    Elapsed: 0:00:41. with Loss: 2.149625778198242 with total_loss: 2641.8677862882614\n",
      "  Batch   299  of  1,613.    Elapsed: 0:00:41. with Loss: 0.8293236494064331 with total_loss: 2644.0174120664597\n",
      "  Batch   300  of  1,613.    Elapsed: 0:00:41. with Loss: 2.782708168029785 with total_loss: 2644.846735715866\n",
      "  Batch   301  of  1,613.    Elapsed: 0:00:42. with Loss: 1.3070101737976074 with total_loss: 2647.629443883896\n",
      "  Batch   302  of  1,613.    Elapsed: 0:00:42. with Loss: 1.9271670579910278 with total_loss: 2648.9364540576935\n",
      "  Batch   303  of  1,613.    Elapsed: 0:00:42. with Loss: 2.1739609241485596 with total_loss: 2650.8636211156845\n",
      "  Batch   304  of  1,613.    Elapsed: 0:00:42. with Loss: 1.8465644121170044 with total_loss: 2653.037582039833\n",
      "  Batch   305  of  1,613.    Elapsed: 0:00:42. with Loss: 2.8154892921447754 with total_loss: 2654.88414645195\n",
      "  Batch   306  of  1,613.    Elapsed: 0:00:42. with Loss: 1.2050493955612183 with total_loss: 2657.699635744095\n",
      "  Batch   307  of  1,613.    Elapsed: 0:00:42. with Loss: 1.7170883417129517 with total_loss: 2658.904685139656\n",
      "  Batch   308  of  1,613.    Elapsed: 0:00:43. with Loss: 1.6736630201339722 with total_loss: 2660.621773481369\n",
      "  Batch   309  of  1,613.    Elapsed: 0:00:43. with Loss: 2.199897527694702 with total_loss: 2662.295436501503\n",
      "  Batch   310  of  1,613.    Elapsed: 0:00:43. with Loss: 1.3031495809555054 with total_loss: 2664.4953340291977\n",
      "  Batch   311  of  1,613.    Elapsed: 0:00:43. with Loss: 2.848797559738159 with total_loss: 2665.798483610153\n",
      "  Batch   312  of  1,613.    Elapsed: 0:00:43. with Loss: 1.363271713256836 with total_loss: 2668.6472811698914\n",
      "  Batch   313  of  1,613.    Elapsed: 0:00:43. with Loss: 1.6589549779891968 with total_loss: 2670.010552883148\n",
      "  Batch   314  of  1,613.    Elapsed: 0:00:43. with Loss: 1.0308865308761597 with total_loss: 2671.6695078611374\n",
      "  Batch   315  of  1,613.    Elapsed: 0:00:44. with Loss: 4.523940086364746 with total_loss: 2672.7003943920135\n",
      "  Batch   316  of  1,613.    Elapsed: 0:00:44. with Loss: 1.9026591777801514 with total_loss: 2677.2243344783783\n",
      "  Batch   317  of  1,613.    Elapsed: 0:00:44. with Loss: 1.19735848903656 with total_loss: 2679.1269936561584\n",
      "  Batch   318  of  1,613.    Elapsed: 0:00:44. with Loss: 1.4665558338165283 with total_loss: 2680.324352145195\n",
      "  Batch   319  of  1,613.    Elapsed: 0:00:44. with Loss: 1.8851953744888306 with total_loss: 2681.7909079790115\n",
      "  Batch   320  of  1,613.    Elapsed: 0:00:44. with Loss: 1.6994736194610596 with total_loss: 2683.6761033535004\n",
      "  Batch   321  of  1,613.    Elapsed: 0:00:44. with Loss: 1.3749337196350098 with total_loss: 2685.3755769729614\n",
      "  Batch   322  of  1,613.    Elapsed: 0:00:44. with Loss: 1.704001545906067 with total_loss: 2686.7505106925964\n",
      "  Batch   323  of  1,613.    Elapsed: 0:00:45. with Loss: 2.1181800365448 with total_loss: 2688.4545122385025\n",
      "  Batch   324  of  1,613.    Elapsed: 0:00:45. with Loss: 1.2594099044799805 with total_loss: 2690.5726922750473\n",
      "  Batch   325  of  1,613.    Elapsed: 0:00:45. with Loss: 0.8751879334449768 with total_loss: 2691.8321021795273\n",
      "  Batch   326  of  1,613.    Elapsed: 0:00:45. with Loss: 1.0468313694000244 with total_loss: 2692.7072901129723\n",
      "  Batch   327  of  1,613.    Elapsed: 0:00:45. with Loss: 1.6766821146011353 with total_loss: 2693.7541214823723\n",
      "  Batch   328  of  1,613.    Elapsed: 0:00:45. with Loss: 2.540789842605591 with total_loss: 2695.4308035969734\n",
      "  Batch   329  of  1,613.    Elapsed: 0:00:45. with Loss: 1.518430471420288 with total_loss: 2697.971593439579\n",
      "  Batch   330  of  1,613.    Elapsed: 0:00:46. with Loss: 1.5970940589904785 with total_loss: 2699.4900239109993\n",
      "  Batch   331  of  1,613.    Elapsed: 0:00:46. with Loss: 1.773301601409912 with total_loss: 2701.08711796999\n",
      "  Batch   332  of  1,613.    Elapsed: 0:00:46. with Loss: 2.7514398097991943 with total_loss: 2702.8604195713997\n",
      "  Batch   333  of  1,613.    Elapsed: 0:00:46. with Loss: 0.9174079895019531 with total_loss: 2705.611859381199\n",
      "  Batch   334  of  1,613.    Elapsed: 0:00:46. with Loss: 3.1457126140594482 with total_loss: 2706.529267370701\n",
      "  Batch   335  of  1,613.    Elapsed: 0:00:46. with Loss: 1.8733911514282227 with total_loss: 2709.6749799847603\n",
      "  Batch   336  of  1,613.    Elapsed: 0:00:46. with Loss: 1.580228567123413 with total_loss: 2711.5483711361885\n",
      "  Batch   337  of  1,613.    Elapsed: 0:00:47. with Loss: 1.5037709474563599 with total_loss: 2713.128599703312\n",
      "  Batch   338  of  1,613.    Elapsed: 0:00:47. with Loss: 6.708778381347656 with total_loss: 2714.6323706507683\n",
      "  Batch   339  of  1,613.    Elapsed: 0:00:47. with Loss: 2.6069774627685547 with total_loss: 2721.341149032116\n",
      "  Batch   340  of  1,613.    Elapsed: 0:00:47. with Loss: 1.1616839170455933 with total_loss: 2723.9481264948845\n",
      "  Batch   341  of  1,613.    Elapsed: 0:00:47. with Loss: 1.0582178831100464 with total_loss: 2725.10981041193\n",
      "  Batch   342  of  1,613.    Elapsed: 0:00:47. with Loss: 1.5034738779067993 with total_loss: 2726.16802829504\n",
      "  Batch   343  of  1,613.    Elapsed: 0:00:47. with Loss: 0.9879491925239563 with total_loss: 2727.671502172947\n",
      "  Batch   344  of  1,613.    Elapsed: 0:00:48. with Loss: 1.788700819015503 with total_loss: 2728.659451365471\n",
      "  Batch   345  of  1,613.    Elapsed: 0:00:48. with Loss: 1.7666434049606323 with total_loss: 2730.4481521844864\n",
      "  Batch   346  of  1,613.    Elapsed: 0:00:48. with Loss: 1.583754539489746 with total_loss: 2732.214795589447\n",
      "  Batch   347  of  1,613.    Elapsed: 0:00:48. with Loss: 1.2355328798294067 with total_loss: 2733.7985501289368\n",
      "  Batch   348  of  1,613.    Elapsed: 0:00:48. with Loss: 1.1114847660064697 with total_loss: 2735.034083008766\n",
      "  Batch   349  of  1,613.    Elapsed: 0:00:48. with Loss: 0.9217908978462219 with total_loss: 2736.1455677747726\n",
      "  Batch   350  of  1,613.    Elapsed: 0:00:48. with Loss: 1.1144300699234009 with total_loss: 2737.067358672619\n",
      "  Batch   351  of  1,613.    Elapsed: 0:00:48. with Loss: 1.829390048980713 with total_loss: 2738.1817887425423\n",
      "  Batch   352  of  1,613.    Elapsed: 0:00:49. with Loss: 3.13213849067688 with total_loss: 2740.011178791523\n",
      "  Batch   353  of  1,613.    Elapsed: 0:00:49. with Loss: 1.417365312576294 with total_loss: 2743.1433172822\n",
      "  Batch   354  of  1,613.    Elapsed: 0:00:49. with Loss: 0.8980553150177002 with total_loss: 2744.560682594776\n",
      "  Batch   355  of  1,613.    Elapsed: 0:00:49. with Loss: 1.4097435474395752 with total_loss: 2745.458737909794\n",
      "  Batch   356  of  1,613.    Elapsed: 0:00:49. with Loss: 1.2774418592453003 with total_loss: 2746.8684814572334\n",
      "  Batch   357  of  1,613.    Elapsed: 0:00:49. with Loss: 1.002611517906189 with total_loss: 2748.1459233164787\n",
      "  Batch   358  of  1,613.    Elapsed: 0:00:49. with Loss: 1.379956841468811 with total_loss: 2749.148534834385\n",
      "  Batch   359  of  1,613.    Elapsed: 0:00:50. with Loss: 2.4368910789489746 with total_loss: 2750.5284916758537\n",
      "  Batch   360  of  1,613.    Elapsed: 0:00:50. with Loss: 4.676334857940674 with total_loss: 2752.9653827548027\n",
      "  Batch   361  of  1,613.    Elapsed: 0:00:50. with Loss: 2.317154884338379 with total_loss: 2757.6417176127434\n",
      "  Batch   362  of  1,613.    Elapsed: 0:00:50. with Loss: 1.5715041160583496 with total_loss: 2759.9588724970818\n",
      "  Batch   363  of  1,613.    Elapsed: 0:00:50. with Loss: 1.0688189268112183 with total_loss: 2761.53037661314\n",
      "  Batch   364  of  1,613.    Elapsed: 0:00:50. with Loss: 1.196379542350769 with total_loss: 2762.5991955399513\n",
      "  Batch   365  of  1,613.    Elapsed: 0:00:50. with Loss: 1.209632396697998 with total_loss: 2763.795575082302\n",
      "  Batch   366  of  1,613.    Elapsed: 0:00:51. with Loss: 1.5958828926086426 with total_loss: 2765.005207479\n",
      "  Batch   367  of  1,613.    Elapsed: 0:00:51. with Loss: 1.7970106601715088 with total_loss: 2766.6010903716087\n",
      "  Batch   368  of  1,613.    Elapsed: 0:00:51. with Loss: 1.3474048376083374 with total_loss: 2768.3981010317802\n",
      "  Batch   369  of  1,613.    Elapsed: 0:00:51. with Loss: 1.8618407249450684 with total_loss: 2769.7455058693886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   370  of  1,613.    Elapsed: 0:00:51. with Loss: 0.9706529974937439 with total_loss: 2771.6073465943336\n",
      "  Batch   371  of  1,613.    Elapsed: 0:00:51. with Loss: 1.2270798683166504 with total_loss: 2772.5779995918274\n",
      "  Batch   372  of  1,613.    Elapsed: 0:00:51. with Loss: 1.603442668914795 with total_loss: 2773.805079460144\n",
      "  Batch   373  of  1,613.    Elapsed: 0:00:51. with Loss: 0.9984570741653442 with total_loss: 2775.408522129059\n",
      "  Batch   374  of  1,613.    Elapsed: 0:00:52. with Loss: 3.032151699066162 with total_loss: 2776.406979203224\n",
      "  Batch   375  of  1,613.    Elapsed: 0:00:52. with Loss: 1.4301583766937256 with total_loss: 2779.4391309022903\n",
      "  Batch   376  of  1,613.    Elapsed: 0:00:52. with Loss: 1.2429563999176025 with total_loss: 2780.869289278984\n",
      "  Batch   377  of  1,613.    Elapsed: 0:00:52. with Loss: 1.6287492513656616 with total_loss: 2782.1122456789017\n",
      "  Batch   378  of  1,613.    Elapsed: 0:00:52. with Loss: 0.9946415424346924 with total_loss: 2783.7409949302673\n",
      "  Batch   379  of  1,613.    Elapsed: 0:00:52. with Loss: 1.3946635723114014 with total_loss: 2784.735636472702\n",
      "  Batch   380  of  1,613.    Elapsed: 0:00:52. with Loss: 2.2367727756500244 with total_loss: 2786.1303000450134\n",
      "  Batch   381  of  1,613.    Elapsed: 0:00:53. with Loss: 3.120913505554199 with total_loss: 2788.3670728206635\n",
      "  Batch   382  of  1,613.    Elapsed: 0:00:53. with Loss: 1.5233206748962402 with total_loss: 2791.4879863262177\n",
      "  Batch   383  of  1,613.    Elapsed: 0:00:53. with Loss: 1.38746178150177 with total_loss: 2793.011307001114\n",
      "  Batch   384  of  1,613.    Elapsed: 0:00:53. with Loss: 1.2300604581832886 with total_loss: 2794.3987687826157\n",
      "  Batch   385  of  1,613.    Elapsed: 0:00:53. with Loss: 2.2728168964385986 with total_loss: 2795.628829240799\n",
      "  Batch   386  of  1,613.    Elapsed: 0:00:53. with Loss: 2.1198441982269287 with total_loss: 2797.9016461372375\n",
      "  Batch   387  of  1,613.    Elapsed: 0:00:53. with Loss: 1.3866616487503052 with total_loss: 2800.0214903354645\n",
      "  Batch   388  of  1,613.    Elapsed: 0:00:54. with Loss: 2.0452511310577393 with total_loss: 2801.408151984215\n",
      "  Batch   389  of  1,613.    Elapsed: 0:00:54. with Loss: 2.0017802715301514 with total_loss: 2803.4534031152725\n",
      "  Batch   390  of  1,613.    Elapsed: 0:00:54. with Loss: 1.3403154611587524 with total_loss: 2805.4551833868027\n",
      "  Batch   391  of  1,613.    Elapsed: 0:00:54. with Loss: 1.5977669954299927 with total_loss: 2806.7954988479614\n",
      "  Batch   392  of  1,613.    Elapsed: 0:00:54. with Loss: 3.068956136703491 with total_loss: 2808.3932658433914\n",
      "  Batch   393  of  1,613.    Elapsed: 0:00:54. with Loss: 1.5125315189361572 with total_loss: 2811.462221980095\n",
      "  Batch   394  of  1,613.    Elapsed: 0:00:54. with Loss: 1.8014392852783203 with total_loss: 2812.974753499031\n",
      "  Batch   395  of  1,613.    Elapsed: 0:00:55. with Loss: 1.7767293453216553 with total_loss: 2814.7761927843094\n",
      "  Batch   396  of  1,613.    Elapsed: 0:00:55. with Loss: 0.9936608672142029 with total_loss: 2816.552922129631\n",
      "  Batch   397  of  1,613.    Elapsed: 0:00:55. with Loss: 0.9347852468490601 with total_loss: 2817.5465829968452\n",
      "  Batch   398  of  1,613.    Elapsed: 0:00:55. with Loss: 0.8931900858879089 with total_loss: 2818.4813682436943\n",
      "  Batch   399  of  1,613.    Elapsed: 0:00:55. with Loss: 2.8896055221557617 with total_loss: 2819.374558329582\n",
      "  Batch   400  of  1,613.    Elapsed: 0:00:55. with Loss: 1.5630640983581543 with total_loss: 2822.264163851738\n",
      "  Batch   401  of  1,613.    Elapsed: 0:00:55. with Loss: 1.1769698858261108 with total_loss: 2823.827227950096\n",
      "  Batch   402  of  1,613.    Elapsed: 0:00:55. with Loss: 1.2352535724639893 with total_loss: 2825.0041978359222\n",
      "  Batch   403  of  1,613.    Elapsed: 0:00:56. with Loss: 1.6554442644119263 with total_loss: 2826.2394514083862\n",
      "  Batch   404  of  1,613.    Elapsed: 0:00:56. with Loss: 1.0748324394226074 with total_loss: 2827.894895672798\n",
      "  Batch   405  of  1,613.    Elapsed: 0:00:56. with Loss: 1.9324212074279785 with total_loss: 2828.9697281122208\n",
      "  Batch   406  of  1,613.    Elapsed: 0:00:56. with Loss: 1.9442895650863647 with total_loss: 2830.9021493196487\n",
      "  Batch   407  of  1,613.    Elapsed: 0:00:56. with Loss: 1.5095168352127075 with total_loss: 2832.846438884735\n",
      "  Batch   408  of  1,613.    Elapsed: 0:00:56. with Loss: 1.8681501150131226 with total_loss: 2834.355955719948\n",
      "  Batch   409  of  1,613.    Elapsed: 0:00:56. with Loss: 1.645111083984375 with total_loss: 2836.224105834961\n",
      "  Batch   410  of  1,613.    Elapsed: 0:00:57. with Loss: 1.5278410911560059 with total_loss: 2837.8692169189453\n",
      "  Batch   411  of  1,613.    Elapsed: 0:00:57. with Loss: 1.3469104766845703 with total_loss: 2839.3970580101013\n",
      "  Batch   412  of  1,613.    Elapsed: 0:00:57. with Loss: 0.7033745050430298 with total_loss: 2840.743968486786\n",
      "  Batch   413  of  1,613.    Elapsed: 0:00:57. with Loss: 1.3916923999786377 with total_loss: 2841.447342991829\n",
      "  Batch   414  of  1,613.    Elapsed: 0:00:57. with Loss: 1.926548719406128 with total_loss: 2842.8390353918076\n",
      "  Batch   415  of  1,613.    Elapsed: 0:00:57. with Loss: 1.8945330381393433 with total_loss: 2844.7655841112137\n",
      "  Batch   416  of  1,613.    Elapsed: 0:00:57. with Loss: 1.2540165185928345 with total_loss: 2846.660117149353\n",
      "  Batch   417  of  1,613.    Elapsed: 0:00:58. with Loss: 1.134064793586731 with total_loss: 2847.914133667946\n",
      "  Batch   418  of  1,613.    Elapsed: 0:00:58. with Loss: 1.5588057041168213 with total_loss: 2849.0481984615326\n",
      "  Batch   419  of  1,613.    Elapsed: 0:00:58. with Loss: 1.778862476348877 with total_loss: 2850.6070041656494\n",
      "  Batch   420  of  1,613.    Elapsed: 0:00:58. with Loss: 2.4796252250671387 with total_loss: 2852.3858666419983\n",
      "  Batch   421  of  1,613.    Elapsed: 0:00:58. with Loss: 1.8514461517333984 with total_loss: 2854.8654918670654\n",
      "  Batch   422  of  1,613.    Elapsed: 0:00:58. with Loss: 1.1011449098587036 with total_loss: 2856.716938018799\n",
      "  Batch   423  of  1,613.    Elapsed: 0:00:58. with Loss: 1.1030821800231934 with total_loss: 2857.8180829286575\n",
      "  Batch   424  of  1,613.    Elapsed: 0:00:59. with Loss: 1.4452929496765137 with total_loss: 2858.9211651086807\n",
      "  Batch   425  of  1,613.    Elapsed: 0:00:59. with Loss: 1.7188777923583984 with total_loss: 2860.3664580583572\n",
      "  Batch   426  of  1,613.    Elapsed: 0:00:59. with Loss: 1.2188045978546143 with total_loss: 2862.0853358507156\n",
      "  Batch   427  of  1,613.    Elapsed: 0:00:59. with Loss: 2.126779794692993 with total_loss: 2863.3041404485703\n",
      "  Batch   428  of  1,613.    Elapsed: 0:00:59. with Loss: 2.113692283630371 with total_loss: 2865.4309202432632\n",
      "  Batch   429  of  1,613.    Elapsed: 0:00:59. with Loss: 0.6647561192512512 with total_loss: 2867.5446125268936\n",
      "  Batch   430  of  1,613.    Elapsed: 0:00:59. with Loss: 1.587306022644043 with total_loss: 2868.209368646145\n",
      "  Batch   431  of  1,613.    Elapsed: 0:00:59. with Loss: 1.0126017332077026 with total_loss: 2869.796674668789\n",
      "  Batch   432  of  1,613.    Elapsed: 0:01:00. with Loss: 1.1097549200057983 with total_loss: 2870.8092764019966\n",
      "  Batch   433  of  1,613.    Elapsed: 0:01:00. with Loss: 1.185935139656067 with total_loss: 2871.9190313220024\n",
      "  Batch   434  of  1,613.    Elapsed: 0:01:00. with Loss: 1.2385042905807495 with total_loss: 2873.1049664616585\n",
      "  Batch   435  of  1,613.    Elapsed: 0:01:00. with Loss: 2.3675200939178467 with total_loss: 2874.343470752239\n",
      "  Batch   436  of  1,613.    Elapsed: 0:01:00. with Loss: 0.8883139491081238 with total_loss: 2876.710990846157\n",
      "  Batch   437  of  1,613.    Elapsed: 0:01:00. with Loss: 1.254014492034912 with total_loss: 2877.599304795265\n",
      "  Batch   438  of  1,613.    Elapsed: 0:01:00. with Loss: 4.8560357093811035 with total_loss: 2878.8533192873\n",
      "  Batch   439  of  1,613.    Elapsed: 0:01:01. with Loss: 1.2227544784545898 with total_loss: 2883.709354996681\n",
      "  Batch   440  of  1,613.    Elapsed: 0:01:01. with Loss: 1.4995499849319458 with total_loss: 2884.932109475136\n",
      "  Batch   441  of  1,613.    Elapsed: 0:01:01. with Loss: 1.6086112260818481 with total_loss: 2886.4316594600677\n",
      "  Batch   442  of  1,613.    Elapsed: 0:01:01. with Loss: 1.6722773313522339 with total_loss: 2888.0402706861496\n",
      "  Batch   443  of  1,613.    Elapsed: 0:01:01. with Loss: 1.122382402420044 with total_loss: 2889.712548017502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   444  of  1,613.    Elapsed: 0:01:01. with Loss: 1.4463189840316772 with total_loss: 2890.834930419922\n",
      "  Batch   445  of  1,613.    Elapsed: 0:01:01. with Loss: 1.4252859354019165 with total_loss: 2892.2812494039536\n",
      "  Batch   446  of  1,613.    Elapsed: 0:01:02. with Loss: 0.9365657567977905 with total_loss: 2893.7065353393555\n",
      "  Batch   447  of  1,613.    Elapsed: 0:01:02. with Loss: 2.329559326171875 with total_loss: 2894.6431010961533\n",
      "  Batch   448  of  1,613.    Elapsed: 0:01:02. with Loss: 1.7424226999282837 with total_loss: 2896.972660422325\n",
      "  Batch   449  of  1,613.    Elapsed: 0:01:02. with Loss: 1.7120659351348877 with total_loss: 2898.7150831222534\n",
      "  Batch   450  of  1,613.    Elapsed: 0:01:02. with Loss: 1.7513617277145386 with total_loss: 2900.4271490573883\n",
      "  Batch   451  of  1,613.    Elapsed: 0:01:02. with Loss: 1.1071624755859375 with total_loss: 2902.178510785103\n",
      "  Batch   452  of  1,613.    Elapsed: 0:01:02. with Loss: 2.9858624935150146 with total_loss: 2903.285673260689\n",
      "  Batch   453  of  1,613.    Elapsed: 0:01:03. with Loss: 1.1265751123428345 with total_loss: 2906.271535754204\n",
      "  Batch   454  of  1,613.    Elapsed: 0:01:03. with Loss: 1.1940888166427612 with total_loss: 2907.3981108665466\n",
      "  Batch   455  of  1,613.    Elapsed: 0:01:03. with Loss: 1.6463568210601807 with total_loss: 2908.5921996831894\n",
      "  Batch   456  of  1,613.    Elapsed: 0:01:03. with Loss: 1.777233600616455 with total_loss: 2910.2385565042496\n",
      "  Batch   457  of  1,613.    Elapsed: 0:01:03. with Loss: 0.9162343740463257 with total_loss: 2912.015790104866\n",
      "  Batch   458  of  1,613.    Elapsed: 0:01:03. with Loss: 2.7561228275299072 with total_loss: 2912.9320244789124\n",
      "  Batch   459  of  1,613.    Elapsed: 0:01:03. with Loss: 1.6067454814910889 with total_loss: 2915.6881473064423\n",
      "  Batch   460  of  1,613.    Elapsed: 0:01:03. with Loss: 1.3667237758636475 with total_loss: 2917.2948927879333\n",
      "  Batch   461  of  1,613.    Elapsed: 0:01:04. with Loss: 0.9637291431427002 with total_loss: 2918.661616563797\n",
      "  Batch   462  of  1,613.    Elapsed: 0:01:04. with Loss: 1.842564582824707 with total_loss: 2919.6253457069397\n",
      "  Batch   463  of  1,613.    Elapsed: 0:01:04. with Loss: 5.324353218078613 with total_loss: 2921.4679102897644\n",
      "  Batch   464  of  1,613.    Elapsed: 0:01:04. with Loss: 1.2291510105133057 with total_loss: 2926.792263507843\n",
      "  Batch   465  of  1,613.    Elapsed: 0:01:04. with Loss: 1.638840675354004 with total_loss: 2928.0214145183563\n",
      "  Batch   466  of  1,613.    Elapsed: 0:01:04. with Loss: 1.1697850227355957 with total_loss: 2929.6602551937103\n",
      "  Batch   467  of  1,613.    Elapsed: 0:01:04. with Loss: 1.7487164735794067 with total_loss: 2930.830040216446\n",
      "  Batch   468  of  1,613.    Elapsed: 0:01:05. with Loss: 1.8183799982070923 with total_loss: 2932.5787566900253\n",
      "  Batch   469  of  1,613.    Elapsed: 0:01:05. with Loss: 1.9431146383285522 with total_loss: 2934.3971366882324\n",
      "  Batch   470  of  1,613.    Elapsed: 0:01:05. with Loss: 1.311419129371643 with total_loss: 2936.340251326561\n",
      "  Batch   471  of  1,613.    Elapsed: 0:01:05. with Loss: 1.5618988275527954 with total_loss: 2937.6516704559326\n",
      "  Batch   472  of  1,613.    Elapsed: 0:01:05. with Loss: 1.960615873336792 with total_loss: 2939.2135692834854\n",
      "  Batch   473  of  1,613.    Elapsed: 0:01:05. with Loss: 1.379493236541748 with total_loss: 2941.174185156822\n",
      "  Batch   474  of  1,613.    Elapsed: 0:01:05. with Loss: 3.8858697414398193 with total_loss: 2942.553678393364\n",
      "  Batch   475  of  1,613.    Elapsed: 0:01:06. with Loss: 1.2511035203933716 with total_loss: 2946.4395481348038\n",
      "  Batch   476  of  1,613.    Elapsed: 0:01:06. with Loss: 1.5186424255371094 with total_loss: 2947.690651655197\n",
      "  Batch   477  of  1,613.    Elapsed: 0:01:06. with Loss: 1.9965969324111938 with total_loss: 2949.2092940807343\n",
      "  Batch   478  of  1,613.    Elapsed: 0:01:06. with Loss: 1.3235259056091309 with total_loss: 2951.2058910131454\n",
      "  Batch   479  of  1,613.    Elapsed: 0:01:06. with Loss: 3.1646018028259277 with total_loss: 2952.5294169187546\n",
      "  Batch   480  of  1,613.    Elapsed: 0:01:06. with Loss: 1.2375483512878418 with total_loss: 2955.6940187215805\n",
      "  Batch   481  of  1,613.    Elapsed: 0:01:06. with Loss: 1.9422128200531006 with total_loss: 2956.9315670728683\n",
      "  Batch   482  of  1,613.    Elapsed: 0:01:07. with Loss: 1.9085955619812012 with total_loss: 2958.8737798929214\n",
      "  Batch   483  of  1,613.    Elapsed: 0:01:07. with Loss: 1.5791248083114624 with total_loss: 2960.7823754549026\n",
      "  Batch   484  of  1,613.    Elapsed: 0:01:07. with Loss: 2.207612991333008 with total_loss: 2962.361500263214\n",
      "  Batch   485  of  1,613.    Elapsed: 0:01:07. with Loss: 1.8898627758026123 with total_loss: 2964.569113254547\n",
      "  Batch   486  of  1,613.    Elapsed: 0:01:07. with Loss: 1.2695995569229126 with total_loss: 2966.4589760303497\n",
      "  Batch   487  of  1,613.    Elapsed: 0:01:07. with Loss: 0.7609913349151611 with total_loss: 2967.7285755872726\n",
      "  Batch   488  of  1,613.    Elapsed: 0:01:07. with Loss: 1.8041311502456665 with total_loss: 2968.489566922188\n",
      "  Batch   489  of  1,613.    Elapsed: 0:01:07. with Loss: 2.605375051498413 with total_loss: 2970.2936980724335\n",
      "  Batch   490  of  1,613.    Elapsed: 0:01:08. with Loss: 2.735311508178711 with total_loss: 2972.899073123932\n",
      "  Batch   491  of  1,613.    Elapsed: 0:01:08. with Loss: 1.4598976373672485 with total_loss: 2975.6343846321106\n",
      "  Batch   492  of  1,613.    Elapsed: 0:01:08. with Loss: 2.01778244972229 with total_loss: 2977.094282269478\n",
      "  Batch   493  of  1,613.    Elapsed: 0:01:08. with Loss: 1.8663122653961182 with total_loss: 2979.1120647192\n",
      "  Batch   494  of  1,613.    Elapsed: 0:01:08. with Loss: 2.4771888256073 with total_loss: 2980.9783769845963\n",
      "  Batch   495  of  1,613.    Elapsed: 0:01:08. with Loss: 1.7621349096298218 with total_loss: 2983.4555658102036\n",
      "  Batch   496  of  1,613.    Elapsed: 0:01:08. with Loss: 1.5393238067626953 with total_loss: 2985.2177007198334\n",
      "  Batch   497  of  1,613.    Elapsed: 0:01:09. with Loss: 6.18541145324707 with total_loss: 2986.757024526596\n",
      "  Batch   498  of  1,613.    Elapsed: 0:01:09. with Loss: 1.5833793878555298 with total_loss: 2992.942435979843\n",
      "  Batch   499  of  1,613.    Elapsed: 0:01:09. with Loss: 1.434167742729187 with total_loss: 2994.5258153676987\n",
      "  Batch   500  of  1,613.    Elapsed: 0:01:09. with Loss: 1.830655574798584 with total_loss: 2995.959983110428\n",
      "  Batch   501  of  1,613.    Elapsed: 0:01:09. with Loss: 3.7972540855407715 with total_loss: 2997.7906386852264\n",
      "  Batch   502  of  1,613.    Elapsed: 0:01:09. with Loss: 1.1183388233184814 with total_loss: 3001.587892770767\n",
      "  Batch   503  of  1,613.    Elapsed: 0:01:09. with Loss: 2.295093536376953 with total_loss: 3002.7062315940857\n",
      "  Batch   504  of  1,613.    Elapsed: 0:01:10. with Loss: 1.6108675003051758 with total_loss: 3005.0013251304626\n",
      "  Batch   505  of  1,613.    Elapsed: 0:01:10. with Loss: 2.3791518211364746 with total_loss: 3006.612192630768\n",
      "  Batch   506  of  1,613.    Elapsed: 0:01:10. with Loss: 1.9469925165176392 with total_loss: 3008.9913444519043\n",
      "  Batch   507  of  1,613.    Elapsed: 0:01:10. with Loss: 1.8756331205368042 with total_loss: 3010.938336968422\n",
      "  Batch   508  of  1,613.    Elapsed: 0:01:10. with Loss: 1.7583668231964111 with total_loss: 3012.8139700889587\n",
      "  Batch   509  of  1,613.    Elapsed: 0:01:10. with Loss: 1.7372714281082153 with total_loss: 3014.572336912155\n",
      "  Batch   510  of  1,613.    Elapsed: 0:01:10. with Loss: 1.3030786514282227 with total_loss: 3016.3096083402634\n",
      "  Batch   511  of  1,613.    Elapsed: 0:01:11. with Loss: 1.116281270980835 with total_loss: 3017.6126869916916\n",
      "  Batch   512  of  1,613.    Elapsed: 0:01:11. with Loss: 1.251651406288147 with total_loss: 3018.7289682626724\n",
      "  Batch   513  of  1,613.    Elapsed: 0:01:11. with Loss: 1.6866309642791748 with total_loss: 3019.9806196689606\n",
      "  Batch   514  of  1,613.    Elapsed: 0:01:11. with Loss: 1.5306622982025146 with total_loss: 3021.6672506332397\n",
      "  Batch   515  of  1,613.    Elapsed: 0:01:11. with Loss: 1.9343359470367432 with total_loss: 3023.1979129314423\n",
      "  Batch   516  of  1,613.    Elapsed: 0:01:11. with Loss: 1.745316982269287 with total_loss: 3025.132248878479\n",
      "  Batch   517  of  1,613.    Elapsed: 0:01:11. with Loss: 4.81045389175415 with total_loss: 3026.8775658607483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   518  of  1,613.    Elapsed: 0:01:11. with Loss: 1.1742223501205444 with total_loss: 3031.6880197525024\n",
      "  Batch   519  of  1,613.    Elapsed: 0:01:12. with Loss: 1.398464322090149 with total_loss: 3032.862242102623\n",
      "  Batch   520  of  1,613.    Elapsed: 0:01:12. with Loss: 1.0955756902694702 with total_loss: 3034.260706424713\n",
      "  Batch   521  of  1,613.    Elapsed: 0:01:12. with Loss: 1.3598434925079346 with total_loss: 3035.3562821149826\n",
      "  Batch   522  of  1,613.    Elapsed: 0:01:12. with Loss: 1.11139976978302 with total_loss: 3036.7161256074905\n",
      "  Batch   523  of  1,613.    Elapsed: 0:01:12. with Loss: 1.7372716665267944 with total_loss: 3037.8275253772736\n",
      "  Batch   524  of  1,613.    Elapsed: 0:01:12. with Loss: 1.246300220489502 with total_loss: 3039.5647970438004\n",
      "  Batch   525  of  1,613.    Elapsed: 0:01:12. with Loss: 1.7501329183578491 with total_loss: 3040.81109726429\n",
      "  Batch   526  of  1,613.    Elapsed: 0:01:13. with Loss: 0.8974202275276184 with total_loss: 3042.5612301826477\n",
      "  Batch   527  of  1,613.    Elapsed: 0:01:13. with Loss: 1.2250690460205078 with total_loss: 3043.4586504101753\n",
      "  Batch   528  of  1,613.    Elapsed: 0:01:13. with Loss: 3.2415771484375 with total_loss: 3044.683719456196\n",
      "  Batch   529  of  1,613.    Elapsed: 0:01:13. with Loss: 1.4267445802688599 with total_loss: 3047.9252966046333\n",
      "  Batch   530  of  1,613.    Elapsed: 0:01:13. with Loss: 1.1987608671188354 with total_loss: 3049.352041184902\n",
      "  Batch   531  of  1,613.    Elapsed: 0:01:13. with Loss: 1.1508711576461792 with total_loss: 3050.550802052021\n",
      "  Batch   532  of  1,613.    Elapsed: 0:01:13. with Loss: 1.446646809577942 with total_loss: 3051.701673209667\n",
      "  Batch   533  of  1,613.    Elapsed: 0:01:14. with Loss: 1.5170553922653198 with total_loss: 3053.148320019245\n",
      "  Batch   534  of  1,613.    Elapsed: 0:01:14. with Loss: 0.9476903676986694 with total_loss: 3054.6653754115105\n",
      "  Batch   535  of  1,613.    Elapsed: 0:01:14. with Loss: 1.5369188785552979 with total_loss: 3055.613065779209\n",
      "  Batch   536  of  1,613.    Elapsed: 0:01:14. with Loss: 2.1539268493652344 with total_loss: 3057.1499846577644\n",
      "  Batch   537  of  1,613.    Elapsed: 0:01:14. with Loss: 1.3418036699295044 with total_loss: 3059.3039115071297\n",
      "  Batch   538  of  1,613.    Elapsed: 0:01:14. with Loss: 1.8202728033065796 with total_loss: 3060.645715177059\n",
      "  Batch   539  of  1,613.    Elapsed: 0:01:14. with Loss: 1.5969902276992798 with total_loss: 3062.4659879803658\n",
      "  Batch   540  of  1,613.    Elapsed: 0:01:15. with Loss: 1.269745111465454 with total_loss: 3064.062978208065\n",
      "  Batch   541  of  1,613.    Elapsed: 0:01:15. with Loss: 1.6491905450820923 with total_loss: 3065.3327233195305\n",
      "  Batch   542  of  1,613.    Elapsed: 0:01:15. with Loss: 1.4677724838256836 with total_loss: 3066.9819138646126\n",
      "  Batch   543  of  1,613.    Elapsed: 0:01:15. with Loss: 1.5666277408599854 with total_loss: 3068.4496863484383\n",
      "  Batch   544  of  1,613.    Elapsed: 0:01:15. with Loss: 1.191664218902588 with total_loss: 3070.0163140892982\n",
      "  Batch   545  of  1,613.    Elapsed: 0:01:15. with Loss: 1.405216097831726 with total_loss: 3071.207978308201\n",
      "  Batch   546  of  1,613.    Elapsed: 0:01:15. with Loss: 1.1411783695220947 with total_loss: 3072.6131944060326\n",
      "  Batch   547  of  1,613.    Elapsed: 0:01:15. with Loss: 1.8305470943450928 with total_loss: 3073.7543727755547\n",
      "  Batch   548  of  1,613.    Elapsed: 0:01:16. with Loss: 2.507607936859131 with total_loss: 3075.5849198698997\n",
      "  Batch   549  of  1,613.    Elapsed: 0:01:16. with Loss: 4.2681989669799805 with total_loss: 3078.092527806759\n",
      "  Batch   550  of  1,613.    Elapsed: 0:01:16. with Loss: 1.8524144887924194 with total_loss: 3082.360726773739\n",
      "  Batch   551  of  1,613.    Elapsed: 0:01:16. with Loss: 2.6162078380584717 with total_loss: 3084.2131412625313\n",
      "  Batch   552  of  1,613.    Elapsed: 0:01:16. with Loss: 2.8275749683380127 with total_loss: 3086.8293491005898\n",
      "  Batch   553  of  1,613.    Elapsed: 0:01:16. with Loss: 1.404554009437561 with total_loss: 3089.6569240689278\n",
      "  Batch   554  of  1,613.    Elapsed: 0:01:16. with Loss: 1.1453635692596436 with total_loss: 3091.0614780783653\n",
      "  Batch   555  of  1,613.    Elapsed: 0:01:17. with Loss: 1.6098774671554565 with total_loss: 3092.206841647625\n",
      "  Batch   556  of  1,613.    Elapsed: 0:01:17. with Loss: 1.0913190841674805 with total_loss: 3093.8167191147804\n",
      "  Batch   557  of  1,613.    Elapsed: 0:01:17. with Loss: 1.6258031129837036 with total_loss: 3094.908038198948\n",
      "  Batch   558  of  1,613.    Elapsed: 0:01:17. with Loss: 1.4599355459213257 with total_loss: 3096.5338413119316\n",
      "  Batch   559  of  1,613.    Elapsed: 0:01:17. with Loss: 2.8207762241363525 with total_loss: 3097.993776857853\n",
      "  Batch   560  of  1,613.    Elapsed: 0:01:17. with Loss: 0.9849967360496521 with total_loss: 3100.8145530819893\n",
      "  Batch   561  of  1,613.    Elapsed: 0:01:17. with Loss: 1.0901280641555786 with total_loss: 3101.799549818039\n",
      "  Batch   562  of  1,613.    Elapsed: 0:01:18. with Loss: 1.4635487794876099 with total_loss: 3102.8896778821945\n",
      "  Batch   563  of  1,613.    Elapsed: 0:01:18. with Loss: 2.1995584964752197 with total_loss: 3104.353226661682\n",
      "  Batch   564  of  1,613.    Elapsed: 0:01:18. with Loss: 1.7622264623641968 with total_loss: 3106.5527851581573\n",
      "  Batch   565  of  1,613.    Elapsed: 0:01:18. with Loss: 2.692887306213379 with total_loss: 3108.3150116205215\n",
      "  Batch   566  of  1,613.    Elapsed: 0:01:18. with Loss: 2.139110565185547 with total_loss: 3111.007898926735\n",
      "  Batch   567  of  1,613.    Elapsed: 0:01:18. with Loss: 2.2293202877044678 with total_loss: 3113.1470094919205\n",
      "  Batch   568  of  1,613.    Elapsed: 0:01:18. with Loss: 3.743215560913086 with total_loss: 3115.376329779625\n",
      "  Batch   569  of  1,613.    Elapsed: 0:01:18. with Loss: 1.3386179208755493 with total_loss: 3119.119545340538\n",
      "  Batch   570  of  1,613.    Elapsed: 0:01:19. with Loss: 1.1773875951766968 with total_loss: 3120.4581632614136\n",
      "  Batch   571  of  1,613.    Elapsed: 0:01:19. with Loss: 1.145127773284912 with total_loss: 3121.6355508565903\n",
      "  Batch   572  of  1,613.    Elapsed: 0:01:19. with Loss: 2.2650721073150635 with total_loss: 3122.780678629875\n",
      "  Batch   573  of  1,613.    Elapsed: 0:01:19. with Loss: 1.1934340000152588 with total_loss: 3125.0457507371902\n",
      "  Batch   574  of  1,613.    Elapsed: 0:01:19. with Loss: 1.4464311599731445 with total_loss: 3126.2391847372055\n",
      "  Batch   575  of  1,613.    Elapsed: 0:01:19. with Loss: 1.0211786031723022 with total_loss: 3127.6856158971786\n",
      "  Batch   576  of  1,613.    Elapsed: 0:01:19. with Loss: 1.145903468132019 with total_loss: 3128.706794500351\n",
      "  Batch   577  of  1,613.    Elapsed: 0:01:20. with Loss: 1.0626224279403687 with total_loss: 3129.852697968483\n",
      "  Batch   578  of  1,613.    Elapsed: 0:01:20. with Loss: 2.479759454727173 with total_loss: 3130.9153203964233\n",
      "  Batch   579  of  1,613.    Elapsed: 0:01:20. with Loss: 1.0817495584487915 with total_loss: 3133.3950798511505\n",
      "  Batch   580  of  1,613.    Elapsed: 0:01:20. with Loss: 1.1593252420425415 with total_loss: 3134.4768294095993\n",
      "  Batch   581  of  1,613.    Elapsed: 0:01:20. with Loss: 1.2576717138290405 with total_loss: 3135.636154651642\n",
      "  Batch   582  of  1,613.    Elapsed: 0:01:20. with Loss: 1.1673243045806885 with total_loss: 3136.893826365471\n",
      "  Batch   583  of  1,613.    Elapsed: 0:01:20. with Loss: 1.352718472480774 with total_loss: 3138.0611506700516\n",
      "  Batch   584  of  1,613.    Elapsed: 0:01:21. with Loss: 0.9279384613037109 with total_loss: 3139.4138691425323\n",
      "  Batch   585  of  1,613.    Elapsed: 0:01:21. with Loss: 1.0723904371261597 with total_loss: 3140.341807603836\n",
      "  Batch   586  of  1,613.    Elapsed: 0:01:21. with Loss: 1.7989882230758667 with total_loss: 3141.414198040962\n",
      "  Batch   587  of  1,613.    Elapsed: 0:01:21. with Loss: 3.1319704055786133 with total_loss: 3143.213186264038\n",
      "  Batch   588  of  1,613.    Elapsed: 0:01:21. with Loss: 1.3917407989501953 with total_loss: 3146.3451566696167\n",
      "  Batch   589  of  1,613.    Elapsed: 0:01:21. with Loss: 0.9338419437408447 with total_loss: 3147.736897468567\n",
      "  Batch   590  of  1,613.    Elapsed: 0:01:21. with Loss: 1.9369436502456665 with total_loss: 3148.6707394123077\n",
      "  Batch   591  of  1,613.    Elapsed: 0:01:22. with Loss: 1.212680459022522 with total_loss: 3150.6076830625534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   592  of  1,613.    Elapsed: 0:01:22. with Loss: 1.0976231098175049 with total_loss: 3151.820363521576\n",
      "  Batch   593  of  1,613.    Elapsed: 0:01:22. with Loss: 1.1226130723953247 with total_loss: 3152.9179866313934\n",
      "  Batch   594  of  1,613.    Elapsed: 0:01:22. with Loss: 1.4512150287628174 with total_loss: 3154.0405997037888\n",
      "  Batch   595  of  1,613.    Elapsed: 0:01:22. with Loss: 1.121066927909851 with total_loss: 3155.4918147325516\n",
      "  Batch   596  of  1,613.    Elapsed: 0:01:22. with Loss: 1.0325746536254883 with total_loss: 3156.6128816604614\n",
      "  Batch   597  of  1,613.    Elapsed: 0:01:22. with Loss: 1.3365777730941772 with total_loss: 3157.645456314087\n",
      "  Batch   598  of  1,613.    Elapsed: 0:01:22. with Loss: 0.9806553721427917 with total_loss: 3158.982034087181\n",
      "  Batch   599  of  1,613.    Elapsed: 0:01:23. with Loss: 1.7093472480773926 with total_loss: 3159.962689459324\n",
      "  Batch   600  of  1,613.    Elapsed: 0:01:23. with Loss: 1.3012011051177979 with total_loss: 3161.6720367074013\n",
      "  Batch   601  of  1,613.    Elapsed: 0:01:23. with Loss: 1.8322269916534424 with total_loss: 3162.973237812519\n",
      "  Batch   602  of  1,613.    Elapsed: 0:01:23. with Loss: 2.1481077671051025 with total_loss: 3164.8054648041725\n",
      "  Batch   603  of  1,613.    Elapsed: 0:01:23. with Loss: 2.2286298274993896 with total_loss: 3166.9535725712776\n",
      "  Batch   604  of  1,613.    Elapsed: 0:01:23. with Loss: 1.2812350988388062 with total_loss: 3169.182202398777\n",
      "  Batch   605  of  1,613.    Elapsed: 0:01:23. with Loss: 1.04499351978302 with total_loss: 3170.463437497616\n",
      "  Batch   606  of  1,613.    Elapsed: 0:01:24. with Loss: 2.0556180477142334 with total_loss: 3171.508431017399\n",
      "  Batch   607  of  1,613.    Elapsed: 0:01:24. with Loss: 1.5105509757995605 with total_loss: 3173.564049065113\n",
      "  Batch   608  of  1,613.    Elapsed: 0:01:24. with Loss: 0.9958692789077759 with total_loss: 3175.0746000409126\n",
      "  Batch   609  of  1,613.    Elapsed: 0:01:24. with Loss: 1.1243244409561157 with total_loss: 3176.0704693198204\n",
      "  Batch   610  of  1,613.    Elapsed: 0:01:24. with Loss: 1.3631783723831177 with total_loss: 3177.1947937607765\n",
      "  Batch   611  of  1,613.    Elapsed: 0:01:24. with Loss: 1.9783318042755127 with total_loss: 3178.5579721331596\n",
      "  Batch   612  of  1,613.    Elapsed: 0:01:24. with Loss: 1.2502909898757935 with total_loss: 3180.536303937435\n",
      "  Batch   613  of  1,613.    Elapsed: 0:01:25. with Loss: 1.6768683195114136 with total_loss: 3181.786594927311\n",
      "  Batch   614  of  1,613.    Elapsed: 0:01:25. with Loss: 1.5854357481002808 with total_loss: 3183.4634632468224\n",
      "  Batch   615  of  1,613.    Elapsed: 0:01:25. with Loss: 3.000539541244507 with total_loss: 3185.0488989949226\n",
      "  Batch   616  of  1,613.    Elapsed: 0:01:25. with Loss: 1.7183700799942017 with total_loss: 3188.049438536167\n",
      "  Batch   617  of  1,613.    Elapsed: 0:01:25. with Loss: 2.1622633934020996 with total_loss: 3189.7678086161613\n",
      "  Batch   618  of  1,613.    Elapsed: 0:01:25. with Loss: 1.8477240800857544 with total_loss: 3191.9300720095634\n",
      "  Batch   619  of  1,613.    Elapsed: 0:01:25. with Loss: 1.4285528659820557 with total_loss: 3193.777796089649\n",
      "  Batch   620  of  1,613.    Elapsed: 0:01:26. with Loss: 1.4903312921524048 with total_loss: 3195.2063489556313\n",
      "  Batch   621  of  1,613.    Elapsed: 0:01:26. with Loss: 1.329699158668518 with total_loss: 3196.6966802477837\n",
      "  Batch   622  of  1,613.    Elapsed: 0:01:26. with Loss: 0.9531610608100891 with total_loss: 3198.026379406452\n",
      "  Batch   623  of  1,613.    Elapsed: 0:01:26. with Loss: 1.28195321559906 with total_loss: 3198.9795404672623\n",
      "  Batch   624  of  1,613.    Elapsed: 0:01:26. with Loss: 1.206123948097229 with total_loss: 3200.2614936828613\n",
      "  Batch   625  of  1,613.    Elapsed: 0:01:26. with Loss: 1.4300442934036255 with total_loss: 3201.4676176309586\n",
      "  Batch   626  of  1,613.    Elapsed: 0:01:26. with Loss: 1.4284948110580444 with total_loss: 3202.897661924362\n",
      "  Batch   627  of  1,613.    Elapsed: 0:01:26. with Loss: 1.1698580980300903 with total_loss: 3204.32615673542\n",
      "  Batch   628  of  1,613.    Elapsed: 0:01:27. with Loss: 1.3721997737884521 with total_loss: 3205.4960148334503\n",
      "  Batch   629  of  1,613.    Elapsed: 0:01:27. with Loss: 1.0434499979019165 with total_loss: 3206.8682146072388\n",
      "  Batch   630  of  1,613.    Elapsed: 0:01:27. with Loss: 2.0089337825775146 with total_loss: 3207.9116646051407\n",
      "  Batch   631  of  1,613.    Elapsed: 0:01:27. with Loss: 0.9582865834236145 with total_loss: 3209.920598387718\n",
      "  Batch   632  of  1,613.    Elapsed: 0:01:27. with Loss: 1.0158170461654663 with total_loss: 3210.878884971142\n",
      "  Batch   633  of  1,613.    Elapsed: 0:01:27. with Loss: 1.2167645692825317 with total_loss: 3211.8947020173073\n",
      "  Batch   634  of  1,613.    Elapsed: 0:01:27. with Loss: 1.566980004310608 with total_loss: 3213.11146658659\n",
      "  Batch   635  of  1,613.    Elapsed: 0:01:28. with Loss: 1.055234670639038 with total_loss: 3214.6784465909004\n",
      "  Batch   636  of  1,613.    Elapsed: 0:01:28. with Loss: 2.6183619499206543 with total_loss: 3215.7336812615395\n",
      "  Batch   637  of  1,613.    Elapsed: 0:01:28. with Loss: 1.5510070323944092 with total_loss: 3218.35204321146\n",
      "  Batch   638  of  1,613.    Elapsed: 0:01:28. with Loss: 1.651224970817566 with total_loss: 3219.9030502438545\n",
      "  Batch   639  of  1,613.    Elapsed: 0:01:28. with Loss: 1.9361203908920288 with total_loss: 3221.554275214672\n",
      "  Batch   640  of  1,613.    Elapsed: 0:01:28. with Loss: 2.1705024242401123 with total_loss: 3223.490395605564\n",
      "  Batch   641  of  1,613.    Elapsed: 0:01:28. with Loss: 2.1294095516204834 with total_loss: 3225.6608980298042\n",
      "  Batch   642  of  1,613.    Elapsed: 0:01:29. with Loss: 1.7204551696777344 with total_loss: 3227.7903075814247\n",
      "  Batch   643  of  1,613.    Elapsed: 0:01:29. with Loss: 1.6134850978851318 with total_loss: 3229.5107627511024\n",
      "  Batch   644  of  1,613.    Elapsed: 0:01:29. with Loss: 1.3199549913406372 with total_loss: 3231.1242478489876\n",
      "  Batch   645  of  1,613.    Elapsed: 0:01:29. with Loss: 2.0363006591796875 with total_loss: 3232.444202840328\n",
      "  Batch   646  of  1,613.    Elapsed: 0:01:29. with Loss: 1.0972647666931152 with total_loss: 3234.480503499508\n",
      "  Batch   647  of  1,613.    Elapsed: 0:01:29. with Loss: 1.1523277759552002 with total_loss: 3235.577768266201\n",
      "  Batch   648  of  1,613.    Elapsed: 0:01:29. with Loss: 1.0303505659103394 with total_loss: 3236.730096042156\n",
      "  Batch   649  of  1,613.    Elapsed: 0:01:30. with Loss: 1.6756420135498047 with total_loss: 3237.7604466080666\n",
      "  Batch   650  of  1,613.    Elapsed: 0:01:30. with Loss: 0.9752069711685181 with total_loss: 3239.4360886216164\n",
      "  Batch   651  of  1,613.    Elapsed: 0:01:30. with Loss: 0.762806236743927 with total_loss: 3240.411295592785\n",
      "  Batch   652  of  1,613.    Elapsed: 0:01:30. with Loss: 1.7648206949234009 with total_loss: 3241.174101829529\n",
      "  Batch   653  of  1,613.    Elapsed: 0:01:30. with Loss: 1.832226276397705 with total_loss: 3242.938922524452\n",
      "  Batch   654  of  1,613.    Elapsed: 0:01:30. with Loss: 1.442795991897583 with total_loss: 3244.77114880085\n",
      "  Batch   655  of  1,613.    Elapsed: 0:01:30. with Loss: 1.113981008529663 with total_loss: 3246.2139447927475\n",
      "  Batch   656  of  1,613.    Elapsed: 0:01:30. with Loss: 1.15492582321167 with total_loss: 3247.327925801277\n",
      "  Batch   657  of  1,613.    Elapsed: 0:01:31. with Loss: 1.8828903436660767 with total_loss: 3248.482851624489\n",
      "  Batch   658  of  1,613.    Elapsed: 0:01:31. with Loss: 1.7308539152145386 with total_loss: 3250.365741968155\n",
      "  Batch   659  of  1,613.    Elapsed: 0:01:31. with Loss: 1.5100735425949097 with total_loss: 3252.0965958833694\n",
      "  Batch   660  of  1,613.    Elapsed: 0:01:31. with Loss: 0.8795180916786194 with total_loss: 3253.6066694259644\n",
      "  Batch   661  of  1,613.    Elapsed: 0:01:31. with Loss: 1.4736659526824951 with total_loss: 3254.486187517643\n",
      "  Batch   662  of  1,613.    Elapsed: 0:01:31. with Loss: 1.2746167182922363 with total_loss: 3255.9598534703255\n",
      "  Batch   663  of  1,613.    Elapsed: 0:01:31. with Loss: 2.716278553009033 with total_loss: 3257.2344701886177\n",
      "  Batch   664  of  1,613.    Elapsed: 0:01:32. with Loss: 1.1620866060256958 with total_loss: 3259.9507487416267\n",
      "  Batch   665  of  1,613.    Elapsed: 0:01:32. with Loss: 1.4159777164459229 with total_loss: 3261.1128353476524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   666  of  1,613.    Elapsed: 0:01:32. with Loss: 2.5540473461151123 with total_loss: 3262.5288130640984\n",
      "  Batch   667  of  1,613.    Elapsed: 0:01:32. with Loss: 1.4114724397659302 with total_loss: 3265.0828604102135\n",
      "  Batch   668  of  1,613.    Elapsed: 0:01:32. with Loss: 0.9013723731040955 with total_loss: 3266.4943328499794\n",
      "  Batch   669  of  1,613.    Elapsed: 0:01:32. with Loss: 2.555370807647705 with total_loss: 3267.3957052230835\n",
      "  Batch   670  of  1,613.    Elapsed: 0:01:32. with Loss: 2.326601028442383 with total_loss: 3269.951076030731\n",
      "  Batch   671  of  1,613.    Elapsed: 0:01:33. with Loss: 1.0926578044891357 with total_loss: 3272.2776770591736\n",
      "  Batch   672  of  1,613.    Elapsed: 0:01:33. with Loss: 2.141857624053955 with total_loss: 3273.3703348636627\n",
      "  Batch   673  of  1,613.    Elapsed: 0:01:33. with Loss: 1.429417610168457 with total_loss: 3275.5121924877167\n",
      "  Batch   674  of  1,613.    Elapsed: 0:01:33. with Loss: 1.2634648084640503 with total_loss: 3276.941610097885\n",
      "  Batch   675  of  1,613.    Elapsed: 0:01:33. with Loss: 1.4780551195144653 with total_loss: 3278.205074906349\n",
      "  Batch   676  of  1,613.    Elapsed: 0:01:33. with Loss: 2.031395196914673 with total_loss: 3279.6831300258636\n",
      "  Batch   677  of  1,613.    Elapsed: 0:01:33. with Loss: 1.1663126945495605 with total_loss: 3281.7145252227783\n",
      "  Batch   678  of  1,613.    Elapsed: 0:01:34. with Loss: 1.864589810371399 with total_loss: 3282.880837917328\n",
      "  Batch   679  of  1,613.    Elapsed: 0:01:34. with Loss: 0.8867769837379456 with total_loss: 3284.7454277276993\n",
      "  Batch   680  of  1,613.    Elapsed: 0:01:34. with Loss: 1.2328243255615234 with total_loss: 3285.632204711437\n",
      "  Batch   681  of  1,613.    Elapsed: 0:01:34. with Loss: 1.6291579008102417 with total_loss: 3286.8650290369987\n",
      "  Batch   682  of  1,613.    Elapsed: 0:01:34. with Loss: 1.1514793634414673 with total_loss: 3288.494186937809\n",
      "  Batch   683  of  1,613.    Elapsed: 0:01:34. with Loss: 1.7614306211471558 with total_loss: 3289.6456663012505\n",
      "  Batch   684  of  1,613.    Elapsed: 0:01:34. with Loss: 2.2251970767974854 with total_loss: 3291.4070969223976\n",
      "  Batch   685  of  1,613.    Elapsed: 0:01:34. with Loss: 0.8116388916969299 with total_loss: 3293.632293999195\n",
      "  Batch   686  of  1,613.    Elapsed: 0:01:35. with Loss: 0.9941262602806091 with total_loss: 3294.443932890892\n",
      "  Batch   687  of  1,613.    Elapsed: 0:01:35. with Loss: 2.0919430255889893 with total_loss: 3295.4380591511726\n",
      "  Batch   688  of  1,613.    Elapsed: 0:01:35. with Loss: 1.7790921926498413 with total_loss: 3297.5300021767616\n",
      "  Batch   689  of  1,613.    Elapsed: 0:01:35. with Loss: 1.7025249004364014 with total_loss: 3299.3090943694115\n",
      "  Batch   690  of  1,613.    Elapsed: 0:01:35. with Loss: 1.5947177410125732 with total_loss: 3301.011619269848\n",
      "  Batch   691  of  1,613.    Elapsed: 0:01:35. with Loss: 3.3432793617248535 with total_loss: 3302.6063370108604\n",
      "  Batch   692  of  1,613.    Elapsed: 0:01:35. with Loss: 3.752551317214966 with total_loss: 3305.9496163725853\n",
      "  Batch   693  of  1,613.    Elapsed: 0:01:36. with Loss: 1.8929060697555542 with total_loss: 3309.7021676898003\n",
      "  Batch   694  of  1,613.    Elapsed: 0:01:36. with Loss: 1.1112008094787598 with total_loss: 3311.595073759556\n",
      "  Batch   695  of  1,613.    Elapsed: 0:01:36. with Loss: 1.3693512678146362 with total_loss: 3312.7062745690346\n",
      "  Batch   696  of  1,613.    Elapsed: 0:01:36. with Loss: 2.587395191192627 with total_loss: 3314.075625836849\n",
      "  Batch   697  of  1,613.    Elapsed: 0:01:36. with Loss: 11.845438957214355 with total_loss: 3316.663021028042\n",
      "  Batch   698  of  1,613.    Elapsed: 0:01:36. with Loss: 0.8674332499504089 with total_loss: 3328.508459985256\n",
      "  Batch   699  of  1,613.    Elapsed: 0:01:36. with Loss: 1.608882188796997 with total_loss: 3329.3758932352066\n",
      "  Batch   700  of  1,613.    Elapsed: 0:01:37. with Loss: 1.498852252960205 with total_loss: 3330.9847754240036\n",
      "  Batch   701  of  1,613.    Elapsed: 0:01:37. with Loss: 1.4597697257995605 with total_loss: 3332.483627676964\n",
      "  Batch   702  of  1,613.    Elapsed: 0:01:37. with Loss: 0.923550546169281 with total_loss: 3333.9433974027634\n",
      "  Batch   703  of  1,613.    Elapsed: 0:01:37. with Loss: 1.1904644966125488 with total_loss: 3334.8669479489326\n",
      "  Batch   704  of  1,613.    Elapsed: 0:01:37. with Loss: 1.7567682266235352 with total_loss: 3336.057412445545\n",
      "  Batch   705  of  1,613.    Elapsed: 0:01:37. with Loss: 1.036372184753418 with total_loss: 3337.8141806721687\n",
      "  Batch   706  of  1,613.    Elapsed: 0:01:37. with Loss: 1.6459157466888428 with total_loss: 3338.850552856922\n",
      "  Batch   707  of  1,613.    Elapsed: 0:01:38. with Loss: 1.4013466835021973 with total_loss: 3340.496468603611\n",
      "  Batch   708  of  1,613.    Elapsed: 0:01:38. with Loss: 1.5342533588409424 with total_loss: 3341.897815287113\n",
      "  Batch   709  of  1,613.    Elapsed: 0:01:38. with Loss: 1.2439348697662354 with total_loss: 3343.432068645954\n",
      "  Batch   710  of  1,613.    Elapsed: 0:01:38. with Loss: 1.3856688737869263 with total_loss: 3344.6760035157204\n",
      "  Batch   711  of  1,613.    Elapsed: 0:01:38. with Loss: 0.9603144526481628 with total_loss: 3346.0616723895073\n",
      "  Batch   712  of  1,613.    Elapsed: 0:01:38. with Loss: 1.4436744451522827 with total_loss: 3347.0219868421555\n",
      "  Batch   713  of  1,613.    Elapsed: 0:01:38. with Loss: 0.9990457892417908 with total_loss: 3348.4656612873077\n",
      "  Batch   714  of  1,613.    Elapsed: 0:01:38. with Loss: 1.683642864227295 with total_loss: 3349.4647070765495\n",
      "  Batch   715  of  1,613.    Elapsed: 0:01:39. with Loss: 2.0414845943450928 with total_loss: 3351.148349940777\n",
      "  Batch   716  of  1,613.    Elapsed: 0:01:39. with Loss: 4.324771881103516 with total_loss: 3353.189834535122\n",
      "  Batch   717  of  1,613.    Elapsed: 0:01:39. with Loss: 1.0551728010177612 with total_loss: 3357.5146064162254\n",
      "  Batch   718  of  1,613.    Elapsed: 0:01:39. with Loss: 1.3941211700439453 with total_loss: 3358.569779217243\n",
      "  Batch   719  of  1,613.    Elapsed: 0:01:39. with Loss: 0.8996137380599976 with total_loss: 3359.963900387287\n",
      "  Batch   720  of  1,613.    Elapsed: 0:01:39. with Loss: 1.5819706916809082 with total_loss: 3360.863514125347\n",
      "  Batch   721  of  1,613.    Elapsed: 0:01:39. with Loss: 1.2842369079589844 with total_loss: 3362.445484817028\n",
      "  Batch   722  of  1,613.    Elapsed: 0:01:40. with Loss: 1.6296547651290894 with total_loss: 3363.729721724987\n",
      "  Batch   723  of  1,613.    Elapsed: 0:01:40. with Loss: 1.2094364166259766 with total_loss: 3365.359376490116\n",
      "  Batch   724  of  1,613.    Elapsed: 0:01:40. with Loss: 1.7109414339065552 with total_loss: 3366.568812906742\n",
      "  Batch   725  of  1,613.    Elapsed: 0:01:40. with Loss: 1.4583054780960083 with total_loss: 3368.2797543406487\n",
      "  Batch   726  of  1,613.    Elapsed: 0:01:40. with Loss: 1.4070796966552734 with total_loss: 3369.7380598187447\n",
      "  Batch   727  of  1,613.    Elapsed: 0:01:40. with Loss: 1.9664777517318726 with total_loss: 3371.1451395154\n",
      "  Batch   728  of  1,613.    Elapsed: 0:01:40. with Loss: 1.8317681550979614 with total_loss: 3373.111617267132\n",
      "  Batch   729  of  1,613.    Elapsed: 0:01:41. with Loss: 1.0385947227478027 with total_loss: 3374.9433854222298\n",
      "  Batch   730  of  1,613.    Elapsed: 0:01:41. with Loss: 2.2571260929107666 with total_loss: 3375.9819801449776\n",
      "  Batch   731  of  1,613.    Elapsed: 0:01:41. with Loss: 1.9215245246887207 with total_loss: 3378.2391062378883\n",
      "  Batch   732  of  1,613.    Elapsed: 0:01:41. with Loss: 1.1557559967041016 with total_loss: 3380.160630762577\n",
      "  Batch   733  of  1,613.    Elapsed: 0:01:41. with Loss: 1.6462424993515015 with total_loss: 3381.316386759281\n",
      "  Batch   734  of  1,613.    Elapsed: 0:01:41. with Loss: 1.7531349658966064 with total_loss: 3382.9626292586327\n",
      "  Batch   735  of  1,613.    Elapsed: 0:01:41. with Loss: 1.882783055305481 with total_loss: 3384.7157642245293\n",
      "  Batch   736  of  1,613.    Elapsed: 0:01:42. with Loss: 1.4180152416229248 with total_loss: 3386.5985472798347\n",
      "  Batch   737  of  1,613.    Elapsed: 0:01:42. with Loss: 1.7472285032272339 with total_loss: 3388.0165625214577\n",
      "  Batch   738  of  1,613.    Elapsed: 0:01:42. with Loss: 1.1079298257827759 with total_loss: 3389.763791024685\n",
      "  Batch   739  of  1,613.    Elapsed: 0:01:42. with Loss: 1.1716115474700928 with total_loss: 3390.8717208504677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   740  of  1,613.    Elapsed: 0:01:42. with Loss: 1.2690458297729492 with total_loss: 3392.043332397938\n",
      "  Batch   741  of  1,613.    Elapsed: 0:01:42. with Loss: 6.3249945640563965 with total_loss: 3393.3123782277107\n",
      "  Batch   742  of  1,613.    Elapsed: 0:01:42. with Loss: 0.9261196851730347 with total_loss: 3399.637372791767\n",
      "  Batch   743  of  1,613.    Elapsed: 0:01:42. with Loss: 1.2785199880599976 with total_loss: 3400.56349247694\n",
      "  Batch   744  of  1,613.    Elapsed: 0:01:43. with Loss: 0.9895740747451782 with total_loss: 3401.842012465\n",
      "  Batch   745  of  1,613.    Elapsed: 0:01:43. with Loss: 2.2287943363189697 with total_loss: 3402.8315865397453\n",
      "  Batch   746  of  1,613.    Elapsed: 0:01:43. with Loss: 0.9433870911598206 with total_loss: 3405.0603808760643\n",
      "  Batch   747  of  1,613.    Elapsed: 0:01:43. with Loss: 1.7889652252197266 with total_loss: 3406.003767967224\n",
      "  Batch   748  of  1,613.    Elapsed: 0:01:43. with Loss: 0.6087000966072083 with total_loss: 3407.792733192444\n",
      "  Batch   749  of  1,613.    Elapsed: 0:01:43. with Loss: 1.582425594329834 with total_loss: 3408.401433289051\n",
      "  Batch   750  of  1,613.    Elapsed: 0:01:43. with Loss: 1.0203638076782227 with total_loss: 3409.983858883381\n",
      "  Batch   751  of  1,613.    Elapsed: 0:01:44. with Loss: 0.8118686676025391 with total_loss: 3411.004222691059\n",
      "  Batch   752  of  1,613.    Elapsed: 0:01:44. with Loss: 1.962403655052185 with total_loss: 3411.8160913586617\n",
      "  Batch   753  of  1,613.    Elapsed: 0:01:44. with Loss: 1.9564659595489502 with total_loss: 3413.778495013714\n",
      "  Batch   754  of  1,613.    Elapsed: 0:01:44. with Loss: 2.4938652515411377 with total_loss: 3415.734960973263\n",
      "  Batch   755  of  1,613.    Elapsed: 0:01:44. with Loss: 1.6868822574615479 with total_loss: 3418.228826224804\n",
      "  Batch   756  of  1,613.    Elapsed: 0:01:44. with Loss: 1.8913602828979492 with total_loss: 3419.9157084822655\n",
      "  Batch   757  of  1,613.    Elapsed: 0:01:44. with Loss: 0.7681503295898438 with total_loss: 3421.8070687651634\n",
      "  Batch   758  of  1,613.    Elapsed: 0:01:45. with Loss: 1.5619138479232788 with total_loss: 3422.5752190947533\n",
      "  Batch   759  of  1,613.    Elapsed: 0:01:45. with Loss: 1.9530962705612183 with total_loss: 3424.1371329426765\n",
      "  Batch   760  of  1,613.    Elapsed: 0:01:45. with Loss: 1.155134916305542 with total_loss: 3426.0902292132378\n",
      "  Batch   761  of  1,613.    Elapsed: 0:01:45. with Loss: 1.1677414178848267 with total_loss: 3427.2453641295433\n",
      "  Batch   762  of  1,613.    Elapsed: 0:01:45. with Loss: 1.245349645614624 with total_loss: 3428.413105547428\n",
      "  Batch   763  of  1,613.    Elapsed: 0:01:45. with Loss: 1.8527106046676636 with total_loss: 3429.6584551930428\n",
      "  Batch   764  of  1,613.    Elapsed: 0:01:45. with Loss: 2.1219630241394043 with total_loss: 3431.5111657977104\n",
      "  Batch   765  of  1,613.    Elapsed: 0:01:46. with Loss: 1.9175121784210205 with total_loss: 3433.63312882185\n",
      "  Batch   766  of  1,613.    Elapsed: 0:01:46. with Loss: 2.073718786239624 with total_loss: 3435.550641000271\n",
      "  Batch   767  of  1,613.    Elapsed: 0:01:46. with Loss: 1.0773569345474243 with total_loss: 3437.6243597865105\n",
      "  Batch   768  of  1,613.    Elapsed: 0:01:46. with Loss: 1.0097243785858154 with total_loss: 3438.701716721058\n",
      "  Batch   769  of  1,613.    Elapsed: 0:01:46. with Loss: 1.372745394706726 with total_loss: 3439.7114410996437\n",
      "  Batch   770  of  1,613.    Elapsed: 0:01:46. with Loss: 1.9474490880966187 with total_loss: 3441.0841864943504\n",
      "  Batch   771  of  1,613.    Elapsed: 0:01:46. with Loss: 1.170599341392517 with total_loss: 3443.031635582447\n",
      "  Batch   772  of  1,613.    Elapsed: 0:01:46. with Loss: 1.7341989278793335 with total_loss: 3444.2022349238396\n",
      "  Batch   773  of  1,613.    Elapsed: 0:01:47. with Loss: 1.5519194602966309 with total_loss: 3445.936433851719\n",
      "  Batch   774  of  1,613.    Elapsed: 0:01:47. with Loss: 0.9418210983276367 with total_loss: 3447.4883533120155\n",
      "  Batch   775  of  1,613.    Elapsed: 0:01:47. with Loss: 1.4126960039138794 with total_loss: 3448.430174410343\n",
      "  Batch   776  of  1,613.    Elapsed: 0:01:47. with Loss: 1.0801992416381836 with total_loss: 3449.842870414257\n",
      "  Batch   777  of  1,613.    Elapsed: 0:01:47. with Loss: 1.47818922996521 with total_loss: 3450.9230696558952\n",
      "  Batch   778  of  1,613.    Elapsed: 0:01:47. with Loss: 1.309110164642334 with total_loss: 3452.4012588858604\n",
      "  Batch   779  of  1,613.    Elapsed: 0:01:47. with Loss: 1.1255944967269897 with total_loss: 3453.710369050503\n",
      "  Batch   780  of  1,613.    Elapsed: 0:01:48. with Loss: 1.2545920610427856 with total_loss: 3454.8359635472298\n",
      "  Batch   781  of  1,613.    Elapsed: 0:01:48. with Loss: 1.1223722696304321 with total_loss: 3456.0905556082726\n",
      "  Batch   782  of  1,613.    Elapsed: 0:01:48. with Loss: 1.9174010753631592 with total_loss: 3457.212927877903\n",
      "  Batch   783  of  1,613.    Elapsed: 0:01:48. with Loss: 1.2871570587158203 with total_loss: 3459.130328953266\n",
      "  Batch   784  of  1,613.    Elapsed: 0:01:48. with Loss: 1.1242780685424805 with total_loss: 3460.417486011982\n",
      "  Batch   785  of  1,613.    Elapsed: 0:01:48. with Loss: 2.181135416030884 with total_loss: 3461.5417640805244\n",
      "  Batch   786  of  1,613.    Elapsed: 0:01:48. with Loss: 2.2568790912628174 with total_loss: 3463.7228994965553\n",
      "  Batch   787  of  1,613.    Elapsed: 0:01:49. with Loss: 1.7729687690734863 with total_loss: 3465.979778587818\n",
      "  Batch   788  of  1,613.    Elapsed: 0:01:49. with Loss: 1.8947077989578247 with total_loss: 3467.7527473568916\n",
      "  Batch   789  of  1,613.    Elapsed: 0:01:49. with Loss: 1.6914087533950806 with total_loss: 3469.6474551558495\n",
      "  Batch   790  of  1,613.    Elapsed: 0:01:49. with Loss: 1.4613043069839478 with total_loss: 3471.3388639092445\n",
      "  Batch   791  of  1,613.    Elapsed: 0:01:49. with Loss: 1.4796143770217896 with total_loss: 3472.8001682162285\n",
      "  Batch   792  of  1,613.    Elapsed: 0:01:49. with Loss: 2.7394585609436035 with total_loss: 3474.2797825932503\n",
      "  Batch   793  of  1,613.    Elapsed: 0:01:49. with Loss: 1.7386287450790405 with total_loss: 3477.019241154194\n",
      "  Batch   794  of  1,613.    Elapsed: 0:01:50. with Loss: 1.7637220621109009 with total_loss: 3478.757869899273\n",
      "  Batch   795  of  1,613.    Elapsed: 0:01:50. with Loss: 1.0938247442245483 with total_loss: 3480.521591961384\n",
      "  Batch   796  of  1,613.    Elapsed: 0:01:50. with Loss: 1.853556513786316 with total_loss: 3481.6154167056084\n",
      "  Batch   797  of  1,613.    Elapsed: 0:01:50. with Loss: 1.0577634572982788 with total_loss: 3483.4689732193947\n",
      "  Batch   798  of  1,613.    Elapsed: 0:01:50. with Loss: 1.0450096130371094 with total_loss: 3484.526736676693\n",
      "  Batch   799  of  1,613.    Elapsed: 0:01:50. with Loss: 1.8814365863800049 with total_loss: 3485.57174628973\n",
      "  Batch   800  of  1,613.    Elapsed: 0:01:50. with Loss: 1.6275981664657593 with total_loss: 3487.45318287611\n",
      "  Batch   801  of  1,613.    Elapsed: 0:01:51. with Loss: 1.3368401527404785 with total_loss: 3489.080781042576\n",
      "  Batch   802  of  1,613.    Elapsed: 0:01:51. with Loss: 1.7258673906326294 with total_loss: 3490.4176211953163\n",
      "  Batch   803  of  1,613.    Elapsed: 0:01:51. with Loss: 0.9471635222434998 with total_loss: 3492.143488585949\n",
      "  Batch   804  of  1,613.    Elapsed: 0:01:51. with Loss: 1.5754650831222534 with total_loss: 3493.0906521081924\n",
      "  Batch   805  of  1,613.    Elapsed: 0:01:51. with Loss: 1.042831540107727 with total_loss: 3494.6661171913147\n",
      "  Batch   806  of  1,613.    Elapsed: 0:01:51. with Loss: 1.0633134841918945 with total_loss: 3495.7089487314224\n",
      "  Batch   807  of  1,613.    Elapsed: 0:01:51. with Loss: 1.2959778308868408 with total_loss: 3496.7722622156143\n",
      "  Batch   808  of  1,613.    Elapsed: 0:01:51. with Loss: 0.872969925403595 with total_loss: 3498.068240046501\n",
      "  Batch   809  of  1,613.    Elapsed: 0:01:52. with Loss: 1.3484108448028564 with total_loss: 3498.9412099719048\n",
      "  Batch   810  of  1,613.    Elapsed: 0:01:52. with Loss: 1.1232004165649414 with total_loss: 3500.2896208167076\n",
      "  Batch   811  of  1,613.    Elapsed: 0:01:52. with Loss: 1.9957610368728638 with total_loss: 3501.4128212332726\n",
      "  Batch   812  of  1,613.    Elapsed: 0:01:52. with Loss: 1.3186806440353394 with total_loss: 3503.4085822701454\n",
      "  Batch   813  of  1,613.    Elapsed: 0:01:52. with Loss: 1.6448910236358643 with total_loss: 3504.7272629141808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   814  of  1,613.    Elapsed: 0:01:52. with Loss: 1.7278634309768677 with total_loss: 3506.3721539378166\n",
      "  Batch   815  of  1,613.    Elapsed: 0:01:52. with Loss: 1.1075522899627686 with total_loss: 3508.1000173687935\n",
      "  Batch   816  of  1,613.    Elapsed: 0:01:53. with Loss: 1.0941060781478882 with total_loss: 3509.2075696587563\n",
      "  Batch   817  of  1,613.    Elapsed: 0:01:53. with Loss: 1.060887336730957 with total_loss: 3510.301675736904\n",
      "  Batch   818  of  1,613.    Elapsed: 0:01:53. with Loss: 1.4683340787887573 with total_loss: 3511.362563073635\n",
      "  Batch   819  of  1,613.    Elapsed: 0:01:53. with Loss: 1.0377765893936157 with total_loss: 3512.830897152424\n",
      "  Batch   820  of  1,613.    Elapsed: 0:01:53. with Loss: 1.10287344455719 with total_loss: 3513.8686737418175\n",
      "  Batch   821  of  1,613.    Elapsed: 0:01:53. with Loss: 1.6529805660247803 with total_loss: 3514.9715471863747\n",
      "  Batch   822  of  1,613.    Elapsed: 0:01:53. with Loss: 3.5719008445739746 with total_loss: 3516.6245277523994\n",
      "  Batch   823  of  1,613.    Elapsed: 0:01:54. with Loss: 1.5735363960266113 with total_loss: 3520.1964285969734\n",
      "  Batch   824  of  1,613.    Elapsed: 0:01:54. with Loss: 1.6864252090454102 with total_loss: 3521.769964993\n",
      "  Batch   825  of  1,613.    Elapsed: 0:01:54. with Loss: 1.5009052753448486 with total_loss: 3523.4563902020454\n",
      "  Batch   826  of  1,613.    Elapsed: 0:01:54. with Loss: 2.193357229232788 with total_loss: 3524.9572954773903\n",
      "  Batch   827  of  1,613.    Elapsed: 0:01:54. with Loss: 1.7785975933074951 with total_loss: 3527.150652706623\n",
      "  Batch   828  of  1,613.    Elapsed: 0:01:54. with Loss: 1.532124638557434 with total_loss: 3528.9292502999306\n",
      "  Batch   829  of  1,613.    Elapsed: 0:01:54. with Loss: 1.6220084428787231 with total_loss: 3530.461374938488\n",
      "  Batch   830  of  1,613.    Elapsed: 0:01:55. with Loss: 1.44413161277771 with total_loss: 3532.0833833813667\n",
      "  Batch   831  of  1,613.    Elapsed: 0:01:55. with Loss: 2.002751350402832 with total_loss: 3533.5275149941444\n",
      "  Batch   832  of  1,613.    Elapsed: 0:01:55. with Loss: 1.6250790357589722 with total_loss: 3535.5302663445473\n",
      "  Batch   833  of  1,613.    Elapsed: 0:01:55. with Loss: 0.8800927996635437 with total_loss: 3537.1553453803062\n",
      "  Batch   834  of  1,613.    Elapsed: 0:01:55. with Loss: 1.7450414896011353 with total_loss: 3538.03543817997\n",
      "  Batch   835  of  1,613.    Elapsed: 0:01:55. with Loss: 0.8684626817703247 with total_loss: 3539.780479669571\n",
      "  Batch   836  of  1,613.    Elapsed: 0:01:55. with Loss: 1.7234916687011719 with total_loss: 3540.6489423513412\n",
      "  Batch   837  of  1,613.    Elapsed: 0:01:55. with Loss: 1.5788687467575073 with total_loss: 3542.3724340200424\n",
      "  Batch   838  of  1,613.    Elapsed: 0:01:56. with Loss: 1.452059268951416 with total_loss: 3543.9513027668\n",
      "  Batch   839  of  1,613.    Elapsed: 0:01:56. with Loss: 0.9609112739562988 with total_loss: 3545.4033620357513\n",
      "  Batch   840  of  1,613.    Elapsed: 0:01:56. with Loss: 0.9781167507171631 with total_loss: 3546.3642733097076\n",
      "  Batch   841  of  1,613.    Elapsed: 0:01:56. with Loss: 0.8346381783485413 with total_loss: 3547.342390060425\n",
      "  Batch   842  of  1,613.    Elapsed: 0:01:56. with Loss: 2.0278148651123047 with total_loss: 3548.1770282387733\n",
      "  Batch   843  of  1,613.    Elapsed: 0:01:56. with Loss: 2.0083553791046143 with total_loss: 3550.2048431038857\n",
      "  Batch   844  of  1,613.    Elapsed: 0:01:56. with Loss: 1.308530569076538 with total_loss: 3552.2131984829903\n",
      "  Batch   845  of  1,613.    Elapsed: 0:01:57. with Loss: 1.2642549276351929 with total_loss: 3553.521729052067\n",
      "  Batch   846  of  1,613.    Elapsed: 0:01:57. with Loss: 1.3569425344467163 with total_loss: 3554.785983979702\n",
      "  Batch   847  of  1,613.    Elapsed: 0:01:57. with Loss: 1.5356569290161133 with total_loss: 3556.1429265141487\n",
      "  Batch   848  of  1,613.    Elapsed: 0:01:57. with Loss: 1.066900610923767 with total_loss: 3557.678583443165\n",
      "  Batch   849  of  1,613.    Elapsed: 0:01:57. with Loss: 0.6134909391403198 with total_loss: 3558.7454840540886\n",
      "  Batch   850  of  1,613.    Elapsed: 0:01:57. with Loss: 1.3444010019302368 with total_loss: 3559.358974993229\n",
      "  Batch   851  of  1,613.    Elapsed: 0:01:57. with Loss: 1.993950366973877 with total_loss: 3560.703375995159\n",
      "  Batch   852  of  1,613.    Elapsed: 0:01:58. with Loss: 1.1743279695510864 with total_loss: 3562.697326362133\n",
      "  Batch   853  of  1,613.    Elapsed: 0:01:58. with Loss: 1.544934630393982 with total_loss: 3563.871654331684\n",
      "  Batch   854  of  1,613.    Elapsed: 0:01:58. with Loss: 2.2007896900177 with total_loss: 3565.416588962078\n",
      "  Batch   855  of  1,613.    Elapsed: 0:01:58. with Loss: 1.1256725788116455 with total_loss: 3567.617378652096\n",
      "  Batch   856  of  1,613.    Elapsed: 0:01:58. with Loss: 1.3857712745666504 with total_loss: 3568.7430512309074\n",
      "  Batch   857  of  1,613.    Elapsed: 0:01:58. with Loss: 0.9513981938362122 with total_loss: 3570.128822505474\n",
      "  Batch   858  of  1,613.    Elapsed: 0:01:58. with Loss: 3.3051650524139404 with total_loss: 3571.0802206993103\n",
      "  Batch   859  of  1,613.    Elapsed: 0:01:59. with Loss: 2.578554630279541 with total_loss: 3574.3853857517242\n",
      "  Batch   860  of  1,613.    Elapsed: 0:01:59. with Loss: 1.6824127435684204 with total_loss: 3576.963940382004\n",
      "  Batch   861  of  1,613.    Elapsed: 0:01:59. with Loss: 2.04343843460083 with total_loss: 3578.646353125572\n",
      "  Batch   862  of  1,613.    Elapsed: 0:01:59. with Loss: 1.7336961030960083 with total_loss: 3580.689791560173\n",
      "  Batch   863  of  1,613.    Elapsed: 0:01:59. with Loss: 2.0191714763641357 with total_loss: 3582.423487663269\n",
      "  Batch   864  of  1,613.    Elapsed: 0:01:59. with Loss: 2.265399932861328 with total_loss: 3584.442659139633\n",
      "  Batch   865  of  1,613.    Elapsed: 0:01:59. with Loss: 0.9392276406288147 with total_loss: 3586.7080590724945\n",
      "  Batch   866  of  1,613.    Elapsed: 0:01:59. with Loss: 1.3975375890731812 with total_loss: 3587.6472867131233\n",
      "  Batch   867  of  1,613.    Elapsed: 0:02:00. with Loss: 1.3474736213684082 with total_loss: 3589.0448243021965\n",
      "  Batch   868  of  1,613.    Elapsed: 0:02:00. with Loss: 2.4261512756347656 with total_loss: 3590.392297923565\n",
      "  Batch   869  of  1,613.    Elapsed: 0:02:00. with Loss: 1.1651887893676758 with total_loss: 3592.8184491991997\n",
      "  Batch   870  of  1,613.    Elapsed: 0:02:00. with Loss: 1.3058078289031982 with total_loss: 3593.9836379885674\n",
      "  Batch   871  of  1,613.    Elapsed: 0:02:00. with Loss: 1.333101749420166 with total_loss: 3595.2894458174706\n",
      "  Batch   872  of  1,613.    Elapsed: 0:02:00. with Loss: 1.174739122390747 with total_loss: 3596.6225475668907\n",
      "  Batch   873  of  1,613.    Elapsed: 0:02:00. with Loss: 1.3273528814315796 with total_loss: 3597.7972866892815\n",
      "  Batch   874  of  1,613.    Elapsed: 0:02:01. with Loss: 0.9227089881896973 with total_loss: 3599.124639570713\n",
      "  Batch   875  of  1,613.    Elapsed: 0:02:01. with Loss: 0.8332240581512451 with total_loss: 3600.0473485589027\n",
      "  Batch   876  of  1,613.    Elapsed: 0:02:01. with Loss: 1.4366724491119385 with total_loss: 3600.880572617054\n",
      "  Batch   877  of  1,613.    Elapsed: 0:02:01. with Loss: 0.837221086025238 with total_loss: 3602.317245066166\n",
      "  Batch   878  of  1,613.    Elapsed: 0:02:01. with Loss: 0.8487687110900879 with total_loss: 3603.154466152191\n",
      "  Batch   879  of  1,613.    Elapsed: 0:02:01. with Loss: 1.070121169090271 with total_loss: 3604.0032348632812\n",
      "  Batch   880  of  1,613.    Elapsed: 0:02:01. with Loss: 3.150357961654663 with total_loss: 3605.0733560323715\n",
      "  Batch   881  of  1,613.    Elapsed: 0:02:02. with Loss: 1.1514978408813477 with total_loss: 3608.223713994026\n",
      "  Batch   882  of  1,613.    Elapsed: 0:02:02. with Loss: 1.8366605043411255 with total_loss: 3609.3752118349075\n",
      "  Batch   883  of  1,613.    Elapsed: 0:02:02. with Loss: 1.0028185844421387 with total_loss: 3611.2118723392487\n",
      "  Batch   884  of  1,613.    Elapsed: 0:02:02. with Loss: 3.096897602081299 with total_loss: 3612.214690923691\n",
      "  Batch   885  of  1,613.    Elapsed: 0:02:02. with Loss: 0.8494821786880493 with total_loss: 3615.311588525772\n",
      "  Batch   886  of  1,613.    Elapsed: 0:02:02. with Loss: 1.325753092765808 with total_loss: 3616.16107070446\n",
      "  Batch   887  of  1,613.    Elapsed: 0:02:02. with Loss: 1.4488918781280518 with total_loss: 3617.486823797226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   888  of  1,613.    Elapsed: 0:02:03. with Loss: 0.6371427178382874 with total_loss: 3618.935715675354\n",
      "  Batch   889  of  1,613.    Elapsed: 0:02:03. with Loss: 0.945197582244873 with total_loss: 3619.5728583931923\n",
      "  Batch   890  of  1,613.    Elapsed: 0:02:03. with Loss: 0.8791372179985046 with total_loss: 3620.518055975437\n",
      "  Batch   891  of  1,613.    Elapsed: 0:02:03. with Loss: 2.7537078857421875 with total_loss: 3621.3971931934357\n",
      "  Batch   892  of  1,613.    Elapsed: 0:02:03. with Loss: 1.7398746013641357 with total_loss: 3624.150901079178\n",
      "  Batch   893  of  1,613.    Elapsed: 0:02:03. with Loss: 2.2927119731903076 with total_loss: 3625.890775680542\n",
      "  Batch   894  of  1,613.    Elapsed: 0:02:03. with Loss: 1.5904585123062134 with total_loss: 3628.1834876537323\n",
      "  Batch   895  of  1,613.    Elapsed: 0:02:03. with Loss: 1.817881464958191 with total_loss: 3629.7739461660385\n",
      "  Batch   896  of  1,613.    Elapsed: 0:02:04. with Loss: 0.9069035053253174 with total_loss: 3631.5918276309967\n",
      "  Batch   897  of  1,613.    Elapsed: 0:02:04. with Loss: 1.8079299926757812 with total_loss: 3632.498731136322\n",
      "  Batch   898  of  1,613.    Elapsed: 0:02:04. with Loss: 1.332962989807129 with total_loss: 3634.306661128998\n",
      "  Batch   899  of  1,613.    Elapsed: 0:02:04. with Loss: 2.4214730262756348 with total_loss: 3635.639624118805\n",
      "  Batch   900  of  1,613.    Elapsed: 0:02:04. with Loss: 0.8554502725601196 with total_loss: 3638.0610971450806\n",
      "  Batch   901  of  1,613.    Elapsed: 0:02:04. with Loss: 1.308741807937622 with total_loss: 3638.9165474176407\n",
      "  Batch   902  of  1,613.    Elapsed: 0:02:04. with Loss: 2.7268240451812744 with total_loss: 3640.2252892255783\n",
      "  Batch   903  of  1,613.    Elapsed: 0:02:05. with Loss: 2.9495644569396973 with total_loss: 3642.9521132707596\n",
      "  Batch   904  of  1,613.    Elapsed: 0:02:05. with Loss: 1.4062072038650513 with total_loss: 3645.9016777276993\n",
      "  Batch   905  of  1,613.    Elapsed: 0:02:05. with Loss: 1.3271321058273315 with total_loss: 3647.3078849315643\n",
      "  Batch   906  of  1,613.    Elapsed: 0:02:05. with Loss: 1.2157961130142212 with total_loss: 3648.6350170373917\n",
      "  Batch   907  of  1,613.    Elapsed: 0:02:05. with Loss: 1.3274314403533936 with total_loss: 3649.850813150406\n",
      "  Batch   908  of  1,613.    Elapsed: 0:02:05. with Loss: 1.5723176002502441 with total_loss: 3651.1782445907593\n",
      "  Batch   909  of  1,613.    Elapsed: 0:02:05. with Loss: 1.3460675477981567 with total_loss: 3652.7505621910095\n",
      "  Batch   910  of  1,613.    Elapsed: 0:02:06. with Loss: 1.1481993198394775 with total_loss: 3654.0966297388077\n",
      "  Batch   911  of  1,613.    Elapsed: 0:02:06. with Loss: 0.7780361771583557 with total_loss: 3655.244829058647\n",
      "  Batch   912  of  1,613.    Elapsed: 0:02:06. with Loss: 1.053787112236023 with total_loss: 3656.0228652358055\n",
      "  Batch   913  of  1,613.    Elapsed: 0:02:06. with Loss: 1.7671210765838623 with total_loss: 3657.0766523480415\n",
      "  Batch   914  of  1,613.    Elapsed: 0:02:06. with Loss: 1.7049369812011719 with total_loss: 3658.8437734246254\n",
      "  Batch   915  of  1,613.    Elapsed: 0:02:06. with Loss: 1.74032461643219 with total_loss: 3660.5487104058266\n",
      "  Batch   916  of  1,613.    Elapsed: 0:02:06. with Loss: 0.8220144510269165 with total_loss: 3662.2890350222588\n",
      "  Batch   917  of  1,613.    Elapsed: 0:02:07. with Loss: 1.4915460348129272 with total_loss: 3663.1110494732857\n",
      "  Batch   918  of  1,613.    Elapsed: 0:02:07. with Loss: 1.2392687797546387 with total_loss: 3664.6025955080986\n",
      "  Batch   919  of  1,613.    Elapsed: 0:02:07. with Loss: 0.8602824807167053 with total_loss: 3665.8418642878532\n",
      "  Batch   920  of  1,613.    Elapsed: 0:02:07. with Loss: 2.103461742401123 with total_loss: 3666.70214676857\n",
      "  Batch   921  of  1,613.    Elapsed: 0:02:07. with Loss: 1.273472547531128 with total_loss: 3668.805608510971\n",
      "  Batch   922  of  1,613.    Elapsed: 0:02:07. with Loss: 1.1578495502471924 with total_loss: 3670.079081058502\n",
      "  Batch   923  of  1,613.    Elapsed: 0:02:07. with Loss: 1.7651090621948242 with total_loss: 3671.2369306087494\n",
      "  Batch   924  of  1,613.    Elapsed: 0:02:07. with Loss: 1.0873528718948364 with total_loss: 3673.002039670944\n",
      "  Batch   925  of  1,613.    Elapsed: 0:02:08. with Loss: 1.5970699787139893 with total_loss: 3674.089392542839\n",
      "  Batch   926  of  1,613.    Elapsed: 0:02:08. with Loss: 1.1787025928497314 with total_loss: 3675.686462521553\n",
      "  Batch   927  of  1,613.    Elapsed: 0:02:08. with Loss: 0.9669461846351624 with total_loss: 3676.8651651144028\n",
      "  Batch   928  of  1,613.    Elapsed: 0:02:08. with Loss: 1.147344946861267 with total_loss: 3677.832111299038\n",
      "  Batch   929  of  1,613.    Elapsed: 0:02:08. with Loss: 2.1139774322509766 with total_loss: 3678.979456245899\n",
      "  Batch   930  of  1,613.    Elapsed: 0:02:08. with Loss: 0.9761903285980225 with total_loss: 3681.09343367815\n",
      "  Batch   931  of  1,613.    Elapsed: 0:02:08. with Loss: 1.693408489227295 with total_loss: 3682.069624006748\n",
      "  Batch   932  of  1,613.    Elapsed: 0:02:09. with Loss: 1.051494836807251 with total_loss: 3683.7630324959755\n",
      "  Batch   933  of  1,613.    Elapsed: 0:02:09. with Loss: 0.6235173940658569 with total_loss: 3684.8145273327827\n",
      "  Batch   934  of  1,613.    Elapsed: 0:02:09. with Loss: 1.1609585285186768 with total_loss: 3685.4380447268486\n",
      "  Batch   935  of  1,613.    Elapsed: 0:02:09. with Loss: 1.3108508586883545 with total_loss: 3686.5990032553673\n",
      "  Batch   936  of  1,613.    Elapsed: 0:02:09. with Loss: 1.0582903623580933 with total_loss: 3687.9098541140556\n",
      "  Batch   937  of  1,613.    Elapsed: 0:02:09. with Loss: 1.1580954790115356 with total_loss: 3688.9681444764137\n",
      "  Batch   938  of  1,613.    Elapsed: 0:02:09. with Loss: 2.602954626083374 with total_loss: 3690.1262399554253\n",
      "  Batch   939  of  1,613.    Elapsed: 0:02:10. with Loss: 1.3917806148529053 with total_loss: 3692.7291945815086\n",
      "  Batch   940  of  1,613.    Elapsed: 0:02:10. with Loss: 1.0035223960876465 with total_loss: 3694.1209751963615\n",
      "  Batch   941  of  1,613.    Elapsed: 0:02:10. with Loss: 2.317286491394043 with total_loss: 3695.124497592449\n",
      "  Batch   942  of  1,613.    Elapsed: 0:02:10. with Loss: 3.6266114711761475 with total_loss: 3697.4417840838432\n",
      "  Batch   943  of  1,613.    Elapsed: 0:02:10. with Loss: 1.6876623630523682 with total_loss: 3701.0683955550194\n",
      "  Batch   944  of  1,613.    Elapsed: 0:02:10. with Loss: 1.0883283615112305 with total_loss: 3702.7560579180717\n",
      "  Batch   945  of  1,613.    Elapsed: 0:02:10. with Loss: 0.8708491325378418 with total_loss: 3703.844386279583\n",
      "  Batch   946  of  1,613.    Elapsed: 0:02:11. with Loss: 1.2046278715133667 with total_loss: 3704.715235412121\n",
      "  Batch   947  of  1,613.    Elapsed: 0:02:11. with Loss: 1.2823512554168701 with total_loss: 3705.919863283634\n",
      "  Batch   948  of  1,613.    Elapsed: 0:02:11. with Loss: 1.3895041942596436 with total_loss: 3707.202214539051\n",
      "  Batch   949  of  1,613.    Elapsed: 0:02:11. with Loss: 2.2183728218078613 with total_loss: 3708.5917187333107\n",
      "  Batch   950  of  1,613.    Elapsed: 0:02:11. with Loss: 2.1304686069488525 with total_loss: 3710.8100915551186\n",
      "  Batch   951  of  1,613.    Elapsed: 0:02:11. with Loss: 1.089385747909546 with total_loss: 3712.9405601620674\n",
      "  Batch   952  of  1,613.    Elapsed: 0:02:11. with Loss: 0.9717747569084167 with total_loss: 3714.029945909977\n",
      "  Batch   953  of  1,613.    Elapsed: 0:02:11. with Loss: 1.2695212364196777 with total_loss: 3715.0017206668854\n",
      "  Batch   954  of  1,613.    Elapsed: 0:02:12. with Loss: 0.8676086664199829 with total_loss: 3716.271241903305\n",
      "  Batch   955  of  1,613.    Elapsed: 0:02:12. with Loss: 1.5194618701934814 with total_loss: 3717.138850569725\n",
      "  Batch   956  of  1,613.    Elapsed: 0:02:12. with Loss: 1.8160755634307861 with total_loss: 3718.6583124399185\n",
      "  Batch   957  of  1,613.    Elapsed: 0:02:12. with Loss: 2.8052382469177246 with total_loss: 3720.4743880033493\n",
      "  Batch   958  of  1,613.    Elapsed: 0:02:12. with Loss: 2.0123424530029297 with total_loss: 3723.279626250267\n",
      "  Batch   959  of  1,613.    Elapsed: 0:02:12. with Loss: 1.4333196878433228 with total_loss: 3725.29196870327\n",
      "  Batch   960  of  1,613.    Elapsed: 0:02:12. with Loss: 1.2098270654678345 with total_loss: 3726.7252883911133\n",
      "  Batch   961  of  1,613.    Elapsed: 0:02:13. with Loss: 2.307962656021118 with total_loss: 3727.935115456581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   962  of  1,613.    Elapsed: 0:02:13. with Loss: 8.455485343933105 with total_loss: 3730.2430781126022\n",
      "  Batch   963  of  1,613.    Elapsed: 0:02:13. with Loss: 1.378860354423523 with total_loss: 3738.6985634565353\n",
      "  Batch   964  of  1,613.    Elapsed: 0:02:13. with Loss: 0.9600383043289185 with total_loss: 3740.077423810959\n",
      "  Batch   965  of  1,613.    Elapsed: 0:02:13. with Loss: 1.3954112529754639 with total_loss: 3741.037462115288\n",
      "  Batch   966  of  1,613.    Elapsed: 0:02:13. with Loss: 1.1537374258041382 with total_loss: 3742.4328733682632\n",
      "  Batch   967  of  1,613.    Elapsed: 0:02:13. with Loss: 1.213782548904419 with total_loss: 3743.5866107940674\n",
      "  Batch   968  of  1,613.    Elapsed: 0:02:14. with Loss: 0.8584368824958801 with total_loss: 3744.800393342972\n",
      "  Batch   969  of  1,613.    Elapsed: 0:02:14. with Loss: 1.285569429397583 with total_loss: 3745.6588302254677\n",
      "  Batch   970  of  1,613.    Elapsed: 0:02:14. with Loss: 1.215874195098877 with total_loss: 3746.9443996548653\n",
      "  Batch   971  of  1,613.    Elapsed: 0:02:14. with Loss: 1.435241460800171 with total_loss: 3748.160273849964\n",
      "  Batch   972  of  1,613.    Elapsed: 0:02:14. with Loss: 1.880355954170227 with total_loss: 3749.5955153107643\n",
      "  Batch   973  of  1,613.    Elapsed: 0:02:14. with Loss: 1.2263308763504028 with total_loss: 3751.4758712649345\n",
      "  Batch   974  of  1,613.    Elapsed: 0:02:14. with Loss: 1.5555154085159302 with total_loss: 3752.702202141285\n",
      "  Batch   975  of  1,613.    Elapsed: 0:02:15. with Loss: 2.2430732250213623 with total_loss: 3754.257717549801\n",
      "  Batch   976  of  1,613.    Elapsed: 0:02:15. with Loss: 1.5235334634780884 with total_loss: 3756.5007907748222\n",
      "  Batch   977  of  1,613.    Elapsed: 0:02:15. with Loss: 1.4321315288543701 with total_loss: 3758.0243242383003\n",
      "  Batch   978  of  1,613.    Elapsed: 0:02:15. with Loss: 1.4734238386154175 with total_loss: 3759.4564557671547\n",
      "  Batch   979  of  1,613.    Elapsed: 0:02:15. with Loss: 6.873166084289551 with total_loss: 3760.92987960577\n",
      "  Batch   980  of  1,613.    Elapsed: 0:02:15. with Loss: 1.543041467666626 with total_loss: 3767.8030456900597\n",
      "  Batch   981  of  1,613.    Elapsed: 0:02:15. with Loss: 0.8721277117729187 with total_loss: 3769.3460871577263\n",
      "  Batch   982  of  1,613.    Elapsed: 0:02:16. with Loss: 1.7231488227844238 with total_loss: 3770.218214869499\n",
      "  Batch   983  of  1,613.    Elapsed: 0:02:16. with Loss: 1.983620285987854 with total_loss: 3771.9413636922836\n",
      "  Batch   984  of  1,613.    Elapsed: 0:02:16. with Loss: 1.6182798147201538 with total_loss: 3773.9249839782715\n",
      "  Batch   985  of  1,613.    Elapsed: 0:02:16. with Loss: 1.0289342403411865 with total_loss: 3775.5432637929916\n",
      "  Batch   986  of  1,613.    Elapsed: 0:02:16. with Loss: 0.647768497467041 with total_loss: 3776.572198033333\n",
      "  Batch   987  of  1,613.    Elapsed: 0:02:16. with Loss: 1.843858242034912 with total_loss: 3777.2199665308\n",
      "  Batch   988  of  1,613.    Elapsed: 0:02:16. with Loss: 1.9192278385162354 with total_loss: 3779.063824772835\n",
      "  Batch   989  of  1,613.    Elapsed: 0:02:16. with Loss: 1.4557336568832397 with total_loss: 3780.983052611351\n",
      "  Batch   990  of  1,613.    Elapsed: 0:02:17. with Loss: 2.1765620708465576 with total_loss: 3782.4387862682343\n",
      "  Batch   991  of  1,613.    Elapsed: 0:02:17. with Loss: 2.1503841876983643 with total_loss: 3784.615348339081\n",
      "  Batch   992  of  1,613.    Elapsed: 0:02:17. with Loss: 1.4438589811325073 with total_loss: 3786.765732526779\n",
      "  Batch   993  of  1,613.    Elapsed: 0:02:17. with Loss: 1.2728606462478638 with total_loss: 3788.2095915079117\n",
      "  Batch   994  of  1,613.    Elapsed: 0:02:17. with Loss: 1.2589471340179443 with total_loss: 3789.4824521541595\n",
      "  Batch   995  of  1,613.    Elapsed: 0:02:17. with Loss: 3.0126843452453613 with total_loss: 3790.7413992881775\n",
      "  Batch   996  of  1,613.    Elapsed: 0:02:17. with Loss: 0.7990262508392334 with total_loss: 3793.754083633423\n",
      "  Batch   997  of  1,613.    Elapsed: 0:02:18. with Loss: 7.075289726257324 with total_loss: 3794.553109884262\n",
      "  Batch   998  of  1,613.    Elapsed: 0:02:18. with Loss: 1.191212773323059 with total_loss: 3801.6283996105194\n",
      "  Batch   999  of  1,613.    Elapsed: 0:02:18. with Loss: 3.5777151584625244 with total_loss: 3802.8196123838425\n",
      "  Batch 1,000  of  1,613.    Elapsed: 0:02:18. with Loss: 1.3227407932281494 with total_loss: 3806.397327542305\n",
      "  Batch 1,001  of  1,613.    Elapsed: 0:02:18. with Loss: 3.2510626316070557 with total_loss: 3807.720068335533\n",
      "  Batch 1,002  of  1,613.    Elapsed: 0:02:18. with Loss: 1.4826489686965942 with total_loss: 3810.97113096714\n",
      "  Batch 1,003  of  1,613.    Elapsed: 0:02:18. with Loss: 2.0445480346679688 with total_loss: 3812.453779935837\n",
      "  Batch 1,004  of  1,613.    Elapsed: 0:02:19. with Loss: 1.802600622177124 with total_loss: 3814.4983279705048\n",
      "  Batch 1,005  of  1,613.    Elapsed: 0:02:19. with Loss: 1.6950348615646362 with total_loss: 3816.300928592682\n",
      "  Batch 1,006  of  1,613.    Elapsed: 0:02:19. with Loss: 1.6381807327270508 with total_loss: 3817.9959634542465\n",
      "  Batch 1,007  of  1,613.    Elapsed: 0:02:19. with Loss: 2.330477714538574 with total_loss: 3819.6341441869736\n",
      "  Batch 1,008  of  1,613.    Elapsed: 0:02:19. with Loss: 1.7398278713226318 with total_loss: 3821.964621901512\n",
      "  Batch 1,009  of  1,613.    Elapsed: 0:02:19. with Loss: 1.7241182327270508 with total_loss: 3823.704449772835\n",
      "  Batch 1,010  of  1,613.    Elapsed: 0:02:19. with Loss: 1.2630727291107178 with total_loss: 3825.428568005562\n",
      "  Batch 1,011  of  1,613.    Elapsed: 0:02:20. with Loss: 1.7111681699752808 with total_loss: 3826.6916407346725\n",
      "  Batch 1,012  of  1,613.    Elapsed: 0:02:20. with Loss: 1.2616572380065918 with total_loss: 3828.402808904648\n",
      "  Batch 1,013  of  1,613.    Elapsed: 0:02:20. with Loss: 1.1009403467178345 with total_loss: 3829.6644661426544\n",
      "  Batch 1,014  of  1,613.    Elapsed: 0:02:20. with Loss: 1.1089320182800293 with total_loss: 3830.7654064893723\n",
      "  Batch 1,015  of  1,613.    Elapsed: 0:02:20. with Loss: 1.2614179849624634 with total_loss: 3831.8743385076523\n",
      "  Batch 1,016  of  1,613.    Elapsed: 0:02:20. with Loss: 1.003593921661377 with total_loss: 3833.1357564926147\n",
      "  Batch 1,017  of  1,613.    Elapsed: 0:02:20. with Loss: 2.1001133918762207 with total_loss: 3834.139350414276\n",
      "  Batch 1,018  of  1,613.    Elapsed: 0:02:20. with Loss: 1.3268975019454956 with total_loss: 3836.2394638061523\n",
      "  Batch 1,019  of  1,613.    Elapsed: 0:02:21. with Loss: 7.741372108459473 with total_loss: 3837.566361308098\n",
      "  Batch 1,020  of  1,613.    Elapsed: 0:02:21. with Loss: 1.1668509244918823 with total_loss: 3845.3077334165573\n",
      "  Batch 1,021  of  1,613.    Elapsed: 0:02:21. with Loss: 1.5752559900283813 with total_loss: 3846.474584341049\n",
      "  Batch 1,022  of  1,613.    Elapsed: 0:02:21. with Loss: 0.9997448921203613 with total_loss: 3848.0498403310776\n",
      "  Batch 1,023  of  1,613.    Elapsed: 0:02:21. with Loss: 1.2625830173492432 with total_loss: 3849.049585223198\n",
      "  Batch 1,024  of  1,613.    Elapsed: 0:02:21. with Loss: 0.995996356010437 with total_loss: 3850.312168240547\n",
      "  Batch 1,025  of  1,613.    Elapsed: 0:02:21. with Loss: 2.0445141792297363 with total_loss: 3851.3081645965576\n",
      "  Batch 1,026  of  1,613.    Elapsed: 0:02:22. with Loss: 1.3465834856033325 with total_loss: 3853.3526787757874\n",
      "  Batch 1,027  of  1,613.    Elapsed: 0:02:22. with Loss: 1.5972106456756592 with total_loss: 3854.6992622613907\n",
      "  Batch 1,028  of  1,613.    Elapsed: 0:02:22. with Loss: 1.6722646951675415 with total_loss: 3856.2964729070663\n",
      "  Batch 1,029  of  1,613.    Elapsed: 0:02:22. with Loss: 1.7179723978042603 with total_loss: 3857.968737602234\n",
      "  Batch 1,030  of  1,613.    Elapsed: 0:02:22. with Loss: 0.7481183409690857 with total_loss: 3859.686710000038\n",
      "  Batch 1,031  of  1,613.    Elapsed: 0:02:22. with Loss: 1.7566516399383545 with total_loss: 3860.4348283410072\n",
      "  Batch 1,032  of  1,613.    Elapsed: 0:02:22. with Loss: 1.4211270809173584 with total_loss: 3862.1914799809456\n",
      "  Batch 1,033  of  1,613.    Elapsed: 0:02:23. with Loss: 1.1595600843429565 with total_loss: 3863.612607061863\n",
      "  Batch 1,034  of  1,613.    Elapsed: 0:02:23. with Loss: 1.461774230003357 with total_loss: 3864.772167146206\n",
      "  Batch 1,035  of  1,613.    Elapsed: 0:02:23. with Loss: 0.9835678935050964 with total_loss: 3866.2339413762093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,036  of  1,613.    Elapsed: 0:02:23. with Loss: 1.130162000656128 with total_loss: 3867.2175092697144\n",
      "  Batch 1,037  of  1,613.    Elapsed: 0:02:23. with Loss: 1.412318229675293 with total_loss: 3868.3476712703705\n",
      "  Batch 1,038  of  1,613.    Elapsed: 0:02:23. with Loss: 2.272186517715454 with total_loss: 3869.759989500046\n",
      "  Batch 1,039  of  1,613.    Elapsed: 0:02:23. with Loss: 2.983778715133667 with total_loss: 3872.0321760177612\n",
      "  Batch 1,040  of  1,613.    Elapsed: 0:02:24. with Loss: 5.557584285736084 with total_loss: 3875.015954732895\n",
      "  Batch 1,041  of  1,613.    Elapsed: 0:02:24. with Loss: 1.21973717212677 with total_loss: 3880.573539018631\n",
      "  Batch 1,042  of  1,613.    Elapsed: 0:02:24. with Loss: 1.3870748281478882 with total_loss: 3881.7932761907578\n",
      "  Batch 1,043  of  1,613.    Elapsed: 0:02:24. with Loss: 1.5862812995910645 with total_loss: 3883.1803510189056\n",
      "  Batch 1,044  of  1,613.    Elapsed: 0:02:24. with Loss: 1.5516471862792969 with total_loss: 3884.7666323184967\n",
      "  Batch 1,045  of  1,613.    Elapsed: 0:02:24. with Loss: 0.8048104047775269 with total_loss: 3886.318279504776\n",
      "  Batch 1,046  of  1,613.    Elapsed: 0:02:24. with Loss: 0.9752791523933411 with total_loss: 3887.1230899095535\n",
      "  Batch 1,047  of  1,613.    Elapsed: 0:02:24. with Loss: 1.480474829673767 with total_loss: 3888.098369061947\n",
      "  Batch 1,048  of  1,613.    Elapsed: 0:02:25. with Loss: 1.6430127620697021 with total_loss: 3889.5788438916206\n",
      "  Batch 1,049  of  1,613.    Elapsed: 0:02:25. with Loss: 1.957006812095642 with total_loss: 3891.2218566536903\n",
      "  Batch 1,050  of  1,613.    Elapsed: 0:02:25. with Loss: 2.125457525253296 with total_loss: 3893.178863465786\n",
      "  Batch 1,051  of  1,613.    Elapsed: 0:02:25. with Loss: 1.2529569864273071 with total_loss: 3895.3043209910393\n",
      "  Batch 1,052  of  1,613.    Elapsed: 0:02:25. with Loss: 1.466423749923706 with total_loss: 3896.5572779774666\n",
      "  Batch 1,053  of  1,613.    Elapsed: 0:02:25. with Loss: 1.5192339420318604 with total_loss: 3898.0237017273903\n",
      "  Batch 1,054  of  1,613.    Elapsed: 0:02:25. with Loss: 2.890993356704712 with total_loss: 3899.542935669422\n",
      "  Batch 1,055  of  1,613.    Elapsed: 0:02:26. with Loss: 1.406821608543396 with total_loss: 3902.433929026127\n",
      "  Batch 1,056  of  1,613.    Elapsed: 0:02:26. with Loss: 2.2842538356781006 with total_loss: 3903.8407506346703\n",
      "  Batch 1,057  of  1,613.    Elapsed: 0:02:26. with Loss: 1.2978776693344116 with total_loss: 3906.1250044703484\n",
      "  Batch 1,058  of  1,613.    Elapsed: 0:02:26. with Loss: 2.239781141281128 with total_loss: 3907.4228821396828\n",
      "  Batch 1,059  of  1,613.    Elapsed: 0:02:26. with Loss: 1.2738925218582153 with total_loss: 3909.662663280964\n",
      "  Batch 1,060  of  1,613.    Elapsed: 0:02:26. with Loss: 1.7186704874038696 with total_loss: 3910.936555802822\n",
      "  Batch 1,061  of  1,613.    Elapsed: 0:02:26. with Loss: 1.2284045219421387 with total_loss: 3912.655226290226\n",
      "  Batch 1,062  of  1,613.    Elapsed: 0:02:27. with Loss: 1.091498851776123 with total_loss: 3913.883630812168\n",
      "  Batch 1,063  of  1,613.    Elapsed: 0:02:27. with Loss: 3.541889190673828 with total_loss: 3914.9751296639442\n",
      "  Batch 1,064  of  1,613.    Elapsed: 0:02:27. with Loss: 0.9148775339126587 with total_loss: 3918.517018854618\n",
      "  Batch 1,065  of  1,613.    Elapsed: 0:02:27. with Loss: 1.0976611375808716 with total_loss: 3919.4318963885307\n",
      "  Batch 1,066  of  1,613.    Elapsed: 0:02:27. with Loss: 2.7170844078063965 with total_loss: 3920.5295575261116\n",
      "  Batch 1,067  of  1,613.    Elapsed: 0:02:27. with Loss: 1.019057035446167 with total_loss: 3923.246641933918\n",
      "  Batch 1,068  of  1,613.    Elapsed: 0:02:27. with Loss: 1.277113676071167 with total_loss: 3924.265698969364\n",
      "  Batch 1,069  of  1,613.    Elapsed: 0:02:28. with Loss: 1.5675665140151978 with total_loss: 3925.5428126454353\n",
      "  Batch 1,070  of  1,613.    Elapsed: 0:02:28. with Loss: 0.8758238554000854 with total_loss: 3927.1103791594505\n",
      "  Batch 1,071  of  1,613.    Elapsed: 0:02:28. with Loss: 1.284908652305603 with total_loss: 3927.9862030148506\n",
      "  Batch 1,072  of  1,613.    Elapsed: 0:02:28. with Loss: 1.0905591249465942 with total_loss: 3929.271111667156\n",
      "  Batch 1,073  of  1,613.    Elapsed: 0:02:28. with Loss: 3.0298221111297607 with total_loss: 3930.361670792103\n",
      "  Batch 1,074  of  1,613.    Elapsed: 0:02:28. with Loss: 1.8654043674468994 with total_loss: 3933.3914929032326\n",
      "  Batch 1,075  of  1,613.    Elapsed: 0:02:28. with Loss: 1.050402283668518 with total_loss: 3935.2568972706795\n",
      "  Batch 1,076  of  1,613.    Elapsed: 0:02:28. with Loss: 1.5788800716400146 with total_loss: 3936.307299554348\n",
      "  Batch 1,077  of  1,613.    Elapsed: 0:02:29. with Loss: 0.8441402912139893 with total_loss: 3937.886179625988\n",
      "  Batch 1,078  of  1,613.    Elapsed: 0:02:29. with Loss: 1.5107321739196777 with total_loss: 3938.730319917202\n",
      "  Batch 1,079  of  1,613.    Elapsed: 0:02:29. with Loss: 3.488994598388672 with total_loss: 3940.2410520911217\n",
      "  Batch 1,080  of  1,613.    Elapsed: 0:02:29. with Loss: 1.855908751487732 with total_loss: 3943.7300466895103\n",
      "  Batch 1,081  of  1,613.    Elapsed: 0:02:29. with Loss: 1.2238295078277588 with total_loss: 3945.585955440998\n",
      "  Batch 1,082  of  1,613.    Elapsed: 0:02:29. with Loss: 1.460172414779663 with total_loss: 3946.809784948826\n",
      "  Batch 1,083  of  1,613.    Elapsed: 0:02:29. with Loss: 1.2000049352645874 with total_loss: 3948.2699573636055\n",
      "  Batch 1,084  of  1,613.    Elapsed: 0:02:30. with Loss: 1.068406105041504 with total_loss: 3949.46996229887\n",
      "  Batch 1,085  of  1,613.    Elapsed: 0:02:30. with Loss: 1.1232877969741821 with total_loss: 3950.5383684039116\n",
      "  Batch 1,086  of  1,613.    Elapsed: 0:02:30. with Loss: 0.7311558723449707 with total_loss: 3951.661656200886\n",
      "  Batch 1,087  of  1,613.    Elapsed: 0:02:30. with Loss: 2.5768096446990967 with total_loss: 3952.3928120732307\n",
      "  Batch 1,088  of  1,613.    Elapsed: 0:02:30. with Loss: 3.731327772140503 with total_loss: 3954.96962171793\n",
      "  Batch 1,089  of  1,613.    Elapsed: 0:02:30. with Loss: 2.8015494346618652 with total_loss: 3958.7009494900703\n",
      "  Batch 1,090  of  1,613.    Elapsed: 0:02:30. with Loss: 1.8567787408828735 with total_loss: 3961.502498924732\n",
      "  Batch 1,091  of  1,613.    Elapsed: 0:02:31. with Loss: 1.3590539693832397 with total_loss: 3963.359277665615\n",
      "  Batch 1,092  of  1,613.    Elapsed: 0:02:31. with Loss: 1.1552373170852661 with total_loss: 3964.7183316349983\n",
      "  Batch 1,093  of  1,613.    Elapsed: 0:02:31. with Loss: 0.9721012115478516 with total_loss: 3965.8735689520836\n",
      "  Batch 1,094  of  1,613.    Elapsed: 0:02:31. with Loss: 1.671484112739563 with total_loss: 3966.8456701636314\n",
      "  Batch 1,095  of  1,613.    Elapsed: 0:02:31. with Loss: 4.055896282196045 with total_loss: 3968.517154276371\n",
      "  Batch 1,096  of  1,613.    Elapsed: 0:02:31. with Loss: 1.7487435340881348 with total_loss: 3972.573050558567\n",
      "  Batch 1,097  of  1,613.    Elapsed: 0:02:31. with Loss: 1.4208852052688599 with total_loss: 3974.321794092655\n",
      "  Batch 1,098  of  1,613.    Elapsed: 0:02:32. with Loss: 41.055320739746094 with total_loss: 3975.742679297924\n",
      "  Batch 1,099  of  1,613.    Elapsed: 0:02:32. with Loss: 1.2288565635681152 with total_loss: 4016.79800003767\n",
      "  Batch 1,100  of  1,613.    Elapsed: 0:02:32. with Loss: 1.4100605249404907 with total_loss: 4018.0268566012383\n",
      "  Batch 1,101  of  1,613.    Elapsed: 0:02:32. with Loss: 1.0427201986312866 with total_loss: 4019.4369171261787\n",
      "  Batch 1,102  of  1,613.    Elapsed: 0:02:32. with Loss: 0.9724112153053284 with total_loss: 4020.47963732481\n",
      "  Batch 1,103  of  1,613.    Elapsed: 0:02:32. with Loss: 1.9593759775161743 with total_loss: 4021.4520485401154\n",
      "  Batch 1,104  of  1,613.    Elapsed: 0:02:32. with Loss: 1.554255723953247 with total_loss: 4023.4114245176315\n",
      "  Batch 1,105  of  1,613.    Elapsed: 0:02:33. with Loss: 1.5464469194412231 with total_loss: 4024.965680241585\n",
      "  Batch 1,106  of  1,613.    Elapsed: 0:02:33. with Loss: 1.6253403425216675 with total_loss: 4026.512127161026\n",
      "  Batch 1,107  of  1,613.    Elapsed: 0:02:33. with Loss: 1.2679989337921143 with total_loss: 4028.1374675035477\n",
      "  Batch 1,108  of  1,613.    Elapsed: 0:02:33. with Loss: 3.7055463790893555 with total_loss: 4029.40546643734\n",
      "  Batch 1,109  of  1,613.    Elapsed: 0:02:33. with Loss: 2.729238748550415 with total_loss: 4033.111012816429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,110  of  1,613.    Elapsed: 0:02:33. with Loss: 1.9775887727737427 with total_loss: 4035.8402515649796\n",
      "  Batch 1,111  of  1,613.    Elapsed: 0:02:33. with Loss: 1.2683742046356201 with total_loss: 4037.8178403377533\n",
      "  Batch 1,112  of  1,613.    Elapsed: 0:02:33. with Loss: 1.5954842567443848 with total_loss: 4039.086214542389\n",
      "  Batch 1,113  of  1,613.    Elapsed: 0:02:34. with Loss: 1.0059229135513306 with total_loss: 4040.6816987991333\n",
      "  Batch 1,114  of  1,613.    Elapsed: 0:02:34. with Loss: 1.2662538290023804 with total_loss: 4041.6876217126846\n",
      "  Batch 1,115  of  1,613.    Elapsed: 0:02:34. with Loss: 1.7873305082321167 with total_loss: 4042.953875541687\n",
      "  Batch 1,116  of  1,613.    Elapsed: 0:02:34. with Loss: 1.4840325117111206 with total_loss: 4044.741206049919\n",
      "  Batch 1,117  of  1,613.    Elapsed: 0:02:34. with Loss: 0.9569239020347595 with total_loss: 4046.2252385616302\n",
      "  Batch 1,118  of  1,613.    Elapsed: 0:02:34. with Loss: 1.2719532251358032 with total_loss: 4047.182162463665\n",
      "  Batch 1,119  of  1,613.    Elapsed: 0:02:34. with Loss: 1.5137296915054321 with total_loss: 4048.454115688801\n",
      "  Batch 1,120  of  1,613.    Elapsed: 0:02:35. with Loss: 1.2289035320281982 with total_loss: 4049.9678453803062\n",
      "  Batch 1,121  of  1,613.    Elapsed: 0:02:35. with Loss: 1.8998316526412964 with total_loss: 4051.1967489123344\n",
      "  Batch 1,122  of  1,613.    Elapsed: 0:02:35. with Loss: 1.4348278045654297 with total_loss: 4053.0965805649757\n",
      "  Batch 1,123  of  1,613.    Elapsed: 0:02:35. with Loss: 1.0600100755691528 with total_loss: 4054.531408369541\n",
      "  Batch 1,124  of  1,613.    Elapsed: 0:02:35. with Loss: 1.8085131645202637 with total_loss: 4055.5914184451103\n",
      "  Batch 1,125  of  1,613.    Elapsed: 0:02:35. with Loss: 1.60287344455719 with total_loss: 4057.3999316096306\n",
      "  Batch 1,126  of  1,613.    Elapsed: 0:02:35. with Loss: 1.4651446342468262 with total_loss: 4059.002805054188\n",
      "  Batch 1,127  of  1,613.    Elapsed: 0:02:36. with Loss: 1.32442307472229 with total_loss: 4060.4679496884346\n",
      "  Batch 1,128  of  1,613.    Elapsed: 0:02:36. with Loss: 1.2135884761810303 with total_loss: 4061.792372763157\n",
      "  Batch 1,129  of  1,613.    Elapsed: 0:02:36. with Loss: 4.937187671661377 with total_loss: 4063.005961239338\n",
      "  Batch 1,130  of  1,613.    Elapsed: 0:02:36. with Loss: 1.078704833984375 with total_loss: 4067.9431489109993\n",
      "  Batch 1,131  of  1,613.    Elapsed: 0:02:36. with Loss: 1.1586501598358154 with total_loss: 4069.0218537449837\n",
      "  Batch 1,132  of  1,613.    Elapsed: 0:02:36. with Loss: 1.1080842018127441 with total_loss: 4070.1805039048195\n",
      "  Batch 1,133  of  1,613.    Elapsed: 0:02:36. with Loss: 2.309096574783325 with total_loss: 4071.2885881066322\n",
      "  Batch 1,134  of  1,613.    Elapsed: 0:02:37. with Loss: 1.0965449810028076 with total_loss: 4073.5976846814156\n",
      "  Batch 1,135  of  1,613.    Elapsed: 0:02:37. with Loss: 1.3434243202209473 with total_loss: 4074.6942296624184\n",
      "  Batch 1,136  of  1,613.    Elapsed: 0:02:37. with Loss: 1.245742917060852 with total_loss: 4076.0376539826393\n",
      "  Batch 1,137  of  1,613.    Elapsed: 0:02:37. with Loss: 1.9174572229385376 with total_loss: 4077.2833968997\n",
      "  Batch 1,138  of  1,613.    Elapsed: 0:02:37. with Loss: 1.1289092302322388 with total_loss: 4079.2008541226387\n",
      "  Batch 1,139  of  1,613.    Elapsed: 0:02:37. with Loss: 1.7378350496292114 with total_loss: 4080.329763352871\n",
      "  Batch 1,140  of  1,613.    Elapsed: 0:02:37. with Loss: 1.3328062295913696 with total_loss: 4082.0675984025\n",
      "  Batch 1,141  of  1,613.    Elapsed: 0:02:37. with Loss: 1.2472280263900757 with total_loss: 4083.4004046320915\n",
      "  Batch 1,142  of  1,613.    Elapsed: 0:02:38. with Loss: 0.7543106079101562 with total_loss: 4084.6476326584816\n",
      "  Batch 1,143  of  1,613.    Elapsed: 0:02:38. with Loss: 2.0463051795959473 with total_loss: 4085.4019432663918\n",
      "  Batch 1,144  of  1,613.    Elapsed: 0:02:38. with Loss: 2.1103551387786865 with total_loss: 4087.4482484459877\n",
      "  Batch 1,145  of  1,613.    Elapsed: 0:02:38. with Loss: 0.7587127089500427 with total_loss: 4089.5586035847664\n",
      "  Batch 1,146  of  1,613.    Elapsed: 0:02:38. with Loss: 1.982370376586914 with total_loss: 4090.3173162937164\n",
      "  Batch 1,147  of  1,613.    Elapsed: 0:02:38. with Loss: 1.4313762187957764 with total_loss: 4092.2996866703033\n",
      "  Batch 1,148  of  1,613.    Elapsed: 0:02:38. with Loss: 2.034015655517578 with total_loss: 4093.731062889099\n",
      "  Batch 1,149  of  1,613.    Elapsed: 0:02:39. with Loss: 1.2825989723205566 with total_loss: 4095.7650785446167\n",
      "  Batch 1,150  of  1,613.    Elapsed: 0:02:39. with Loss: 1.6304166316986084 with total_loss: 4097.047677516937\n",
      "  Batch 1,151  of  1,613.    Elapsed: 0:02:39. with Loss: 2.3211159706115723 with total_loss: 4098.678094148636\n",
      "  Batch 1,152  of  1,613.    Elapsed: 0:02:39. with Loss: 1.6891902685165405 with total_loss: 4100.999210119247\n",
      "  Batch 1,153  of  1,613.    Elapsed: 0:02:39. with Loss: 1.8201587200164795 with total_loss: 4102.688400387764\n",
      "  Batch 1,154  of  1,613.    Elapsed: 0:02:39. with Loss: 2.63861346244812 with total_loss: 4104.5085591077805\n",
      "  Batch 1,155  of  1,613.    Elapsed: 0:02:39. with Loss: 0.9539880156517029 with total_loss: 4107.147172570229\n",
      "  Batch 1,156  of  1,613.    Elapsed: 0:02:40. with Loss: 1.3776637315750122 with total_loss: 4108.10116058588\n",
      "  Batch 1,157  of  1,613.    Elapsed: 0:02:40. with Loss: 0.8032799363136292 with total_loss: 4109.478824317455\n",
      "  Batch 1,158  of  1,613.    Elapsed: 0:02:40. with Loss: 0.7789570689201355 with total_loss: 4110.282104253769\n",
      "  Batch 1,159  of  1,613.    Elapsed: 0:02:40. with Loss: 2.0098822116851807 with total_loss: 4111.061061322689\n",
      "  Batch 1,160  of  1,613.    Elapsed: 0:02:40. with Loss: 1.3116264343261719 with total_loss: 4113.070943534374\n",
      "  Batch 1,161  of  1,613.    Elapsed: 0:02:40. with Loss: 1.4716310501098633 with total_loss: 4114.3825699687\n",
      "  Batch 1,162  of  1,613.    Elapsed: 0:02:40. with Loss: 5.072754859924316 with total_loss: 4115.85420101881\n",
      "  Batch 1,163  of  1,613.    Elapsed: 0:02:41. with Loss: 0.9875319004058838 with total_loss: 4120.926955878735\n",
      "  Batch 1,164  of  1,613.    Elapsed: 0:02:41. with Loss: 1.4322316646575928 with total_loss: 4121.9144877791405\n",
      "  Batch 1,165  of  1,613.    Elapsed: 0:02:41. with Loss: 1.4894979000091553 with total_loss: 4123.346719443798\n",
      "  Batch 1,166  of  1,613.    Elapsed: 0:02:41. with Loss: 1.3620175123214722 with total_loss: 4124.836217343807\n",
      "  Batch 1,167  of  1,613.    Elapsed: 0:02:41. with Loss: 2.449136734008789 with total_loss: 4126.198234856129\n",
      "  Batch 1,168  of  1,613.    Elapsed: 0:02:41. with Loss: 0.8498589396476746 with total_loss: 4128.6473715901375\n",
      "  Batch 1,169  of  1,613.    Elapsed: 0:02:41. with Loss: 1.4276434183120728 with total_loss: 4129.497230529785\n",
      "  Batch 1,170  of  1,613.    Elapsed: 0:02:42. with Loss: 1.3249403238296509 with total_loss: 4130.924873948097\n",
      "  Batch 1,171  of  1,613.    Elapsed: 0:02:42. with Loss: 1.140932559967041 with total_loss: 4132.249814271927\n",
      "  Batch 1,172  of  1,613.    Elapsed: 0:02:42. with Loss: 0.5870320200920105 with total_loss: 4133.390746831894\n",
      "  Batch 1,173  of  1,613.    Elapsed: 0:02:42. with Loss: 1.5982333421707153 with total_loss: 4133.977778851986\n",
      "  Batch 1,174  of  1,613.    Elapsed: 0:02:42. with Loss: 1.3296244144439697 with total_loss: 4135.576012194157\n",
      "  Batch 1,175  of  1,613.    Elapsed: 0:02:42. with Loss: 2.4029996395111084 with total_loss: 4136.905636608601\n",
      "  Batch 1,176  of  1,613.    Elapsed: 0:02:42. with Loss: 1.6927720308303833 with total_loss: 4139.308636248112\n",
      "  Batch 1,177  of  1,613.    Elapsed: 0:02:42. with Loss: 0.9979692697525024 with total_loss: 4141.001408278942\n",
      "  Batch 1,178  of  1,613.    Elapsed: 0:02:43. with Loss: 1.061468482017517 with total_loss: 4141.999377548695\n",
      "  Batch 1,179  of  1,613.    Elapsed: 0:02:43. with Loss: 1.651447057723999 with total_loss: 4143.060846030712\n",
      "  Batch 1,180  of  1,613.    Elapsed: 0:02:43. with Loss: 1.3985508680343628 with total_loss: 4144.712293088436\n",
      "  Batch 1,181  of  1,613.    Elapsed: 0:02:43. with Loss: 3.136659860610962 with total_loss: 4146.1108439564705\n",
      "  Batch 1,182  of  1,613.    Elapsed: 0:02:43. with Loss: 1.0875053405761719 with total_loss: 4149.247503817081\n",
      "  Batch 1,183  of  1,613.    Elapsed: 0:02:43. with Loss: 0.860596239566803 with total_loss: 4150.335009157658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,184  of  1,613.    Elapsed: 0:02:43. with Loss: 1.1569020748138428 with total_loss: 4151.195605397224\n",
      "  Batch 1,185  of  1,613.    Elapsed: 0:02:44. with Loss: 1.252174735069275 with total_loss: 4152.352507472038\n",
      "  Batch 1,186  of  1,613.    Elapsed: 0:02:44. with Loss: 0.899920642375946 with total_loss: 4153.6046822071075\n",
      "  Batch 1,187  of  1,613.    Elapsed: 0:02:44. with Loss: 3.0043418407440186 with total_loss: 4154.5046028494835\n",
      "  Batch 1,188  of  1,613.    Elapsed: 0:02:44. with Loss: 1.6954985857009888 with total_loss: 4157.5089446902275\n",
      "  Batch 1,189  of  1,613.    Elapsed: 0:02:44. with Loss: 1.775815725326538 with total_loss: 4159.2044432759285\n",
      "  Batch 1,190  of  1,613.    Elapsed: 0:02:44. with Loss: 1.1369132995605469 with total_loss: 4160.980259001255\n",
      "  Batch 1,191  of  1,613.    Elapsed: 0:02:44. with Loss: 0.9679614901542664 with total_loss: 4162.117172300816\n",
      "  Batch 1,192  of  1,613.    Elapsed: 0:02:45. with Loss: 1.4674980640411377 with total_loss: 4163.08513379097\n",
      "  Batch 1,193  of  1,613.    Elapsed: 0:02:45. with Loss: 4.00074577331543 with total_loss: 4164.552631855011\n",
      "  Batch 1,194  of  1,613.    Elapsed: 0:02:45. with Loss: 1.52519953250885 with total_loss: 4168.553377628326\n",
      "  Batch 1,195  of  1,613.    Elapsed: 0:02:45. with Loss: 2.0590922832489014 with total_loss: 4170.078577160835\n",
      "  Batch 1,196  of  1,613.    Elapsed: 0:02:45. with Loss: 1.1835887432098389 with total_loss: 4172.137669444084\n",
      "  Batch 1,197  of  1,613.    Elapsed: 0:02:45. with Loss: 2.5901830196380615 with total_loss: 4173.321258187294\n",
      "  Batch 1,198  of  1,613.    Elapsed: 0:02:45. with Loss: 1.0799177885055542 with total_loss: 4175.911441206932\n",
      "  Batch 1,199  of  1,613.    Elapsed: 0:02:46. with Loss: 2.2617125511169434 with total_loss: 4176.991358995438\n",
      "  Batch 1,200  of  1,613.    Elapsed: 0:02:46. with Loss: 1.3168660402297974 with total_loss: 4179.253071546555\n",
      "  Batch 1,201  of  1,613.    Elapsed: 0:02:46. with Loss: 2.095419406890869 with total_loss: 4180.569937586784\n",
      "  Batch 1,202  of  1,613.    Elapsed: 0:02:46. with Loss: 2.042382001876831 with total_loss: 4182.665356993675\n",
      "  Batch 1,203  of  1,613.    Elapsed: 0:02:46. with Loss: 1.747956395149231 with total_loss: 4184.707738995552\n",
      "  Batch 1,204  of  1,613.    Elapsed: 0:02:46. with Loss: 1.85323166847229 with total_loss: 4186.455695390701\n",
      "  Batch 1,205  of  1,613.    Elapsed: 0:02:46. with Loss: 1.3964695930480957 with total_loss: 4188.308927059174\n",
      "  Batch 1,206  of  1,613.    Elapsed: 0:02:46. with Loss: 1.3034100532531738 with total_loss: 4189.705396652222\n",
      "  Batch 1,207  of  1,613.    Elapsed: 0:02:47. with Loss: 1.791037678718567 with total_loss: 4191.008806705475\n",
      "  Batch 1,208  of  1,613.    Elapsed: 0:02:47. with Loss: 1.3558800220489502 with total_loss: 4192.799844384193\n",
      "  Batch 1,209  of  1,613.    Elapsed: 0:02:47. with Loss: 1.0879466533660889 with total_loss: 4194.155724406242\n",
      "  Batch 1,210  of  1,613.    Elapsed: 0:02:47. with Loss: 1.575167179107666 with total_loss: 4195.2436710596085\n",
      "  Batch 1,211  of  1,613.    Elapsed: 0:02:47. with Loss: 0.9210289120674133 with total_loss: 4196.818838238716\n",
      "  Batch 1,212  of  1,613.    Elapsed: 0:02:47. with Loss: 1.2855759859085083 with total_loss: 4197.7398671507835\n",
      "  Batch 1,213  of  1,613.    Elapsed: 0:02:47. with Loss: 1.6449248790740967 with total_loss: 4199.025443136692\n",
      "  Batch 1,214  of  1,613.    Elapsed: 0:02:48. with Loss: 1.5491442680358887 with total_loss: 4200.670368015766\n",
      "  Batch 1,215  of  1,613.    Elapsed: 0:02:48. with Loss: 1.797865390777588 with total_loss: 4202.219512283802\n",
      "  Batch 1,216  of  1,613.    Elapsed: 0:02:48. with Loss: 0.5414533019065857 with total_loss: 4204.01737767458\n",
      "  Batch 1,217  of  1,613.    Elapsed: 0:02:48. with Loss: 0.9952383041381836 with total_loss: 4204.558830976486\n",
      "  Batch 1,218  of  1,613.    Elapsed: 0:02:48. with Loss: 1.3630166053771973 with total_loss: 4205.554069280624\n",
      "  Batch 1,219  of  1,613.    Elapsed: 0:02:48. with Loss: 1.2181456089019775 with total_loss: 4206.917085886002\n",
      "  Batch 1,220  of  1,613.    Elapsed: 0:02:48. with Loss: 2.842585325241089 with total_loss: 4208.135231494904\n",
      "  Batch 1,221  of  1,613.    Elapsed: 0:02:49. with Loss: 1.4266812801361084 with total_loss: 4210.977816820145\n",
      "  Batch 1,222  of  1,613.    Elapsed: 0:02:49. with Loss: 1.0329545736312866 with total_loss: 4212.404498100281\n",
      "  Batch 1,223  of  1,613.    Elapsed: 0:02:49. with Loss: 4.186570644378662 with total_loss: 4213.437452673912\n",
      "  Batch 1,224  of  1,613.    Elapsed: 0:02:49. with Loss: 1.3929957151412964 with total_loss: 4217.624023318291\n",
      "  Batch 1,225  of  1,613.    Elapsed: 0:02:49. with Loss: 2.5524778366088867 with total_loss: 4219.017019033432\n",
      "  Batch 1,226  of  1,613.    Elapsed: 0:02:49. with Loss: 1.9983270168304443 with total_loss: 4221.569496870041\n",
      "  Batch 1,227  of  1,613.    Elapsed: 0:02:49. with Loss: 1.5622148513793945 with total_loss: 4223.567823886871\n",
      "  Batch 1,228  of  1,613.    Elapsed: 0:02:50. with Loss: 2.7814252376556396 with total_loss: 4225.130038738251\n",
      "  Batch 1,229  of  1,613.    Elapsed: 0:02:50. with Loss: 1.2626606225967407 with total_loss: 4227.911463975906\n",
      "  Batch 1,230  of  1,613.    Elapsed: 0:02:50. with Loss: 1.7283744812011719 with total_loss: 4229.174124598503\n",
      "  Batch 1,231  of  1,613.    Elapsed: 0:02:50. with Loss: 1.0655128955841064 with total_loss: 4230.902499079704\n",
      "  Batch 1,232  of  1,613.    Elapsed: 0:02:50. with Loss: 2.026181221008301 with total_loss: 4231.968011975288\n",
      "  Batch 1,233  of  1,613.    Elapsed: 0:02:50. with Loss: 0.7968463897705078 with total_loss: 4233.994193196297\n",
      "  Batch 1,234  of  1,613.    Elapsed: 0:02:50. with Loss: 0.6576935052871704 with total_loss: 4234.791039586067\n",
      "  Batch 1,235  of  1,613.    Elapsed: 0:02:50. with Loss: 0.9751349091529846 with total_loss: 4235.448733091354\n",
      "  Batch 1,236  of  1,613.    Elapsed: 0:02:51. with Loss: 1.4750168323516846 with total_loss: 4236.423868000507\n",
      "  Batch 1,237  of  1,613.    Elapsed: 0:02:51. with Loss: 1.2747588157653809 with total_loss: 4237.898884832859\n",
      "  Batch 1,238  of  1,613.    Elapsed: 0:02:51. with Loss: 1.9049897193908691 with total_loss: 4239.173643648624\n",
      "  Batch 1,239  of  1,613.    Elapsed: 0:02:51. with Loss: 1.0378233194351196 with total_loss: 4241.078633368015\n",
      "  Batch 1,240  of  1,613.    Elapsed: 0:02:51. with Loss: 1.8201504945755005 with total_loss: 4242.11645668745\n",
      "  Batch 1,241  of  1,613.    Elapsed: 0:02:51. with Loss: 1.2859464883804321 with total_loss: 4243.936607182026\n",
      "  Batch 1,242  of  1,613.    Elapsed: 0:02:51. with Loss: 0.7858912348747253 with total_loss: 4245.222553670406\n",
      "  Batch 1,243  of  1,613.    Elapsed: 0:02:52. with Loss: 1.1852225065231323 with total_loss: 4246.008444905281\n",
      "  Batch 1,244  of  1,613.    Elapsed: 0:02:52. with Loss: 0.9236652255058289 with total_loss: 4247.193667411804\n",
      "  Batch 1,245  of  1,613.    Elapsed: 0:02:52. with Loss: 1.618255853652954 with total_loss: 4248.11733263731\n",
      "  Batch 1,246  of  1,613.    Elapsed: 0:02:52. with Loss: 0.6885830163955688 with total_loss: 4249.735588490963\n",
      "  Batch 1,247  of  1,613.    Elapsed: 0:02:52. with Loss: 2.038083553314209 with total_loss: 4250.424171507359\n",
      "  Batch 1,248  of  1,613.    Elapsed: 0:02:52. with Loss: 1.5833443403244019 with total_loss: 4252.462255060673\n",
      "  Batch 1,249  of  1,613.    Elapsed: 0:02:52. with Loss: 1.0799373388290405 with total_loss: 4254.045599400997\n",
      "  Batch 1,250  of  1,613.    Elapsed: 0:02:53. with Loss: 1.203018069267273 with total_loss: 4255.125536739826\n",
      "  Batch 1,251  of  1,613.    Elapsed: 0:02:53. with Loss: 1.6015311479568481 with total_loss: 4256.3285548090935\n",
      "  Batch 1,252  of  1,613.    Elapsed: 0:02:53. with Loss: 1.896400809288025 with total_loss: 4257.93008595705\n",
      "  Batch 1,253  of  1,613.    Elapsed: 0:02:53. with Loss: 1.3316665887832642 with total_loss: 4259.826486766338\n",
      "  Batch 1,254  of  1,613.    Elapsed: 0:02:53. with Loss: 1.2266398668289185 with total_loss: 4261.158153355122\n",
      "  Batch 1,255  of  1,613.    Elapsed: 0:02:53. with Loss: 14.767374038696289 with total_loss: 4262.3847932219505\n",
      "  Batch 1,256  of  1,613.    Elapsed: 0:02:53. with Loss: 1.120318055152893 with total_loss: 4277.152167260647\n",
      "  Batch 1,257  of  1,613.    Elapsed: 0:02:54. with Loss: 1.634718418121338 with total_loss: 4278.2724853158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,258  of  1,613.    Elapsed: 0:02:54. with Loss: 1.9947454929351807 with total_loss: 4279.907203733921\n",
      "  Batch 1,259  of  1,613.    Elapsed: 0:02:54. with Loss: 1.654732584953308 with total_loss: 4281.901949226856\n",
      "  Batch 1,260  of  1,613.    Elapsed: 0:02:54. with Loss: 1.1261684894561768 with total_loss: 4283.5566818118095\n",
      "  Batch 1,261  of  1,613.    Elapsed: 0:02:54. with Loss: 1.009238600730896 with total_loss: 4284.682850301266\n",
      "  Batch 1,262  of  1,613.    Elapsed: 0:02:54. with Loss: 1.0822198390960693 with total_loss: 4285.692088901997\n",
      "  Batch 1,263  of  1,613.    Elapsed: 0:02:54. with Loss: 1.1781970262527466 with total_loss: 4286.774308741093\n",
      "  Batch 1,264  of  1,613.    Elapsed: 0:02:54. with Loss: 2.441056966781616 with total_loss: 4287.952505767345\n",
      "  Batch 1,265  of  1,613.    Elapsed: 0:02:55. with Loss: 1.1051900386810303 with total_loss: 4290.393562734127\n",
      "  Batch 1,266  of  1,613.    Elapsed: 0:02:55. with Loss: 1.5672879219055176 with total_loss: 4291.498752772808\n",
      "  Batch 1,267  of  1,613.    Elapsed: 0:02:55. with Loss: 1.410459041595459 with total_loss: 4293.066040694714\n",
      "  Batch 1,268  of  1,613.    Elapsed: 0:02:55. with Loss: 1.3528467416763306 with total_loss: 4294.476499736309\n",
      "  Batch 1,269  of  1,613.    Elapsed: 0:02:55. with Loss: 1.229627013206482 with total_loss: 4295.829346477985\n",
      "  Batch 1,270  of  1,613.    Elapsed: 0:02:55. with Loss: 2.359980821609497 with total_loss: 4297.058973491192\n",
      "  Batch 1,271  of  1,613.    Elapsed: 0:02:55. with Loss: 2.35284686088562 with total_loss: 4299.418954312801\n",
      "  Batch 1,272  of  1,613.    Elapsed: 0:02:56. with Loss: 2.2466020584106445 with total_loss: 4301.771801173687\n",
      "  Batch 1,273  of  1,613.    Elapsed: 0:02:56. with Loss: 1.3169447183609009 with total_loss: 4304.018403232098\n",
      "  Batch 1,274  of  1,613.    Elapsed: 0:02:56. with Loss: 1.1795573234558105 with total_loss: 4305.3353479504585\n",
      "  Batch 1,275  of  1,613.    Elapsed: 0:02:56. with Loss: 0.8156713247299194 with total_loss: 4306.514905273914\n",
      "  Batch 1,276  of  1,613.    Elapsed: 0:02:56. with Loss: 0.9796048998832703 with total_loss: 4307.330576598644\n",
      "  Batch 1,277  of  1,613.    Elapsed: 0:02:56. with Loss: 0.9939687252044678 with total_loss: 4308.3101814985275\n",
      "  Batch 1,278  of  1,613.    Elapsed: 0:02:56. with Loss: 2.4597203731536865 with total_loss: 4309.304150223732\n",
      "  Batch 1,279  of  1,613.    Elapsed: 0:02:57. with Loss: 1.2035362720489502 with total_loss: 4311.763870596886\n",
      "  Batch 1,280  of  1,613.    Elapsed: 0:02:57. with Loss: 3.399153709411621 with total_loss: 4312.967406868935\n",
      "  Batch 1,281  of  1,613.    Elapsed: 0:02:57. with Loss: 1.0938200950622559 with total_loss: 4316.366560578346\n",
      "  Batch 1,282  of  1,613.    Elapsed: 0:02:57. with Loss: 1.3287783861160278 with total_loss: 4317.4603806734085\n",
      "  Batch 1,283  of  1,613.    Elapsed: 0:02:57. with Loss: 1.3870025873184204 with total_loss: 4318.7891590595245\n",
      "  Batch 1,284  of  1,613.    Elapsed: 0:02:57. with Loss: 9.707030296325684 with total_loss: 4320.176161646843\n",
      "  Batch 1,285  of  1,613.    Elapsed: 0:02:57. with Loss: 1.5955551862716675 with total_loss: 4329.883191943169\n",
      "  Batch 1,286  of  1,613.    Elapsed: 0:02:58. with Loss: 0.8582922220230103 with total_loss: 4331.47874712944\n",
      "  Batch 1,287  of  1,613.    Elapsed: 0:02:58. with Loss: 1.0096193552017212 with total_loss: 4332.337039351463\n",
      "  Batch 1,288  of  1,613.    Elapsed: 0:02:58. with Loss: 2.1350414752960205 with total_loss: 4333.346658706665\n",
      "  Batch 1,289  of  1,613.    Elapsed: 0:02:58. with Loss: 1.593491792678833 with total_loss: 4335.481700181961\n",
      "  Batch 1,290  of  1,613.    Elapsed: 0:02:58. with Loss: 1.22782564163208 with total_loss: 4337.07519197464\n",
      "  Batch 1,291  of  1,613.    Elapsed: 0:02:58. with Loss: 1.507756233215332 with total_loss: 4338.303017616272\n",
      "  Batch 1,292  of  1,613.    Elapsed: 0:02:58. with Loss: 0.9372983574867249 with total_loss: 4339.810773849487\n",
      "  Batch 1,293  of  1,613.    Elapsed: 0:02:59. with Loss: 1.4085986614227295 with total_loss: 4340.748072206974\n",
      "  Batch 1,294  of  1,613.    Elapsed: 0:02:59. with Loss: 2.7991156578063965 with total_loss: 4342.156670868397\n",
      "  Batch 1,295  of  1,613.    Elapsed: 0:02:59. with Loss: 1.8656128644943237 with total_loss: 4344.955786526203\n",
      "  Batch 1,296  of  1,613.    Elapsed: 0:02:59. with Loss: 2.146568775177002 with total_loss: 4346.8213993906975\n",
      "  Batch 1,297  of  1,613.    Elapsed: 0:02:59. with Loss: 0.8635746836662292 with total_loss: 4348.9679681658745\n",
      "  Batch 1,298  of  1,613.    Elapsed: 0:02:59. with Loss: 1.402803897857666 with total_loss: 4349.831542849541\n",
      "  Batch 1,299  of  1,613.    Elapsed: 0:02:59. with Loss: 1.7695198059082031 with total_loss: 4351.234346747398\n",
      "  Batch 1,300  of  1,613.    Elapsed: 0:02:59. with Loss: 0.8653753399848938 with total_loss: 4353.003866553307\n",
      "  Batch 1,301  of  1,613.    Elapsed: 0:03:00. with Loss: 2.0281472206115723 with total_loss: 4353.8692418932915\n",
      "  Batch 1,302  of  1,613.    Elapsed: 0:03:00. with Loss: 0.9699432849884033 with total_loss: 4355.897389113903\n",
      "  Batch 1,303  of  1,613.    Elapsed: 0:03:00. with Loss: 0.8383248448371887 with total_loss: 4356.867332398891\n",
      "  Batch 1,304  of  1,613.    Elapsed: 0:03:00. with Loss: 1.4449330568313599 with total_loss: 4357.705657243729\n",
      "  Batch 1,305  of  1,613.    Elapsed: 0:03:00. with Loss: 4.873485088348389 with total_loss: 4359.15059030056\n",
      "  Batch 1,306  of  1,613.    Elapsed: 0:03:00. with Loss: 2.392812490463257 with total_loss: 4364.024075388908\n",
      "  Batch 1,307  of  1,613.    Elapsed: 0:03:00. with Loss: 2.054786205291748 with total_loss: 4366.416887879372\n",
      "  Batch 1,308  of  1,613.    Elapsed: 0:03:01. with Loss: 1.179005742073059 with total_loss: 4368.471674084663\n",
      "  Batch 1,309  of  1,613.    Elapsed: 0:03:01. with Loss: 1.5612502098083496 with total_loss: 4369.650679826736\n",
      "  Batch 1,310  of  1,613.    Elapsed: 0:03:01. with Loss: 1.5013039112091064 with total_loss: 4371.211930036545\n",
      "  Batch 1,311  of  1,613.    Elapsed: 0:03:01. with Loss: 1.4886506795883179 with total_loss: 4372.713233947754\n",
      "  Batch 1,312  of  1,613.    Elapsed: 0:03:01. with Loss: 2.545435905456543 with total_loss: 4374.201884627342\n",
      "  Batch 1,313  of  1,613.    Elapsed: 0:03:01. with Loss: 1.4209611415863037 with total_loss: 4376.747320532799\n",
      "  Batch 1,314  of  1,613.    Elapsed: 0:03:01. with Loss: 3.2308897972106934 with total_loss: 4378.168281674385\n",
      "  Batch 1,315  of  1,613.    Elapsed: 0:03:02. with Loss: 0.7239429950714111 with total_loss: 4381.399171471596\n",
      "  Batch 1,316  of  1,613.    Elapsed: 0:03:02. with Loss: 1.792233943939209 with total_loss: 4382.123114466667\n",
      "  Batch 1,317  of  1,613.    Elapsed: 0:03:02. with Loss: 2.9020192623138428 with total_loss: 4383.915348410606\n",
      "  Batch 1,318  of  1,613.    Elapsed: 0:03:02. with Loss: 1.847517728805542 with total_loss: 4386.81736767292\n",
      "  Batch 1,319  of  1,613.    Elapsed: 0:03:02. with Loss: 2.9611942768096924 with total_loss: 4388.664885401726\n",
      "  Batch 1,320  of  1,613.    Elapsed: 0:03:02. with Loss: 1.2041174173355103 with total_loss: 4391.6260796785355\n",
      "  Batch 1,321  of  1,613.    Elapsed: 0:03:02. with Loss: 1.0683139562606812 with total_loss: 4392.830197095871\n",
      "  Batch 1,322  of  1,613.    Elapsed: 0:03:03. with Loss: 0.9460635781288147 with total_loss: 4393.898511052132\n",
      "  Batch 1,323  of  1,613.    Elapsed: 0:03:03. with Loss: 1.6156448125839233 with total_loss: 4394.8445746302605\n",
      "  Batch 1,324  of  1,613.    Elapsed: 0:03:03. with Loss: 1.955291509628296 with total_loss: 4396.460219442844\n",
      "  Batch 1,325  of  1,613.    Elapsed: 0:03:03. with Loss: 0.9905690550804138 with total_loss: 4398.415510952473\n",
      "  Batch 1,326  of  1,613.    Elapsed: 0:03:03. with Loss: 1.1726783514022827 with total_loss: 4399.406080007553\n",
      "  Batch 1,327  of  1,613.    Elapsed: 0:03:03. with Loss: 1.0367311239242554 with total_loss: 4400.578758358955\n",
      "  Batch 1,328  of  1,613.    Elapsed: 0:03:03. with Loss: 1.1070351600646973 with total_loss: 4401.61548948288\n",
      "  Batch 1,329  of  1,613.    Elapsed: 0:03:03. with Loss: 1.2716423273086548 with total_loss: 4402.722524642944\n",
      "  Batch 1,330  of  1,613.    Elapsed: 0:03:04. with Loss: 0.9920530319213867 with total_loss: 4403.994166970253\n",
      "  Batch 1,331  of  1,613.    Elapsed: 0:03:04. with Loss: 2.3499581813812256 with total_loss: 4404.986220002174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,332  of  1,613.    Elapsed: 0:03:04. with Loss: 1.86995267868042 with total_loss: 4407.336178183556\n",
      "  Batch 1,333  of  1,613.    Elapsed: 0:03:04. with Loss: 1.2330914735794067 with total_loss: 4409.206130862236\n",
      "  Batch 1,334  of  1,613.    Elapsed: 0:03:04. with Loss: 1.0768464803695679 with total_loss: 4410.439222335815\n",
      "  Batch 1,335  of  1,613.    Elapsed: 0:03:04. with Loss: 3.0855748653411865 with total_loss: 4411.516068816185\n",
      "  Batch 1,336  of  1,613.    Elapsed: 0:03:04. with Loss: 1.198714017868042 with total_loss: 4414.601643681526\n",
      "  Batch 1,337  of  1,613.    Elapsed: 0:03:05. with Loss: 1.9255188703536987 with total_loss: 4415.800357699394\n",
      "  Batch 1,338  of  1,613.    Elapsed: 0:03:05. with Loss: 2.49580979347229 with total_loss: 4417.725876569748\n",
      "  Batch 1,339  of  1,613.    Elapsed: 0:03:05. with Loss: 1.2904771566390991 with total_loss: 4420.22168636322\n",
      "  Batch 1,340  of  1,613.    Elapsed: 0:03:05. with Loss: 1.4504567384719849 with total_loss: 4421.512163519859\n",
      "  Batch 1,341  of  1,613.    Elapsed: 0:03:05. with Loss: 0.8948025703430176 with total_loss: 4422.962620258331\n",
      "  Batch 1,342  of  1,613.    Elapsed: 0:03:05. with Loss: 1.297762393951416 with total_loss: 4423.857422828674\n",
      "  Batch 1,343  of  1,613.    Elapsed: 0:03:05. with Loss: 1.831866979598999 with total_loss: 4425.155185222626\n",
      "  Batch 1,344  of  1,613.    Elapsed: 0:03:06. with Loss: 0.8755877614021301 with total_loss: 4426.987052202225\n",
      "  Batch 1,345  of  1,613.    Elapsed: 0:03:06. with Loss: 0.8855918049812317 with total_loss: 4427.862639963627\n",
      "  Batch 1,346  of  1,613.    Elapsed: 0:03:06. with Loss: 0.7143856287002563 with total_loss: 4428.748231768608\n",
      "  Batch 1,347  of  1,613.    Elapsed: 0:03:06. with Loss: 1.706336498260498 with total_loss: 4429.462617397308\n",
      "  Batch 1,348  of  1,613.    Elapsed: 0:03:06. with Loss: 1.9033477306365967 with total_loss: 4431.168953895569\n",
      "  Batch 1,349  of  1,613.    Elapsed: 0:03:06. with Loss: 1.6042686700820923 with total_loss: 4433.072301626205\n",
      "  Batch 1,350  of  1,613.    Elapsed: 0:03:06. with Loss: 0.9476092457771301 with total_loss: 4434.6765702962875\n",
      "  Batch 1,351  of  1,613.    Elapsed: 0:03:07. with Loss: 2.506889820098877 with total_loss: 4435.624179542065\n",
      "  Batch 1,352  of  1,613.    Elapsed: 0:03:07. with Loss: 1.063220500946045 with total_loss: 4438.1310693621635\n",
      "  Batch 1,353  of  1,613.    Elapsed: 0:03:07. with Loss: 1.405315637588501 with total_loss: 4439.19428986311\n",
      "  Batch 1,354  of  1,613.    Elapsed: 0:03:07. with Loss: 1.1913245916366577 with total_loss: 4440.599605500698\n",
      "  Batch 1,355  of  1,613.    Elapsed: 0:03:07. with Loss: 1.339054822921753 with total_loss: 4441.790930092335\n",
      "  Batch 1,356  of  1,613.    Elapsed: 0:03:07. with Loss: 0.9598548412322998 with total_loss: 4443.1299849152565\n",
      "  Batch 1,357  of  1,613.    Elapsed: 0:03:07. with Loss: 2.1789162158966064 with total_loss: 4444.089839756489\n",
      "  Batch 1,358  of  1,613.    Elapsed: 0:03:08. with Loss: 1.626481056213379 with total_loss: 4446.268755972385\n",
      "  Batch 1,359  of  1,613.    Elapsed: 0:03:08. with Loss: 1.028519868850708 with total_loss: 4447.895237028599\n",
      "  Batch 1,360  of  1,613.    Elapsed: 0:03:08. with Loss: 1.738354206085205 with total_loss: 4448.9237568974495\n",
      "  Batch 1,361  of  1,613.    Elapsed: 0:03:08. with Loss: 1.4704655408859253 with total_loss: 4450.662111103535\n",
      "  Batch 1,362  of  1,613.    Elapsed: 0:03:08. with Loss: 1.6400909423828125 with total_loss: 4452.132576644421\n",
      "  Batch 1,363  of  1,613.    Elapsed: 0:03:08. with Loss: 1.7167880535125732 with total_loss: 4453.772667586803\n",
      "  Batch 1,364  of  1,613.    Elapsed: 0:03:08. with Loss: 1.8134055137634277 with total_loss: 4455.489455640316\n",
      "  Batch 1,365  of  1,613.    Elapsed: 0:03:08. with Loss: 1.1268514394760132 with total_loss: 4457.302861154079\n",
      "  Batch 1,366  of  1,613.    Elapsed: 0:03:09. with Loss: 0.9432881474494934 with total_loss: 4458.429712593555\n",
      "  Batch 1,367  of  1,613.    Elapsed: 0:03:09. with Loss: 0.8458442091941833 with total_loss: 4459.373000741005\n",
      "  Batch 1,368  of  1,613.    Elapsed: 0:03:09. with Loss: 1.000104308128357 with total_loss: 4460.218844950199\n",
      "  Batch 1,369  of  1,613.    Elapsed: 0:03:09. with Loss: 1.8113902807235718 with total_loss: 4461.2189492583275\n",
      "  Batch 1,370  of  1,613.    Elapsed: 0:03:09. with Loss: 2.082975149154663 with total_loss: 4463.030339539051\n",
      "  Batch 1,371  of  1,613.    Elapsed: 0:03:09. with Loss: 1.2492178678512573 with total_loss: 4465.113314688206\n",
      "  Batch 1,372  of  1,613.    Elapsed: 0:03:09. with Loss: 0.8209569454193115 with total_loss: 4466.362532556057\n",
      "  Batch 1,373  of  1,613.    Elapsed: 0:03:10. with Loss: 1.2965596914291382 with total_loss: 4467.183489501476\n",
      "  Batch 1,374  of  1,613.    Elapsed: 0:03:10. with Loss: 0.9947736859321594 with total_loss: 4468.480049192905\n",
      "  Batch 1,375  of  1,613.    Elapsed: 0:03:10. with Loss: 1.1768661737442017 with total_loss: 4469.474822878838\n",
      "  Batch 1,376  of  1,613.    Elapsed: 0:03:10. with Loss: 1.015047550201416 with total_loss: 4470.651689052582\n",
      "  Batch 1,377  of  1,613.    Elapsed: 0:03:10. with Loss: 1.696347951889038 with total_loss: 4471.666736602783\n",
      "  Batch 1,378  of  1,613.    Elapsed: 0:03:10. with Loss: 2.1941559314727783 with total_loss: 4473.363084554672\n",
      "  Batch 1,379  of  1,613.    Elapsed: 0:03:10. with Loss: 1.4764325618743896 with total_loss: 4475.557240486145\n",
      "  Batch 1,380  of  1,613.    Elapsed: 0:03:11. with Loss: 0.699833333492279 with total_loss: 4477.033673048019\n",
      "  Batch 1,381  of  1,613.    Elapsed: 0:03:11. with Loss: 1.503071904182434 with total_loss: 4477.733506381512\n",
      "  Batch 1,382  of  1,613.    Elapsed: 0:03:11. with Loss: 0.979228675365448 with total_loss: 4479.236578285694\n",
      "  Batch 1,383  of  1,613.    Elapsed: 0:03:11. with Loss: 3.660335063934326 with total_loss: 4480.21580696106\n",
      "  Batch 1,384  of  1,613.    Elapsed: 0:03:11. with Loss: 1.0463889837265015 with total_loss: 4483.876142024994\n",
      "  Batch 1,385  of  1,613.    Elapsed: 0:03:11. with Loss: 1.8477553129196167 with total_loss: 4484.92253100872\n",
      "  Batch 1,386  of  1,613.    Elapsed: 0:03:11. with Loss: 0.9816986322402954 with total_loss: 4486.77028632164\n",
      "  Batch 1,387  of  1,613.    Elapsed: 0:03:12. with Loss: 0.9133785367012024 with total_loss: 4487.75198495388\n",
      "  Batch 1,388  of  1,613.    Elapsed: 0:03:12. with Loss: 1.4219601154327393 with total_loss: 4488.6653634905815\n",
      "  Batch 1,389  of  1,613.    Elapsed: 0:03:12. with Loss: 0.981188952922821 with total_loss: 4490.087323606014\n",
      "  Batch 1,390  of  1,613.    Elapsed: 0:03:12. with Loss: 0.9526710510253906 with total_loss: 4491.068512558937\n",
      "  Batch 1,391  of  1,613.    Elapsed: 0:03:12. with Loss: 0.8685925006866455 with total_loss: 4492.0211836099625\n",
      "  Batch 1,392  of  1,613.    Elapsed: 0:03:12. with Loss: 1.3571127653121948 with total_loss: 4492.889776110649\n",
      "  Batch 1,393  of  1,613.    Elapsed: 0:03:12. with Loss: 0.8963730335235596 with total_loss: 4494.246888875961\n",
      "  Batch 1,394  of  1,613.    Elapsed: 0:03:12. with Loss: 1.5817822217941284 with total_loss: 4495.143261909485\n",
      "  Batch 1,395  of  1,613.    Elapsed: 0:03:13. with Loss: 1.109587550163269 with total_loss: 4496.725044131279\n",
      "  Batch 1,396  of  1,613.    Elapsed: 0:03:13. with Loss: 0.9028880596160889 with total_loss: 4497.834631681442\n",
      "  Batch 1,397  of  1,613.    Elapsed: 0:03:13. with Loss: 1.0600690841674805 with total_loss: 4498.737519741058\n",
      "  Batch 1,398  of  1,613.    Elapsed: 0:03:13. with Loss: 0.9518786668777466 with total_loss: 4499.797588825226\n",
      "  Batch 1,399  of  1,613.    Elapsed: 0:03:13. with Loss: 2.091425895690918 with total_loss: 4500.749467492104\n",
      "  Batch 1,400  of  1,613.    Elapsed: 0:03:13. with Loss: 2.53169846534729 with total_loss: 4502.8408933877945\n",
      "  Batch 1,401  of  1,613.    Elapsed: 0:03:13. with Loss: 1.403733491897583 with total_loss: 4505.372591853142\n",
      "  Batch 1,402  of  1,613.    Elapsed: 0:03:14. with Loss: 1.1741292476654053 with total_loss: 4506.776325345039\n",
      "  Batch 1,403  of  1,613.    Elapsed: 0:03:14. with Loss: 1.0600249767303467 with total_loss: 4507.950454592705\n",
      "  Batch 1,404  of  1,613.    Elapsed: 0:03:14. with Loss: 2.63582444190979 with total_loss: 4509.010479569435\n",
      "  Batch 1,405  of  1,613.    Elapsed: 0:03:14. with Loss: 1.4788562059402466 with total_loss: 4511.646304011345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,406  of  1,613.    Elapsed: 0:03:14. with Loss: 1.797303318977356 with total_loss: 4513.125160217285\n",
      "  Batch 1,407  of  1,613.    Elapsed: 0:03:14. with Loss: 1.7976888418197632 with total_loss: 4514.9224635362625\n",
      "  Batch 1,408  of  1,613.    Elapsed: 0:03:14. with Loss: 1.3084427118301392 with total_loss: 4516.720152378082\n",
      "  Batch 1,409  of  1,613.    Elapsed: 0:03:15. with Loss: 1.6651207208633423 with total_loss: 4518.028595089912\n",
      "  Batch 1,410  of  1,613.    Elapsed: 0:03:15. with Loss: 1.7092986106872559 with total_loss: 4519.693715810776\n",
      "  Batch 1,411  of  1,613.    Elapsed: 0:03:15. with Loss: 1.4832028150558472 with total_loss: 4521.403014421463\n",
      "  Batch 1,412  of  1,613.    Elapsed: 0:03:15. with Loss: 1.6628719568252563 with total_loss: 4522.886217236519\n",
      "  Batch 1,413  of  1,613.    Elapsed: 0:03:15. with Loss: 0.7562878727912903 with total_loss: 4524.549089193344\n",
      "  Batch 1,414  of  1,613.    Elapsed: 0:03:15. with Loss: 1.1039865016937256 with total_loss: 4525.305377066135\n",
      "  Batch 1,415  of  1,613.    Elapsed: 0:03:15. with Loss: 3.553685426712036 with total_loss: 4526.409363567829\n",
      "  Batch 1,416  of  1,613.    Elapsed: 0:03:16. with Loss: 1.5734330415725708 with total_loss: 4529.963048994541\n",
      "  Batch 1,417  of  1,613.    Elapsed: 0:03:16. with Loss: 0.8583390116691589 with total_loss: 4531.536482036114\n",
      "  Batch 1,418  of  1,613.    Elapsed: 0:03:16. with Loss: 1.3469667434692383 with total_loss: 4532.394821047783\n",
      "  Batch 1,419  of  1,613.    Elapsed: 0:03:16. with Loss: 1.741944432258606 with total_loss: 4533.741787791252\n",
      "  Batch 1,420  of  1,613.    Elapsed: 0:03:16. with Loss: 1.316253662109375 with total_loss: 4535.483732223511\n",
      "  Batch 1,421  of  1,613.    Elapsed: 0:03:16. with Loss: 1.1735790967941284 with total_loss: 4536.79998588562\n",
      "  Batch 1,422  of  1,613.    Elapsed: 0:03:16. with Loss: 1.3217220306396484 with total_loss: 4537.973564982414\n",
      "  Batch 1,423  of  1,613.    Elapsed: 0:03:17. with Loss: 1.5742801427841187 with total_loss: 4539.295287013054\n",
      "  Batch 1,424  of  1,613.    Elapsed: 0:03:17. with Loss: 1.037343144416809 with total_loss: 4540.869567155838\n",
      "  Batch 1,425  of  1,613.    Elapsed: 0:03:17. with Loss: 1.2595542669296265 with total_loss: 4541.906910300255\n",
      "  Batch 1,426  of  1,613.    Elapsed: 0:03:17. with Loss: 0.7107993960380554 with total_loss: 4543.166464567184\n",
      "  Batch 1,427  of  1,613.    Elapsed: 0:03:17. with Loss: 1.2613344192504883 with total_loss: 4543.8772639632225\n",
      "  Batch 1,428  of  1,613.    Elapsed: 0:03:17. with Loss: 1.5099759101867676 with total_loss: 4545.138598382473\n",
      "  Batch 1,429  of  1,613.    Elapsed: 0:03:17. with Loss: 1.2058391571044922 with total_loss: 4546.64857429266\n",
      "  Batch 1,430  of  1,613.    Elapsed: 0:03:17. with Loss: 0.9045518040657043 with total_loss: 4547.854413449764\n",
      "  Batch 1,431  of  1,613.    Elapsed: 0:03:18. with Loss: 1.2580547332763672 with total_loss: 4548.75896525383\n",
      "  Batch 1,432  of  1,613.    Elapsed: 0:03:18. with Loss: 1.7518748044967651 with total_loss: 4550.017019987106\n",
      "  Batch 1,433  of  1,613.    Elapsed: 0:03:18. with Loss: 1.1155799627304077 with total_loss: 4551.768894791603\n",
      "  Batch 1,434  of  1,613.    Elapsed: 0:03:18. with Loss: 1.4076275825500488 with total_loss: 4552.8844747543335\n",
      "  Batch 1,435  of  1,613.    Elapsed: 0:03:18. with Loss: 2.145862579345703 with total_loss: 4554.2921023368835\n",
      "  Batch 1,436  of  1,613.    Elapsed: 0:03:18. with Loss: 0.9293228983879089 with total_loss: 4556.437964916229\n",
      "  Batch 1,437  of  1,613.    Elapsed: 0:03:18. with Loss: 1.0742640495300293 with total_loss: 4557.367287814617\n",
      "  Batch 1,438  of  1,613.    Elapsed: 0:03:19. with Loss: 2.3743419647216797 with total_loss: 4558.441551864147\n",
      "  Batch 1,439  of  1,613.    Elapsed: 0:03:19. with Loss: 0.9396302700042725 with total_loss: 4560.815893828869\n",
      "  Batch 1,440  of  1,613.    Elapsed: 0:03:19. with Loss: 1.4001271724700928 with total_loss: 4561.755524098873\n",
      "  Batch 1,441  of  1,613.    Elapsed: 0:03:19. with Loss: 2.9471542835235596 with total_loss: 4563.155651271343\n",
      "  Batch 1,442  of  1,613.    Elapsed: 0:03:19. with Loss: 1.0934888124465942 with total_loss: 4566.102805554867\n",
      "  Batch 1,443  of  1,613.    Elapsed: 0:03:19. with Loss: 1.5378037691116333 with total_loss: 4567.196294367313\n",
      "  Batch 1,444  of  1,613.    Elapsed: 0:03:19. with Loss: 1.2023882865905762 with total_loss: 4568.734098136425\n",
      "  Batch 1,445  of  1,613.    Elapsed: 0:03:20. with Loss: 1.772520899772644 with total_loss: 4569.936486423016\n",
      "  Batch 1,446  of  1,613.    Elapsed: 0:03:20. with Loss: 1.1449854373931885 with total_loss: 4571.709007322788\n",
      "  Batch 1,447  of  1,613.    Elapsed: 0:03:20. with Loss: 1.1365708112716675 with total_loss: 4572.853992760181\n",
      "  Batch 1,448  of  1,613.    Elapsed: 0:03:20. with Loss: 1.298902988433838 with total_loss: 4573.990563571453\n",
      "  Batch 1,449  of  1,613.    Elapsed: 0:03:20. with Loss: 1.1582598686218262 with total_loss: 4575.289466559887\n",
      "  Batch 1,450  of  1,613.    Elapsed: 0:03:20. with Loss: 1.0054630041122437 with total_loss: 4576.447726428509\n",
      "  Batch 1,451  of  1,613.    Elapsed: 0:03:20. with Loss: 0.7964977622032166 with total_loss: 4577.453189432621\n",
      "  Batch 1,452  of  1,613.    Elapsed: 0:03:21. with Loss: 1.153984546661377 with total_loss: 4578.249687194824\n",
      "  Batch 1,453  of  1,613.    Elapsed: 0:03:21. with Loss: 1.3892711400985718 with total_loss: 4579.403671741486\n",
      "  Batch 1,454  of  1,613.    Elapsed: 0:03:21. with Loss: 2.3073129653930664 with total_loss: 4580.792942881584\n",
      "  Batch 1,455  of  1,613.    Elapsed: 0:03:21. with Loss: 0.966105043888092 with total_loss: 4583.100255846977\n",
      "  Batch 1,456  of  1,613.    Elapsed: 0:03:21. with Loss: 1.7278352975845337 with total_loss: 4584.066360890865\n",
      "  Batch 1,457  of  1,613.    Elapsed: 0:03:21. with Loss: 1.6608344316482544 with total_loss: 4585.79419618845\n",
      "  Batch 1,458  of  1,613.    Elapsed: 0:03:21. with Loss: 1.7788231372833252 with total_loss: 4587.455030620098\n",
      "  Batch 1,459  of  1,613.    Elapsed: 0:03:21. with Loss: 1.4956690073013306 with total_loss: 4589.233853757381\n",
      "  Batch 1,460  of  1,613.    Elapsed: 0:03:22. with Loss: 0.975176990032196 with total_loss: 4590.729522764683\n",
      "  Batch 1,461  of  1,613.    Elapsed: 0:03:22. with Loss: 1.2802090644836426 with total_loss: 4591.704699754715\n",
      "  Batch 1,462  of  1,613.    Elapsed: 0:03:22. with Loss: 1.1702367067337036 with total_loss: 4592.984908819199\n",
      "  Batch 1,463  of  1,613.    Elapsed: 0:03:22. with Loss: 1.0478566884994507 with total_loss: 4594.155145525932\n",
      "  Batch 1,464  of  1,613.    Elapsed: 0:03:22. with Loss: 2.0762648582458496 with total_loss: 4595.203002214432\n",
      "  Batch 1,465  of  1,613.    Elapsed: 0:03:22. with Loss: 0.8591088652610779 with total_loss: 4597.279267072678\n",
      "  Batch 1,466  of  1,613.    Elapsed: 0:03:22. with Loss: 1.6716820001602173 with total_loss: 4598.138375937939\n",
      "  Batch 1,467  of  1,613.    Elapsed: 0:03:23. with Loss: 1.4770430326461792 with total_loss: 4599.810057938099\n",
      "  Batch 1,468  of  1,613.    Elapsed: 0:03:23. with Loss: 1.4727914333343506 with total_loss: 4601.287100970745\n",
      "  Batch 1,469  of  1,613.    Elapsed: 0:03:23. with Loss: 2.4931812286376953 with total_loss: 4602.759892404079\n",
      "  Batch 1,470  of  1,613.    Elapsed: 0:03:23. with Loss: 0.9908416271209717 with total_loss: 4605.253073632717\n",
      "  Batch 1,471  of  1,613.    Elapsed: 0:03:23. with Loss: 1.9366828203201294 with total_loss: 4606.243915259838\n",
      "  Batch 1,472  of  1,613.    Elapsed: 0:03:23. with Loss: 0.8537271618843079 with total_loss: 4608.180598080158\n",
      "  Batch 1,473  of  1,613.    Elapsed: 0:03:23. with Loss: 1.701360821723938 with total_loss: 4609.0343252420425\n",
      "  Batch 1,474  of  1,613.    Elapsed: 0:03:24. with Loss: 1.2873318195343018 with total_loss: 4610.7356860637665\n",
      "  Batch 1,475  of  1,613.    Elapsed: 0:03:24. with Loss: 1.357223391532898 with total_loss: 4612.023017883301\n",
      "  Batch 1,476  of  1,613.    Elapsed: 0:03:24. with Loss: 2.6051223278045654 with total_loss: 4613.380241274834\n",
      "  Batch 1,477  of  1,613.    Elapsed: 0:03:24. with Loss: 1.0574575662612915 with total_loss: 4615.985363602638\n",
      "  Batch 1,478  of  1,613.    Elapsed: 0:03:24. with Loss: 1.5227879285812378 with total_loss: 4617.0428211688995\n",
      "  Batch 1,479  of  1,613.    Elapsed: 0:03:24. with Loss: 1.1926809549331665 with total_loss: 4618.565609097481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,480  of  1,613.    Elapsed: 0:03:24. with Loss: 2.301257610321045 with total_loss: 4619.758290052414\n",
      "  Batch 1,481  of  1,613.    Elapsed: 0:03:25. with Loss: 0.9648369550704956 with total_loss: 4622.059547662735\n",
      "  Batch 1,482  of  1,613.    Elapsed: 0:03:25. with Loss: 1.4035813808441162 with total_loss: 4623.0243846178055\n",
      "  Batch 1,483  of  1,613.    Elapsed: 0:03:25. with Loss: 0.9719473123550415 with total_loss: 4624.42796599865\n",
      "  Batch 1,484  of  1,613.    Elapsed: 0:03:25. with Loss: 2.7788898944854736 with total_loss: 4625.399913311005\n",
      "  Batch 1,485  of  1,613.    Elapsed: 0:03:25. with Loss: 1.2290046215057373 with total_loss: 4628.17880320549\n",
      "  Batch 1,486  of  1,613.    Elapsed: 0:03:25. with Loss: 1.645119547843933 with total_loss: 4629.407807826996\n",
      "  Batch 1,487  of  1,613.    Elapsed: 0:03:25. with Loss: 1.0316216945648193 with total_loss: 4631.05292737484\n",
      "  Batch 1,488  of  1,613.    Elapsed: 0:03:25. with Loss: 0.5702798962593079 with total_loss: 4632.084549069405\n",
      "  Batch 1,489  of  1,613.    Elapsed: 0:03:26. with Loss: 2.6961941719055176 with total_loss: 4632.654828965664\n",
      "  Batch 1,490  of  1,613.    Elapsed: 0:03:26. with Loss: 1.718120813369751 with total_loss: 4635.351023137569\n",
      "  Batch 1,491  of  1,613.    Elapsed: 0:03:26. with Loss: 0.9498719573020935 with total_loss: 4637.069143950939\n",
      "  Batch 1,492  of  1,613.    Elapsed: 0:03:26. with Loss: 1.208733320236206 with total_loss: 4638.019015908241\n",
      "  Batch 1,493  of  1,613.    Elapsed: 0:03:26. with Loss: 1.4913415908813477 with total_loss: 4639.2277492284775\n",
      "  Batch 1,494  of  1,613.    Elapsed: 0:03:26. with Loss: 1.0311113595962524 with total_loss: 4640.719090819359\n",
      "  Batch 1,495  of  1,613.    Elapsed: 0:03:26. with Loss: 1.7034486532211304 with total_loss: 4641.750202178955\n",
      "  Batch 1,496  of  1,613.    Elapsed: 0:03:27. with Loss: 1.0105212926864624 with total_loss: 4643.453650832176\n",
      "  Batch 1,497  of  1,613.    Elapsed: 0:03:27. with Loss: 2.2844431400299072 with total_loss: 4644.464172124863\n",
      "  Batch 1,498  of  1,613.    Elapsed: 0:03:27. with Loss: 1.3339452743530273 with total_loss: 4646.748615264893\n",
      "  Batch 1,499  of  1,613.    Elapsed: 0:03:27. with Loss: 1.3148764371871948 with total_loss: 4648.082560539246\n",
      "  Batch 1,500  of  1,613.    Elapsed: 0:03:27. with Loss: 0.9496977925300598 with total_loss: 4649.397436976433\n",
      "  Batch 1,501  of  1,613.    Elapsed: 0:03:27. with Loss: 1.4406449794769287 with total_loss: 4650.347134768963\n",
      "  Batch 1,502  of  1,613.    Elapsed: 0:03:27. with Loss: 1.6530472040176392 with total_loss: 4651.78777974844\n",
      "  Batch 1,503  of  1,613.    Elapsed: 0:03:28. with Loss: 2.2723991870880127 with total_loss: 4653.440826952457\n",
      "  Batch 1,504  of  1,613.    Elapsed: 0:03:28. with Loss: 1.6033388376235962 with total_loss: 4655.713226139545\n",
      "  Batch 1,505  of  1,613.    Elapsed: 0:03:28. with Loss: 2.1225123405456543 with total_loss: 4657.316564977169\n",
      "  Batch 1,506  of  1,613.    Elapsed: 0:03:28. with Loss: 1.351413369178772 with total_loss: 4659.439077317715\n",
      "  Batch 1,507  of  1,613.    Elapsed: 0:03:28. with Loss: 1.977540135383606 with total_loss: 4660.7904906868935\n",
      "  Batch 1,508  of  1,613.    Elapsed: 0:03:28. with Loss: 0.8747807145118713 with total_loss: 4662.768030822277\n",
      "  Batch 1,509  of  1,613.    Elapsed: 0:03:28. with Loss: 1.3429266214370728 with total_loss: 4663.642811536789\n",
      "  Batch 1,510  of  1,613.    Elapsed: 0:03:29. with Loss: 3.910939931869507 with total_loss: 4664.985738158226\n",
      "  Batch 1,511  of  1,613.    Elapsed: 0:03:29. with Loss: 1.9011013507843018 with total_loss: 4668.8966780900955\n",
      "  Batch 1,512  of  1,613.    Elapsed: 0:03:29. with Loss: 1.931738257408142 with total_loss: 4670.79777944088\n",
      "  Batch 1,513  of  1,613.    Elapsed: 0:03:29. with Loss: 1.9954559803009033 with total_loss: 4672.729517698288\n",
      "  Batch 1,514  of  1,613.    Elapsed: 0:03:29. with Loss: 1.45760178565979 with total_loss: 4674.724973678589\n",
      "  Batch 1,515  of  1,613.    Elapsed: 0:03:29. with Loss: 1.4903146028518677 with total_loss: 4676.182575464249\n",
      "  Batch 1,516  of  1,613.    Elapsed: 0:03:29. with Loss: 1.0302460193634033 with total_loss: 4677.6728900671005\n",
      "  Batch 1,517  of  1,613.    Elapsed: 0:03:30. with Loss: 0.7311620712280273 with total_loss: 4678.703136086464\n",
      "  Batch 1,518  of  1,613.    Elapsed: 0:03:30. with Loss: 1.072278380393982 with total_loss: 4679.434298157692\n",
      "  Batch 1,519  of  1,613.    Elapsed: 0:03:30. with Loss: 1.0029315948486328 with total_loss: 4680.506576538086\n",
      "  Batch 1,520  of  1,613.    Elapsed: 0:03:30. with Loss: 0.7065224647521973 with total_loss: 4681.509508132935\n",
      "  Batch 1,521  of  1,613.    Elapsed: 0:03:30. with Loss: 1.5413923263549805 with total_loss: 4682.216030597687\n",
      "  Batch 1,522  of  1,613.    Elapsed: 0:03:30. with Loss: 1.445643424987793 with total_loss: 4683.757422924042\n",
      "  Batch 1,523  of  1,613.    Elapsed: 0:03:30. with Loss: 2.1005454063415527 with total_loss: 4685.2030663490295\n",
      "  Batch 1,524  of  1,613.    Elapsed: 0:03:30. with Loss: 1.5150434970855713 with total_loss: 4687.303611755371\n",
      "  Batch 1,525  of  1,613.    Elapsed: 0:03:31. with Loss: 0.9468621015548706 with total_loss: 4688.818655252457\n",
      "  Batch 1,526  of  1,613.    Elapsed: 0:03:31. with Loss: 1.3804666996002197 with total_loss: 4689.7655173540115\n",
      "  Batch 1,527  of  1,613.    Elapsed: 0:03:31. with Loss: 3.0851387977600098 with total_loss: 4691.145984053612\n",
      "  Batch 1,528  of  1,613.    Elapsed: 0:03:31. with Loss: 0.8848480582237244 with total_loss: 4694.231122851372\n",
      "  Batch 1,529  of  1,613.    Elapsed: 0:03:31. with Loss: 0.9644428491592407 with total_loss: 4695.1159709095955\n",
      "  Batch 1,530  of  1,613.    Elapsed: 0:03:31. with Loss: 1.2018488645553589 with total_loss: 4696.080413758755\n",
      "  Batch 1,531  of  1,613.    Elapsed: 0:03:31. with Loss: 1.0449345111846924 with total_loss: 4697.28226262331\n",
      "  Batch 1,532  of  1,613.    Elapsed: 0:03:32. with Loss: 1.210209608078003 with total_loss: 4698.327197134495\n",
      "  Batch 1,533  of  1,613.    Elapsed: 0:03:32. with Loss: 1.7919602394104004 with total_loss: 4699.537406742573\n",
      "  Batch 1,534  of  1,613.    Elapsed: 0:03:32. with Loss: 1.7360987663269043 with total_loss: 4701.329366981983\n",
      "  Batch 1,535  of  1,613.    Elapsed: 0:03:32. with Loss: 1.0456327199935913 with total_loss: 4703.06546574831\n",
      "  Batch 1,536  of  1,613.    Elapsed: 0:03:32. with Loss: 0.896405041217804 with total_loss: 4704.111098468304\n",
      "  Batch 1,537  of  1,613.    Elapsed: 0:03:32. with Loss: 1.127602458000183 with total_loss: 4705.0075035095215\n",
      "  Batch 1,538  of  1,613.    Elapsed: 0:03:32. with Loss: 1.794152855873108 with total_loss: 4706.135105967522\n",
      "  Batch 1,539  of  1,613.    Elapsed: 0:03:33. with Loss: 1.52567458152771 with total_loss: 4707.929258823395\n",
      "  Batch 1,540  of  1,613.    Elapsed: 0:03:33. with Loss: 1.649160385131836 with total_loss: 4709.4549334049225\n",
      "  Batch 1,541  of  1,613.    Elapsed: 0:03:33. with Loss: 0.92012619972229 with total_loss: 4711.104093790054\n",
      "  Batch 1,542  of  1,613.    Elapsed: 0:03:33. with Loss: 1.0976706743240356 with total_loss: 4712.024219989777\n",
      "  Batch 1,543  of  1,613.    Elapsed: 0:03:33. with Loss: 1.3185932636260986 with total_loss: 4713.121890664101\n",
      "  Batch 1,544  of  1,613.    Elapsed: 0:03:33. with Loss: 1.4118444919586182 with total_loss: 4714.440483927727\n",
      "  Batch 1,545  of  1,613.    Elapsed: 0:03:33. with Loss: 1.9071325063705444 with total_loss: 4715.852328419685\n",
      "  Batch 1,546  of  1,613.    Elapsed: 0:03:34. with Loss: 1.0704846382141113 with total_loss: 4717.759460926056\n",
      "  Batch 1,547  of  1,613.    Elapsed: 0:03:34. with Loss: 1.6240217685699463 with total_loss: 4718.82994556427\n",
      "  Batch 1,548  of  1,613.    Elapsed: 0:03:34. with Loss: 1.8037997484207153 with total_loss: 4720.45396733284\n",
      "  Batch 1,549  of  1,613.    Elapsed: 0:03:34. with Loss: 2.151259422302246 with total_loss: 4722.257767081261\n",
      "  Batch 1,550  of  1,613.    Elapsed: 0:03:34. with Loss: 1.516601324081421 with total_loss: 4724.409026503563\n",
      "  Batch 1,551  of  1,613.    Elapsed: 0:03:34. with Loss: 1.1120082139968872 with total_loss: 4725.925627827644\n",
      "  Batch 1,552  of  1,613.    Elapsed: 0:03:34. with Loss: 2.0904793739318848 with total_loss: 4727.037636041641\n",
      "  Batch 1,553  of  1,613.    Elapsed: 0:03:34. with Loss: 1.0623670816421509 with total_loss: 4729.128115415573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,554  of  1,613.    Elapsed: 0:03:35. with Loss: 1.6217607259750366 with total_loss: 4730.190482497215\n",
      "  Batch 1,555  of  1,613.    Elapsed: 0:03:35. with Loss: 2.4549014568328857 with total_loss: 4731.81224322319\n",
      "  Batch 1,556  of  1,613.    Elapsed: 0:03:35. with Loss: 1.2297760248184204 with total_loss: 4734.267144680023\n",
      "  Batch 1,557  of  1,613.    Elapsed: 0:03:35. with Loss: 1.3755370378494263 with total_loss: 4735.496920704842\n",
      "  Batch 1,558  of  1,613.    Elapsed: 0:03:35. with Loss: 0.8238126635551453 with total_loss: 4736.872457742691\n",
      "  Batch 1,559  of  1,613.    Elapsed: 0:03:35. with Loss: 2.3545212745666504 with total_loss: 4737.696270406246\n",
      "  Batch 1,560  of  1,613.    Elapsed: 0:03:35. with Loss: 0.878126859664917 with total_loss: 4740.050791680813\n",
      "  Batch 1,561  of  1,613.    Elapsed: 0:03:36. with Loss: 1.2982407808303833 with total_loss: 4740.928918540478\n",
      "  Batch 1,562  of  1,613.    Elapsed: 0:03:36. with Loss: 1.5520846843719482 with total_loss: 4742.227159321308\n",
      "  Batch 1,563  of  1,613.    Elapsed: 0:03:36. with Loss: 1.7285722494125366 with total_loss: 4743.77924400568\n",
      "  Batch 1,564  of  1,613.    Elapsed: 0:03:36. with Loss: 1.3430979251861572 with total_loss: 4745.507816255093\n",
      "  Batch 1,565  of  1,613.    Elapsed: 0:03:36. with Loss: 0.6352583765983582 with total_loss: 4746.850914180279\n",
      "  Batch 1,566  of  1,613.    Elapsed: 0:03:36. with Loss: 0.8985654711723328 with total_loss: 4747.486172556877\n",
      "  Batch 1,567  of  1,613.    Elapsed: 0:03:36. with Loss: 1.2428160905838013 with total_loss: 4748.3847380280495\n",
      "  Batch 1,568  of  1,613.    Elapsed: 0:03:37. with Loss: 2.157623529434204 with total_loss: 4749.627554118633\n",
      "  Batch 1,569  of  1,613.    Elapsed: 0:03:37. with Loss: 1.4911247491836548 with total_loss: 4751.7851776480675\n",
      "  Batch 1,570  of  1,613.    Elapsed: 0:03:37. with Loss: 1.2674189805984497 with total_loss: 4753.276302397251\n",
      "  Batch 1,571  of  1,613.    Elapsed: 0:03:37. with Loss: 1.4759893417358398 with total_loss: 4754.54372137785\n",
      "  Batch 1,572  of  1,613.    Elapsed: 0:03:37. with Loss: 1.692165732383728 with total_loss: 4756.019710719585\n",
      "  Batch 1,573  of  1,613.    Elapsed: 0:03:37. with Loss: 1.585839033126831 with total_loss: 4757.711876451969\n",
      "  Batch 1,574  of  1,613.    Elapsed: 0:03:37. with Loss: 3.25797963142395 with total_loss: 4759.297715485096\n",
      "  Batch 1,575  of  1,613.    Elapsed: 0:03:38. with Loss: 1.1089832782745361 with total_loss: 4762.55569511652\n",
      "  Batch 1,576  of  1,613.    Elapsed: 0:03:38. with Loss: 1.4090484380722046 with total_loss: 4763.6646783947945\n",
      "  Batch 1,577  of  1,613.    Elapsed: 0:03:38. with Loss: 0.7177808284759521 with total_loss: 4765.073726832867\n",
      "  Batch 1,578  of  1,613.    Elapsed: 0:03:38. with Loss: 1.3012398481369019 with total_loss: 4765.791507661343\n",
      "  Batch 1,579  of  1,613.    Elapsed: 0:03:38. with Loss: 1.4783151149749756 with total_loss: 4767.0927475094795\n",
      "  Batch 1,580  of  1,613.    Elapsed: 0:03:38. with Loss: 1.1801408529281616 with total_loss: 4768.5710626244545\n",
      "  Batch 1,581  of  1,613.    Elapsed: 0:03:38. with Loss: 0.7484981417655945 with total_loss: 4769.751203477383\n",
      "  Batch 1,582  of  1,613.    Elapsed: 0:03:39. with Loss: 2.640958309173584 with total_loss: 4770.499701619148\n",
      "  Batch 1,583  of  1,613.    Elapsed: 0:03:39. with Loss: 1.5961873531341553 with total_loss: 4773.140659928322\n",
      "  Batch 1,584  of  1,613.    Elapsed: 0:03:39. with Loss: 1.4627012014389038 with total_loss: 4774.736847281456\n",
      "  Batch 1,585  of  1,613.    Elapsed: 0:03:39. with Loss: 0.8578595519065857 with total_loss: 4776.199548482895\n",
      "  Batch 1,586  of  1,613.    Elapsed: 0:03:39. with Loss: 0.9551653861999512 with total_loss: 4777.0574080348015\n",
      "  Batch 1,587  of  1,613.    Elapsed: 0:03:39. with Loss: 1.3581253290176392 with total_loss: 4778.012573421001\n",
      "  Batch 1,588  of  1,613.    Elapsed: 0:03:39. with Loss: 1.9799612760543823 with total_loss: 4779.370698750019\n",
      "  Batch 1,589  of  1,613.    Elapsed: 0:03:39. with Loss: 2.029351234436035 with total_loss: 4781.3506600260735\n",
      "  Batch 1,590  of  1,613.    Elapsed: 0:03:40. with Loss: 1.038209319114685 with total_loss: 4783.3800112605095\n",
      "  Batch 1,591  of  1,613.    Elapsed: 0:03:40. with Loss: 1.407623052597046 with total_loss: 4784.418220579624\n",
      "  Batch 1,592  of  1,613.    Elapsed: 0:03:40. with Loss: 1.1007293462753296 with total_loss: 4785.825843632221\n",
      "  Batch 1,593  of  1,613.    Elapsed: 0:03:40. with Loss: 2.5103068351745605 with total_loss: 4786.926572978497\n",
      "  Batch 1,594  of  1,613.    Elapsed: 0:03:40. with Loss: 1.0480172634124756 with total_loss: 4789.436879813671\n",
      "  Batch 1,595  of  1,613.    Elapsed: 0:03:40. with Loss: 1.5898637771606445 with total_loss: 4790.484897077084\n",
      "  Batch 1,596  of  1,613.    Elapsed: 0:03:40. with Loss: 0.5978237390518188 with total_loss: 4792.074760854244\n",
      "  Batch 1,597  of  1,613.    Elapsed: 0:03:41. with Loss: 1.4294278621673584 with total_loss: 4792.672584593296\n",
      "  Batch 1,598  of  1,613.    Elapsed: 0:03:41. with Loss: 1.7112218141555786 with total_loss: 4794.102012455463\n",
      "  Batch 1,599  of  1,613.    Elapsed: 0:03:41. with Loss: 1.412158727645874 with total_loss: 4795.813234269619\n",
      "  Batch 1,600  of  1,613.    Elapsed: 0:03:41. with Loss: 1.8078566789627075 with total_loss: 4797.225392997265\n",
      "  Batch 1,601  of  1,613.    Elapsed: 0:03:41. with Loss: 1.6573208570480347 with total_loss: 4799.033249676228\n",
      "  Batch 1,602  of  1,613.    Elapsed: 0:03:41. with Loss: 2.2245140075683594 with total_loss: 4800.690570533276\n",
      "  Batch 1,603  of  1,613.    Elapsed: 0:03:41. with Loss: 1.3150689601898193 with total_loss: 4802.915084540844\n",
      "  Batch 1,604  of  1,613.    Elapsed: 0:03:42. with Loss: 0.9395924210548401 with total_loss: 4804.230153501034\n",
      "  Batch 1,605  of  1,613.    Elapsed: 0:03:42. with Loss: 0.9419031739234924 with total_loss: 4805.169745922089\n",
      "  Batch 1,606  of  1,613.    Elapsed: 0:03:42. with Loss: 1.78880774974823 with total_loss: 4806.111649096012\n",
      "  Batch 1,607  of  1,613.    Elapsed: 0:03:42. with Loss: 1.3616256713867188 with total_loss: 4807.90045684576\n",
      "  Batch 1,608  of  1,613.    Elapsed: 0:03:42. with Loss: 1.3187143802642822 with total_loss: 4809.262082517147\n",
      "  Batch 1,609  of  1,613.    Elapsed: 0:03:42. with Loss: 1.1649491786956787 with total_loss: 4810.580796897411\n",
      "  Batch 1,610  of  1,613.    Elapsed: 0:03:42. with Loss: 1.178405523300171 with total_loss: 4811.745746076107\n",
      "  Batch 1,611  of  1,613.    Elapsed: 0:03:43. with Loss: 1.4288668632507324 with total_loss: 4812.924151599407\n",
      "  Batch 1,612  of  1,613.    Elapsed: 0:03:43. with Loss: 0.5194501280784607 with total_loss: 4814.353018462658\n",
      "\n",
      "  Average training loss: 2.99\n",
      "  Training epoch took: 0:03:43\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "# model.to(device)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    " \n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "\n",
    "    \n",
    "        loss = outputs[0]\n",
    "        if step%1==0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}. with Loss: {:} with total_loss: {:}'.format(step, len(train_dataloader), elapsed,loss.item(),total_loss))\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "    \n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "#         \n",
    "        # Report progress.\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)  \n",
    "\n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "training_loss.append(avg_train_loss)\n",
    "training_time.append(elapsed) \n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Validation...\n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4688, 1.4706, 1.4689,  ..., 1.4835, 1.4835, 1.4835], device='cuda:0')\n",
      "tensor([3, 2, 2,  ..., 1, 2, 1], device='cuda:0')\n",
      "  Batch     0  of    180. with Loss: 1.663072943687439 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4587, 1.4587, 1.4587,  ..., 1.4688, 1.4688, 1.4688], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch     1  of    180. with Loss: 1.7168136835098267 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4827, 1.4804, 1.4804,  ..., 1.4927, 1.4927, 1.4927], device='cuda:0')\n",
      "tensor([6, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch     2  of    180. with Loss: 1.2133539915084839 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4725, 1.4722, 1.4722,  ..., 1.4795, 1.4795, 1.4795], device='cuda:0')\n",
      "tensor([2, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch     3  of    180. with Loss: 1.1280325651168823 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4904, 1.4904, 1.4904,  ..., 1.4742, 1.4742, 1.4742], device='cuda:0')\n",
      "tensor([4, 4, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch     4  of    180. with Loss: 1.3388111591339111 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4792, 1.4792, 1.4792,  ..., 1.4465, 1.4742, 1.4771], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch     5  of    180. with Loss: 1.597593069076538 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4813, 1.4813, 1.4813,  ..., 1.4705, 1.4705, 1.4701], device='cuda:0')\n",
      "tensor([1, 1, 3,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch     6  of    180. with Loss: 1.6713117361068726 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4800, 1.4800, 1.4800,  ..., 1.4836, 1.4836, 1.4836], device='cuda:0')\n",
      "tensor([1, 2, 4,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch     7  of    180. with Loss: 1.526261329650879 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4755, 1.4755, 1.4755,  ..., 1.4556, 1.4556, 1.4556], device='cuda:0')\n",
      "tensor([1, 1, 3,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch     8  of    180. with Loss: 0.9239985942840576 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4921, 1.4958, 1.4934,  ..., 1.4687, 1.4687, 1.4821], device='cuda:0')\n",
      "tensor([4, 2, 2,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch     9  of    180. with Loss: 1.5664279460906982 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4571, 1.4571, 1.4571,  ..., 1.4921, 1.4921, 1.4921], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 1, 2, 1], device='cuda:0')\n",
      "  Batch    10  of    180. with Loss: 1.5546143054962158 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4905, 1.4905, 1.4858,  ..., 1.4809, 1.4809, 1.4810], device='cuda:0')\n",
      "tensor([2, 2, 2,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    11  of    180. with Loss: 1.171096682548523 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4796, 1.4796, 1.4796,  ..., 1.4608, 1.4608, 1.4608], device='cuda:0')\n",
      "tensor([1, 1, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    12  of    180. with Loss: 1.7231510877609253 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4831, 1.4831, 1.4831,  ..., 1.4521, 1.4521, 1.4521], device='cuda:0')\n",
      "tensor([1, 2, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    13  of    180. with Loss: 1.2564165592193604 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4855, 1.4855, 1.4855,  ..., 1.4890, 1.4885, 1.4890], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    14  of    180. with Loss: 1.1695141792297363 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4637, 1.4637, 1.4637,  ..., 1.4797, 1.4798, 1.4798], device='cuda:0')\n",
      "tensor([1, 3, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    15  of    180. with Loss: 1.6072739362716675 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4726, 1.4726, 1.4726,  ..., 1.5088, 1.5088, 1.5088], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    16  of    180. with Loss: 0.7074615955352783 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4864, 1.4864, 1.4864,  ..., 1.5007, 1.5017, 1.5007], device='cuda:0')\n",
      "tensor([2, 1, 1,  ..., 1, 1, 2], device='cuda:0')\n",
      "  Batch    17  of    180. with Loss: 2.4723610877990723 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4824, 1.4824, 1.4824,  ..., 1.4708, 1.4708, 1.4708], device='cuda:0')\n",
      "tensor([2, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    18  of    180. with Loss: 1.8609483242034912 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4825, 1.4826, 1.4826,  ..., 1.4660, 1.4660, 1.4660], device='cuda:0')\n",
      "tensor([2, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    19  of    180. with Loss: 1.0780609846115112 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4793, 1.4793, 1.4793,  ..., 1.4498, 1.4498, 1.4499], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    20  of    180. with Loss: 1.3223036527633667 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4854, 1.4854, 1.4854,  ..., 1.4678, 1.4678, 1.4678], device='cuda:0')\n",
      "tensor([1, 1, 2,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    21  of    180. with Loss: 1.7570866346359253 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4621, 1.4621, 1.4621,  ..., 1.4859, 1.4852, 1.4852], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    22  of    180. with Loss: 1.4751392602920532 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4772, 1.4755, 1.4755,  ..., 1.4688, 1.4688, 1.4688], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    23  of    180. with Loss: 0.9761620163917542 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4891, 1.4895, 1.4891,  ..., 1.4791, 1.4791, 1.4791], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    24  of    180. with Loss: 1.2991148233413696 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4812, 1.4812, 1.4812,  ..., 1.4794, 1.4794, 1.4794], device='cuda:0')\n",
      "tensor([3, 1, 3,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    25  of    180. with Loss: 1.1575251817703247 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4668, 1.4680, 1.4684,  ..., 1.4636, 1.4636, 1.4649], device='cuda:0')\n",
      "tensor([2, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    26  of    180. with Loss: 1.5511616468429565 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.5000, 1.5000, 1.5000,  ..., 1.4889, 1.4889, 1.4889], device='cuda:0')\n",
      "tensor([6, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    27  of    180. with Loss: 1.4946542978286743 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4617, 1.4617, 1.4617,  ..., 1.4670, 1.4670, 1.4656], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    28  of    180. with Loss: 0.8449299931526184 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4879, 1.4874, 1.4879,  ..., 1.4523, 1.4518, 1.4523], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    29  of    180. with Loss: 1.664368987083435 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4817, 1.4817, 1.4817,  ..., 1.4938, 1.4938, 1.4938], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 1, 2, 2], device='cuda:0')\n",
      "  Batch    30  of    180. with Loss: 1.6078054904937744 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4787, 1.4787, 1.4727,  ..., 1.4566, 1.4566, 1.4566], device='cuda:0')\n",
      "tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    31  of    180. with Loss: 1.2256927490234375 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4609, 1.4609, 1.4609,  ..., 1.4870, 1.4870, 1.4870], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    32  of    180. with Loss: 0.7016307711601257 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4927, 1.4927, 1.4962,  ..., 1.4907, 1.4907, 1.4907], device='cuda:0')\n",
      "tensor([2, 3, 2,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    33  of    180. with Loss: 1.143733024597168 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4773, 1.4801, 1.4773,  ..., 1.4634, 1.4634, 1.4634], device='cuda:0')\n",
      "tensor([1, 1, 2,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    34  of    180. with Loss: 1.0299913883209229 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.5031, 1.5031, 1.5031,  ..., 1.4887, 1.4887, 1.4741], device='cuda:0')\n",
      "tensor([7, 4, 3,  ..., 1, 1, 0], device='cuda:0')\n",
      "  Batch    35  of    180. with Loss: 0.89718097448349 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4849, 1.4850, 1.4850,  ..., 1.4713, 1.4713, 1.4714], device='cuda:0')\n",
      "tensor([3, 1, 7,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    36  of    180. with Loss: 1.0689172744750977 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4734, 1.4783, 1.4735,  ..., 1.4746, 1.4746, 1.4747], device='cuda:0')\n",
      "tensor([1, 3, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    37  of    180. with Loss: 1.5577328205108643 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4544, 1.4544, 1.4544,  ..., 1.4926, 1.4926, 1.4926], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 1, 1, 4], device='cuda:0')\n",
      "  Batch    38  of    180. with Loss: 1.5357526540756226 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.5111, 1.5111, 1.5138,  ..., 1.4864, 1.4864, 1.4863], device='cuda:0')\n",
      "tensor([4, 5, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    39  of    180. with Loss: 1.7487342357635498 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4740, 1.4735, 1.4735,  ..., 1.4404, 1.4404, 1.4404], device='cuda:0')\n",
      "tensor([3, 1, 2,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    40  of    180. with Loss: 1.1452873945236206 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4767, 1.4832, 1.4729,  ..., 1.4874, 1.4874, 1.4880], device='cuda:0')\n",
      "tensor([7, 3, 1,  ..., 2, 1, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    41  of    180. with Loss: 0.9535590410232544 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4542, 1.4542, 1.4542,  ..., 1.4630, 1.4630, 1.4630], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    42  of    180. with Loss: 1.2852098941802979 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4868, 1.4868, 1.4868,  ..., 1.4663, 1.4634, 1.4665], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    43  of    180. with Loss: 1.899733066558838 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4890, 1.4889, 1.4889,  ..., 1.4503, 1.4503, 1.4503], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    44  of    180. with Loss: 1.2564078569412231 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4829, 1.4826, 1.4827,  ..., 1.4873, 1.4873, 1.4849], device='cuda:0')\n",
      "tensor([3, 2, 1,  ..., 1, 1, 1], device='cuda:0')\n",
      "  Batch    45  of    180. with Loss: 1.5188480615615845 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4643, 1.4643, 1.4643,  ..., 1.4866, 1.4861, 1.4866], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    46  of    180. with Loss: 2.011375665664673 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4925, 1.4932, 1.4925,  ..., 1.4885, 1.4906, 1.4885], device='cuda:0')\n",
      "tensor([2, 1, 1,  ..., 1, 2, 1], device='cuda:0')\n",
      "  Batch    47  of    180. with Loss: 2.3377325534820557 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4717, 1.4717, 1.4717,  ..., 1.4855, 1.4855, 1.4855], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    48  of    180. with Loss: 1.8671869039535522 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4800, 1.4848, 1.4800,  ..., 1.4766, 1.4766, 1.4766], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    49  of    180. with Loss: 0.8833388090133667 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4547, 1.4547, 1.4547,  ..., 1.4937, 1.4937, 1.5032], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    50  of    180. with Loss: 0.9322293996810913 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4506, 1.4506, 1.4506,  ..., 1.4830, 1.4830, 1.4830], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    51  of    180. with Loss: 0.8009798526763916 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4692, 1.4692, 1.4692,  ..., 1.4890, 1.4890, 1.4889], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    52  of    180. with Loss: 1.8552712202072144 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4607, 1.4607, 1.4607,  ..., 1.4598, 1.4598, 1.4598], device='cuda:0')\n",
      "tensor([2, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    53  of    180. with Loss: 1.098013162612915 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4849, 1.4884, 1.4967,  ..., 1.5041, 1.5043, 1.5060], device='cuda:0')\n",
      "tensor([1, 4, 2,  ..., 1, 1, 1], device='cuda:0')\n",
      "  Batch    54  of    180. with Loss: 1.601540207862854 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4634, 1.4630, 1.4656,  ..., 1.4581, 1.4581, 1.4581], device='cuda:0')\n",
      "tensor([2, 2, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    55  of    180. with Loss: 1.6723647117614746 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4801, 1.4801, 1.4802,  ..., 1.5215, 1.5215, 1.5215], device='cuda:0')\n",
      "tensor([1, 2, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    56  of    180. with Loss: 1.3597540855407715 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.5037, 1.4995, 1.5037,  ..., 1.4858, 1.4858, 1.4858], device='cuda:0')\n",
      "tensor([3, 1, 4,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    57  of    180. with Loss: 0.7346790432929993 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4660, 1.4660, 1.4660,  ..., 1.4607, 1.4607, 1.4607], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    58  of    180. with Loss: 1.6667371988296509 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4811, 1.4873, 1.4873,  ..., 1.4766, 1.4766, 1.4766], device='cuda:0')\n",
      "tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    59  of    180. with Loss: 2.6587698459625244 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4618, 1.4618, 1.4618,  ..., 1.4864, 1.4864, 1.4864], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    60  of    180. with Loss: 1.6090483665466309 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4691, 1.4691, 1.4691,  ..., 1.4893, 1.4893, 1.4893], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 1, 1, 1], device='cuda:0')\n",
      "  Batch    61  of    180. with Loss: 1.4290192127227783 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4609, 1.4609, 1.4609,  ..., 1.4714, 1.4714, 1.4714], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    62  of    180. with Loss: 0.9515581130981445 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4897, 1.4882, 1.4897,  ..., 1.4630, 1.4630, 1.4630], device='cuda:0')\n",
      "tensor([2, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    63  of    180. with Loss: 3.0231218338012695 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4845, 1.4845, 1.4845,  ..., 1.4814, 1.4814, 1.4814], device='cuda:0')\n",
      "tensor([3, 1, 2,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    64  of    180. with Loss: 1.3774139881134033 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4740, 1.4740, 1.4740,  ..., 1.4808, 1.4808, 1.4808], device='cuda:0')\n",
      "tensor([1, 1, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    65  of    180. with Loss: 2.270284652709961 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4874, 1.4896, 1.4874,  ..., 1.4858, 1.4858, 1.4858], device='cuda:0')\n",
      "tensor([2, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    66  of    180. with Loss: 2.89985990524292 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4688, 1.4689, 1.4689,  ..., 1.4754, 1.4740, 1.4740], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    67  of    180. with Loss: 1.0212571620941162 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4538, 1.4538, 1.4538,  ..., 1.4751, 1.4751, 1.4751], device='cuda:0')\n",
      "tensor([2, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    68  of    180. with Loss: 1.7049827575683594 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4917, 1.4969, 1.4903,  ..., 1.4822, 1.4822, 1.4822], device='cuda:0')\n",
      "tensor([1, 1, 4,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    69  of    180. with Loss: 1.2019908428192139 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4708, 1.4712, 1.4708,  ..., 1.4829, 1.4829, 1.4829], device='cuda:0')\n",
      "tensor([1, 1, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    70  of    180. with Loss: 1.603722333908081 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4897, 1.4866, 1.4872,  ..., 1.4798, 1.4798, 1.4798], device='cuda:0')\n",
      "tensor([1, 1, 4,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    71  of    180. with Loss: 2.3826677799224854 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4798, 1.4810, 1.4810,  ..., 1.4854, 1.4836, 1.4854], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    72  of    180. with Loss: 1.6473833322525024 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4609, 1.4609, 1.4609,  ..., 1.4690, 1.4690, 1.4690], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    73  of    180. with Loss: 1.2714123725891113 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4778, 1.4778, 1.4790,  ..., 1.4708, 1.4708, 1.4708], device='cuda:0')\n",
      "tensor([3, 2, 2,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    74  of    180. with Loss: 1.406833291053772 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4768, 1.4768, 1.4768,  ..., 1.4703, 1.4704, 1.4703], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    75  of    180. with Loss: 1.2897666692733765 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4889, 1.4902, 1.4902,  ..., 1.4765, 1.4801, 1.4766], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 1, 2, 1], device='cuda:0')\n",
      "  Batch    76  of    180. with Loss: 0.685886800289154 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4793, 1.4793, 1.4793,  ..., 1.4701, 1.4701, 1.4701], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    77  of    180. with Loss: 1.0694165229797363 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4867, 1.4867, 1.4867,  ..., 1.4592, 1.4592, 1.4592], device='cuda:0')\n",
      "tensor([2, 1, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    78  of    180. with Loss: 0.737285852432251 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4911, 1.4942, 1.4911,  ..., 1.4645, 1.4645, 1.4645], device='cuda:0')\n",
      "tensor([1, 1, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    79  of    180. with Loss: 1.7459098100662231 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4809, 1.4809, 1.4809,  ..., 1.4847, 1.4847, 1.4847], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    80  of    180. with Loss: 2.4106602668762207 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4703, 1.4703, 1.4703,  ..., 1.4757, 1.4757, 1.4757], device='cuda:0')\n",
      "tensor([2, 1, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    81  of    180. with Loss: 1.6209535598754883 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4626, 1.4626, 1.4626,  ..., 1.4880, 1.4880, 1.4880], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    82  of    180. with Loss: 7.572835922241211 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4510, 1.4510, 1.4510,  ..., 1.4651, 1.4651, 1.4651], device='cuda:0')\n",
      "tensor([2, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    83  of    180. with Loss: 1.5820116996765137 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4660, 1.4660, 1.4660,  ..., 1.4809, 1.4809, 1.4809], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    84  of    180. with Loss: 1.4843777418136597 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4913, 1.4913, 1.4913,  ..., 1.4817, 1.4817, 1.4816], device='cuda:0')\n",
      "tensor([ 1,  1, 14,  ...,  2,  1,  1], device='cuda:0')\n",
      "  Batch    85  of    180. with Loss: 2.0134780406951904 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4850, 1.4866, 1.4852,  ..., 1.4778, 1.4778, 1.4795], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    86  of    180. with Loss: 1.4587604999542236 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4875, 1.4875, 1.4875,  ..., 1.4836, 1.4836, 1.4836], device='cuda:0')\n",
      "tensor([2, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    87  of    180. with Loss: 0.9145861268043518 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4823, 1.4872, 1.4824,  ..., 1.4865, 1.4865, 1.4865], device='cuda:0')\n",
      "tensor([1, 2, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    88  of    180. with Loss: 1.6178548336029053 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4643, 1.4643, 1.4643,  ..., 1.4876, 1.4876, 1.4876], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 1, 1, 1], device='cuda:0')\n",
      "  Batch    89  of    180. with Loss: 1.5265491008758545 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4844, 1.4843, 1.4844,  ..., 1.4703, 1.4703, 1.4703], device='cuda:0')\n",
      "tensor([1, 2, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    90  of    180. with Loss: 1.318411111831665 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4857, 1.4857, 1.4857,  ..., 1.4795, 1.4806, 1.4795], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    91  of    180. with Loss: 0.98360675573349 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4568, 1.4568, 1.4568,  ..., 1.4772, 1.4772, 1.4777], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 1, 1, 1], device='cuda:0')\n",
      "  Batch    92  of    180. with Loss: 3.9350743293762207 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4657, 1.4657, 1.4657,  ..., 1.4702, 1.4702, 1.4702], device='cuda:0')\n",
      "tensor([1, 2, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    93  of    180. with Loss: 1.3130395412445068 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4657, 1.4657, 1.4657,  ..., 1.4831, 1.4833, 1.4831], device='cuda:0')\n",
      "tensor([1, 1, 0,  ..., 1, 1, 1], device='cuda:0')\n",
      "  Batch    94  of    180. with Loss: 1.0764942169189453 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4917, 1.4917, 1.4917,  ..., 1.5115, 1.5115, 1.5113], device='cuda:0')\n",
      "tensor([6, 2, 2,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    95  of    180. with Loss: 1.1874862909317017 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4657, 1.4657, 1.4657,  ..., 1.4525, 1.4525, 1.4525], device='cuda:0')\n",
      "tensor([1, 2, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    96  of    180. with Loss: 1.1903088092803955 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4764, 1.4764, 1.4764,  ..., 1.4768, 1.4751, 1.4768], device='cuda:0')\n",
      "tensor([6, 4, 2,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    97  of    180. with Loss: 1.377462387084961 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4658, 1.4670, 1.4670,  ..., 1.4628, 1.4628, 1.4628], device='cuda:0')\n",
      "tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    98  of    180. with Loss: 1.3865342140197754 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4880, 1.4880, 1.4880,  ..., 1.4609, 1.4609, 1.4609], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch    99  of    180. with Loss: 1.7150553464889526 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4850, 1.4850, 1.4850,  ..., 1.4742, 1.4742, 1.4742], device='cuda:0')\n",
      "tensor([2, 1, 3,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   100  of    180. with Loss: 0.8995546698570251 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4524, 1.4524, 1.4524,  ..., 1.4862, 1.4863, 1.4863], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   101  of    180. with Loss: 1.1156216859817505 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4754, 1.4754, 1.4754,  ..., 1.4770, 1.4770, 1.4770], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   102  of    180. with Loss: 2.3807590007781982 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4651, 1.4652, 1.4652,  ..., 1.4839, 1.4785, 1.4871], device='cuda:0')\n",
      "tensor([1, 1, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   103  of    180. with Loss: 1.1110515594482422 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4546, 1.4546, 1.4546,  ..., 1.4871, 1.4872, 1.4872], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   104  of    180. with Loss: 3.082810878753662 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4619, 1.4619, 1.4619,  ..., 1.4876, 1.4876, 1.5055], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   105  of    180. with Loss: 1.2952232360839844 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4975, 1.4978, 1.4978,  ..., 1.4621, 1.4621, 1.4621], device='cuda:0')\n",
      "tensor([2, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   106  of    180. with Loss: 1.6665427684783936 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4862, 1.4868, 1.4868,  ..., 1.4729, 1.4729, 1.4729], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   107  of    180. with Loss: 1.9263770580291748 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4542, 1.4542, 1.4542,  ..., 1.4763, 1.4764, 1.4764], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   108  of    180. with Loss: 1.4440159797668457 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4808, 1.4793, 1.4808,  ..., 1.4882, 1.4882, 1.4882], device='cuda:0')\n",
      "tensor([2, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   109  of    180. with Loss: 4.3557000160217285 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4664, 1.4664, 1.4664,  ..., 1.4927, 1.4927, 1.4927], device='cuda:0')\n",
      "tensor([1, 1, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   110  of    180. with Loss: 7.3426079750061035 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4901, 1.4901, 1.4901,  ..., 1.4832, 1.4832, 1.4832], device='cuda:0')\n",
      "tensor([1, 2, 2,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   111  of    180. with Loss: 2.271347761154175 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4671, 1.4671, 1.4671,  ..., 1.4834, 1.4834, 1.4834], device='cuda:0')\n",
      "tensor([3, 1, 2,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   112  of    180. with Loss: 2.890549421310425 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4830, 1.4830, 1.4829,  ..., 1.4579, 1.4579, 1.4579], device='cuda:0')\n",
      "tensor([2, 1, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   113  of    180. with Loss: 1.2773317098617554 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4831, 1.4831, 1.4831,  ..., 1.4872, 1.4872, 1.4883], device='cuda:0')\n",
      "tensor([2, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   114  of    180. with Loss: 1.4574320316314697 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4657, 1.4657, 1.4657,  ..., 1.4728, 1.4705, 1.4704], device='cuda:0')\n",
      "tensor([1, 2, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   115  of    180. with Loss: 1.6169086694717407 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4539, 1.4551, 1.4551,  ..., 1.4604, 1.4604, 1.4614], device='cuda:0')\n",
      "tensor([1, 1, 2,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   116  of    180. with Loss: 1.0161069631576538 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4572, 1.4572, 1.4572,  ..., 1.4766, 1.4749, 1.4773], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   117  of    180. with Loss: 2.369866132736206 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4732, 1.4732, 1.4732,  ..., 1.4767, 1.4767, 1.4767], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 1, 1, 1], device='cuda:0')\n",
      "  Batch   118  of    180. with Loss: 1.2722686529159546 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4821, 1.4816, 1.4821,  ..., 1.4829, 1.4829, 1.4829], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   119  of    180. with Loss: 2.1138010025024414 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4633, 1.4633, 1.4633,  ..., 1.4609, 1.4609, 1.4609], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   120  of    180. with Loss: 1.180250883102417 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4774, 1.4774, 1.4774,  ..., 1.4977, 1.4975, 1.4976], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   121  of    180. with Loss: 0.882958173751831 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4813, 1.4813, 1.4817,  ..., 1.4873, 1.4887, 1.4887], device='cuda:0')\n",
      "tensor([2, 1, 3,  ..., 2, 2, 1], device='cuda:0')\n",
      "  Batch   122  of    180. with Loss: 0.8237053751945496 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4625, 1.4625, 1.4625,  ..., 1.4835, 1.4835, 1.4835], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   123  of    180. with Loss: 1.0892668962478638 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4884, 1.4884, 1.4884,  ..., 1.4951, 1.4950, 1.4951], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   124  of    180. with Loss: 1.1187596321105957 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4866, 1.4884, 1.4884,  ..., 1.4883, 1.4883, 1.4883], device='cuda:0')\n",
      "tensor([2, 1, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   125  of    180. with Loss: 1.4361237287521362 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4837, 1.4837, 1.4834,  ..., 1.4618, 1.4618, 1.4618], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   126  of    180. with Loss: 1.6238758563995361 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4875, 1.4879, 1.4875,  ..., 1.4908, 1.4908, 1.4871], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   127  of    180. with Loss: 1.3223799467086792 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4809, 1.5017, 1.4809,  ..., 1.4716, 1.4717, 1.4732], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   128  of    180. with Loss: 1.5871614217758179 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4788, 1.4851, 1.4788,  ..., 1.4617, 1.4617, 1.4617], device='cuda:0')\n",
      "tensor([1, 2, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   129  of    180. with Loss: 1.0453269481658936 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4907, 1.4894, 1.4893,  ..., 1.4716, 1.4716, 1.4728], device='cuda:0')\n",
      "tensor([10,  1,  1,  ...,  0,  0,  0], device='cuda:0')\n",
      "  Batch   130  of    180. with Loss: 1.621017575263977 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4933, 1.4933, 1.4933,  ..., 1.4787, 1.4787, 1.4788], device='cuda:0')\n",
      "tensor([2, 2, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   131  of    180. with Loss: 2.3065576553344727 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4574, 1.4574, 1.4574,  ..., 1.4807, 1.4807, 1.4807], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   132  of    180. with Loss: 0.8158519268035889 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4666, 1.4667, 1.4666,  ..., 1.4761, 1.4761, 1.4761], device='cuda:0')\n",
      "tensor([2, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   133  of    180. with Loss: 1.669421672821045 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.5022, 1.5061, 1.5037,  ..., 1.4861, 1.4862, 1.4829], device='cuda:0')\n",
      "tensor([9, 1, 2,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   134  of    180. with Loss: 1.4214507341384888 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4613, 1.4948, 1.4613,  ..., 1.4676, 1.4676, 1.4676], device='cuda:0')\n",
      "tensor([16,  1,  7,  ...,  0,  0,  0], device='cuda:0')\n",
      "  Batch   135  of    180. with Loss: 2.247711658477783 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4579, 1.4579, 1.4579,  ..., 1.4565, 1.4565, 1.4565], device='cuda:0')\n",
      "tensor([3, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   136  of    180. with Loss: 2.1515183448791504 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4713, 1.4713, 1.4713,  ..., 1.5569, 1.5569, 1.5569], device='cuda:0')\n",
      "tensor([2, 1, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   137  of    180. with Loss: 2.008599042892456 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4594, 1.4594, 1.4594,  ..., 1.4597, 1.4597, 1.4597], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   138  of    180. with Loss: 1.06656813621521 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4774, 1.4774, 1.4774,  ..., 1.4687, 1.4687, 1.4687], device='cuda:0')\n",
      "tensor([1, 1, 2,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   139  of    180. with Loss: 0.7811875343322754 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4609, 1.4609, 1.4609,  ..., 1.4913, 1.4926, 1.4903], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 1, 1, 1], device='cuda:0')\n",
      "  Batch   140  of    180. with Loss: 1.1433751583099365 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4786, 1.4828, 1.4730,  ..., 1.4760, 1.4780, 1.4780], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   141  of    180. with Loss: 1.7399524450302124 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4660, 1.5252, 1.4660,  ..., 1.4773, 1.4773, 1.4773], device='cuda:0')\n",
      "tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   142  of    180. with Loss: 1.8602864742279053 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4881, 1.4881, 1.4881,  ..., 1.4763, 1.4783, 1.4763], device='cuda:0')\n",
      "tensor([1, 4, 1,  ..., 1, 1, 2], device='cuda:0')\n",
      "  Batch   143  of    180. with Loss: 1.2556437253952026 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4564, 1.4564, 1.4564,  ..., 1.4447, 1.4447, 1.4447], device='cuda:0')\n",
      "tensor([2, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   144  of    180. with Loss: 1.1139615774154663 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4587, 1.4605, 1.4587,  ..., 1.4755, 1.4755, 1.4769], device='cuda:0')\n",
      "tensor([2, 1, 2,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   145  of    180. with Loss: 2.2155182361602783 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4612, 1.4612, 1.4612,  ..., 1.5617, 1.5617, 1.5617], device='cuda:0')\n",
      "tensor([1, 1, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   146  of    180. with Loss: 1.086574673652649 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4744, 1.4744, 1.4744,  ..., 1.4803, 1.4831, 1.4803], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 2, 1, 1], device='cuda:0')\n",
      "  Batch   147  of    180. with Loss: 0.8240506052970886 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4629, 1.4547, 1.4547,  ..., 1.4894, 1.4976, 1.4968], device='cuda:0')\n",
      "tensor([1, 2, 0,  ..., 2, 1, 1], device='cuda:0')\n",
      "  Batch   148  of    180. with Loss: 1.2228366136550903 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4735, 1.4735, 1.4735,  ..., 1.4790, 1.4790, 1.4790], device='cuda:0')\n",
      "tensor([1, 1, 0,  ..., 1, 1, 1], device='cuda:0')\n",
      "  Batch   149  of    180. with Loss: 1.1653034687042236 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.5177, 1.5177, 1.5177,  ..., 1.4948, 1.4948, 1.4948], device='cuda:0')\n",
      "tensor([2, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   150  of    180. with Loss: 0.9646123647689819 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4641, 1.4641, 1.4641,  ..., 1.4622, 1.4622, 1.4622], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   151  of    180. with Loss: 1.6948974132537842 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4503, 1.4503, 1.4503,  ..., 1.4618, 1.4618, 1.4618], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   152  of    180. with Loss: 1.0064828395843506 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4681, 1.4681, 1.4681,  ..., 1.4735, 1.4735, 1.4735], device='cuda:0')\n",
      "tensor([1, 1, 2,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   153  of    180. with Loss: 3.053560972213745 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4746, 1.4766, 1.4766,  ..., 1.4892, 1.4892, 1.4892], device='cuda:0')\n",
      "tensor([1, 2, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   154  of    180. with Loss: 1.2504067420959473 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4743, 1.4743, 1.4743,  ..., 1.4784, 1.4798, 1.4784], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 3, 4, 1], device='cuda:0')\n",
      "  Batch   155  of    180. with Loss: 0.7945988178253174 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4885, 1.4892, 1.4916,  ..., 1.4809, 1.4809, 1.4804], device='cuda:0')\n",
      "tensor([7, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   156  of    180. with Loss: 1.6792956590652466 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4880, 1.4894, 1.4894,  ..., 1.4814, 1.4814, 1.4814], device='cuda:0')\n",
      "tensor([2, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   157  of    180. with Loss: 2.0556094646453857 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4629, 1.4547, 1.4547,  ..., 1.4841, 1.4841, 1.4841], device='cuda:0')\n",
      "tensor([1, 1, 0,  ..., 1, 1, 1], device='cuda:0')\n",
      "  Batch   158  of    180. with Loss: 2.6559531688690186 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4906, 1.4906, 1.4906,  ..., 1.4941, 1.4941, 1.4941], device='cuda:0')\n",
      "tensor([4, 5, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   159  of    180. with Loss: 1.3570103645324707 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4570, 1.4570, 1.4570,  ..., 1.4856, 1.4857, 1.4856], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   160  of    180. with Loss: 1.7172954082489014 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4601, 1.4601, 1.4601,  ..., 1.4827, 1.4866, 1.4726], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 1, 1, 1], device='cuda:0')\n",
      "  Batch   161  of    180. with Loss: 1.0489270687103271 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4802, 1.4833, 1.4802,  ..., 1.4727, 1.4727, 1.4727], device='cuda:0')\n",
      "tensor([1, 2, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   162  of    180. with Loss: 3.2705349922180176 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4546, 1.4546, 1.4546,  ..., 1.4591, 1.4591, 1.4591], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   163  of    180. with Loss: 3.1984829902648926 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.5053, 1.5053, 1.5053,  ..., 1.4863, 1.4863, 1.4840], device='cuda:0')\n",
      "tensor([3, 3, 3,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   164  of    180. with Loss: 1.2982287406921387 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.5010, 1.4960, 1.5010,  ..., 1.4848, 1.4848, 1.4848], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   165  of    180. with Loss: 0.914241373538971 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4882, 1.4882, 1.4882,  ..., 1.4680, 1.4680, 1.4680], device='cuda:0')\n",
      "tensor([3, 1, 2,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   166  of    180. with Loss: 1.224425196647644 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4648, 1.4648, 1.4648,  ..., 1.4768, 1.4732, 1.4768], device='cuda:0')\n",
      "tensor([2, 1, 1,  ..., 1, 1, 1], device='cuda:0')\n",
      "  Batch   167  of    180. with Loss: 1.4768844842910767 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4872, 1.4872, 1.4872,  ..., 1.4771, 1.4771, 1.4777], device='cuda:0')\n",
      "tensor([2, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   168  of    180. with Loss: 1.5523184537887573 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4781, 1.4771, 1.4781,  ..., 1.4903, 1.4903, 1.4905], device='cuda:0')\n",
      "tensor([1, 1, 2,  ..., 1, 1, 1], device='cuda:0')\n",
      "  Batch   169  of    180. with Loss: 2.4311604499816895 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4762, 1.4763, 1.4763,  ..., 1.4554, 1.4554, 1.4554], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   170  of    180. with Loss: 1.797329068183899 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4707, 1.4734, 1.4707,  ..., 1.4888, 1.4888, 1.4888], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   171  of    180. with Loss: 1.524019718170166 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4722, 1.4778, 1.4722,  ..., 1.4568, 1.4568, 1.4568], device='cuda:0')\n",
      "tensor([2, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   172  of    180. with Loss: 1.620447039604187 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4881, 1.4884, 1.4881,  ..., 1.4570, 1.4570, 1.4570], device='cuda:0')\n",
      "tensor([3, 1, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   173  of    180. with Loss: 1.3148131370544434 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4742, 1.4742, 1.4742,  ..., 1.4855, 1.4855, 1.4834], device='cuda:0')\n",
      "tensor([1, 1, 3,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   174  of    180. with Loss: 1.1974718570709229 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4913, 1.4921, 1.4921,  ..., 1.4811, 1.4808, 1.4808], device='cuda:0')\n",
      "tensor([1, 1, 1,  ..., 1, 6, 1], device='cuda:0')\n",
      "  Batch   175  of    180. with Loss: 1.7444766759872437 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4674, 1.4674, 1.4674,  ..., 1.4584, 1.4584, 1.4584], device='cuda:0')\n",
      "tensor([1, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   176  of    180. with Loss: 0.8942700624465942 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4846, 1.4855, 1.4846,  ..., 1.4689, 1.4417, 1.4689], device='cuda:0')\n",
      "tensor([1, 2, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   177  of    180. with Loss: 0.875059187412262 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4753, 1.4780, 1.4780,  ..., 1.4692, 1.4692, 1.4692], device='cuda:0')\n",
      "tensor([1, 2, 1,  ..., 0, 0, 0], device='cuda:0')\n",
      "  Batch   178  of    180. with Loss: 0.7952480912208557 \n",
      "tensor([[[  2.2067,   3.8163,  10.0187,   0.5076,   9.7738,   5.8654,  -1.0755,\n",
      "           -2.0206],\n",
      "         [ -4.7991, -11.1803,  -7.4794,  18.8062, -10.7389,   6.2963,   4.5104,\n",
      "           -4.0958],\n",
      "         [-15.9040,   3.8586,  20.5955,  -4.5642,  -0.7351, -13.7387,   0.9565,\n",
      "           13.1330],\n",
      "         [  2.7264, -10.4844,   3.8281,   6.3467,  -4.3431, -14.5901, -14.8127,\n",
      "           14.8808],\n",
      "         [  7.5410,   8.2542,   9.0923,   1.7846,  10.0602,  11.6901,  13.8824,\n",
      "           -7.2468],\n",
      "         [ 10.0713,  15.5448, -14.1321,  -3.8358,  -4.7070,  -6.5288,  13.5627,\n",
      "          -13.8278],\n",
      "         [  9.0768,   2.5700,   1.4071,  14.0057,   2.6624,   0.2883,  14.7992,\n",
      "            2.2367],\n",
      "         [  6.1124,  -2.7214,  20.7036,  -0.0799,  -8.5900, -13.2694,  -1.6314,\n",
      "           -3.1690],\n",
      "         [  1.9259,  -3.0852,  -3.7788,   1.1874,   0.0633, -14.0997,  -2.3863,\n",
      "          -18.3550]]], device='cuda:0')\n",
      "tensor([1.4512, 1.4512, 1.4512, 1.4512, 1.4512, 1.4512, 1.4512, 1.4512, 1.4512,\n",
      "        1.4512, 1.4512, 1.4512, 1.4512, 1.4512, 1.4512, 1.4512, 1.4512, 1.4512,\n",
      "        1.4512, 1.4512, 1.4512, 1.4512, 1.4512, 1.4512, 1.4512, 1.4512, 1.4512,\n",
      "        1.4512, 1.4512, 1.4512, 1.4512, 1.4512, 1.4777, 1.4777, 1.4777, 1.4777,\n",
      "        1.4777, 1.4777, 1.4777, 1.4777, 1.4777, 1.4777, 1.4777, 1.4777, 1.4777,\n",
      "        1.4777, 1.4777, 1.4777, 1.4777, 1.4777, 1.4777, 1.4777, 1.4777, 1.4777,\n",
      "        1.4777, 1.4777, 1.4777, 1.4777, 1.4777, 1.4777, 1.4777, 1.4777, 1.4777,\n",
      "        1.4777, 1.4657, 1.4657, 1.4657, 1.4657, 1.4657, 1.4657, 1.4657, 1.4657,\n",
      "        1.4651, 1.4651, 1.4657, 1.4657, 1.4657, 1.4657, 1.4651, 1.4651, 1.4651,\n",
      "        1.4657, 1.4651, 1.4651, 1.4657, 1.4657, 1.4657, 1.4657, 1.4657, 1.4657,\n",
      "        1.4657, 1.4651, 1.4657, 1.4651, 1.4651, 1.4651, 1.4637, 1.4637, 1.4637,\n",
      "        1.4637, 1.4637, 1.4637, 1.4637, 1.4637, 1.4637, 1.4637, 1.4637, 1.4637,\n",
      "        1.4637, 1.4637, 1.4637, 1.4637, 1.4637, 1.4637, 1.4637, 1.4637, 1.4637,\n",
      "        1.4637, 1.4637, 1.4637, 1.4637, 1.4637, 1.4637, 1.4637, 1.4637, 1.4637,\n",
      "        1.4637, 1.4637, 1.4609, 1.4609, 1.4609, 1.4609, 1.4609, 1.4609, 1.4609,\n",
      "        1.4609, 1.4609, 1.4609, 1.4609, 1.4609, 1.4609, 1.4609, 1.4609, 1.4609,\n",
      "        1.4609, 1.4609, 1.4609, 1.4609, 1.4609, 1.4609, 1.4609, 1.4609, 1.4609,\n",
      "        1.4609, 1.4609, 1.4609, 1.4609, 1.4609, 1.4609, 1.4609, 1.4565, 1.4565,\n",
      "        1.4565, 1.4565, 1.4565, 1.4565, 1.4565, 1.4565, 1.4565, 1.4565, 1.4565,\n",
      "        1.4565, 1.4565, 1.4565, 1.4565, 1.4565, 1.4565, 1.4565, 1.4565, 1.4565,\n",
      "        1.4565, 1.4565, 1.4565, 1.4569, 1.4565, 1.4565, 1.4565, 1.4565, 1.4565,\n",
      "        1.4565, 1.4565, 1.4565, 1.4789, 1.4793, 1.4822, 1.4841, 1.4821, 1.4821,\n",
      "        1.4824, 1.4801, 1.4789, 1.4836, 1.4865, 1.4821, 1.4820, 1.4821, 1.4821,\n",
      "        1.4789, 1.4821, 1.4821, 1.4789, 1.4809, 1.4800, 1.4820, 1.4821, 1.4820,\n",
      "        1.4820, 1.4841, 1.4821, 1.4821, 1.4821, 1.4800, 1.4793, 1.4800, 1.4844,\n",
      "        1.4844, 1.4844, 1.4844, 1.4844, 1.4844, 1.4844, 1.4844, 1.4844, 1.4844,\n",
      "        1.4844, 1.4844, 1.4844, 1.4844, 1.4844, 1.4844, 1.4844, 1.4844, 1.4844,\n",
      "        1.4844, 1.4844, 1.4844, 1.4844, 1.4844, 1.4844, 1.4844, 1.4844, 1.4844,\n",
      "        1.4844, 1.4844, 1.4844, 1.4844], device='cuda:0')\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 2, 3, 2, 1, 3, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        2, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "  Batch   179  of    180. with Loss: 0.49821627140045166 \n",
      "  Loss: 1.58\n",
      "  Validation took: 0:00:02\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "\n",
    "print(\"Running Validation...\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Put the model in evaluation mode--the dropout layers behave differently\n",
    "# during evaluation.\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "total=0\n",
    "for batch in validation_dataloader:\n",
    "\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask,labels=b_labels)\n",
    "\n",
    "    logits = outputs\n",
    "    print(logits[1].view(-1))\n",
    "    print(b_labels.view(-1))\n",
    "    print('  Batch {:>5,}  of  {:>5,}. with Loss: {:} '.format(nb_eval_steps, len(validation_dataloader),logits[0].item()))\n",
    "    \n",
    "    total+=logits[0].item()\n",
    "\n",
    "\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "# Report the final accuracy for this validation run.\n",
    "print(\"  Loss: {0:.2f}\".format(total/nb_eval_steps))\n",
    "print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "validation_loss.append(total/nb_eval_steps)\n",
    "validation_time.append(format_time(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_loss)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TAPED.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
